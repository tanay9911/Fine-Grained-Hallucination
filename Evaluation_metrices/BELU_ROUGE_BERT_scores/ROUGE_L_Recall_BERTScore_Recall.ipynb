{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "!pip install bert-score\n"
      ],
      "metadata": {
        "id": "5UoIlXpEZrNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAsHcYEGZD1o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from rouge import Rouge\n",
        "from bert_score import score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===== File paths (Google Drive) =====\n",
        "base_path = \"/content/drive/MyDrive/csvs/\"\n",
        "ref_df = pd.read_csv(base_path + \"DrawBenchPrompts.csv\")\n",
        "flux_df = pd.read_csv(base_path + \"meta_captions_Flux-Dev.csv\")\n",
        "sd2_df = pd.read_csv(base_path + \"meta_captions_sd_2.csv\")\n",
        "sdxl_df = pd.read_csv(base_path + \"meta_captions_sdxl.csv\")\n",
        "\n",
        "# ===== Merge on image_name =====\n",
        "merged = ref_df[[\"image_name\", \"Prompts\", \"Category\"]].merge(\n",
        "    flux_df[[\"image_name\", \"Meta Caption\"]].rename(columns={\"Meta Caption\": \"Flux-Dev\"}),\n",
        "    on=\"image_name\"\n",
        ").merge(\n",
        "    sd2_df[[\"image_name\", \"Meta Caption\"]].rename(columns={\"Meta Caption\": \"sd_2\"}),\n",
        "    on=\"image_name\"\n",
        ").merge(\n",
        "    sdxl_df[[\"image_name\", \"Meta Caption\"]].rename(columns={\"Meta Caption\": \"sdxl\"}),\n",
        "    on=\"image_name\"\n",
        ")\n",
        "\n",
        "# ===== Metric Setup =====\n",
        "rouge = Rouge()\n",
        "\n",
        "# ===== Initialize accumulators =====\n",
        "avg_scores = {\n",
        "    \"Flux-Dev\": {\"ROUGE-L_Recall\": 0, \"BERTScore_Recall\": 0},\n",
        "    \"sd_2\": {\"ROUGE-L_Recall\": 0, \"BERTScore_Recall\": 0},\n",
        "    \"sdxl\": {\"ROUGE-L_Recall\": 0, \"BERTScore_Recall\": 0}\n",
        "}\n",
        "\n",
        "num_images = len(merged)\n",
        "\n",
        "# ===== Compute ROUGE Recall + BERT Recall =====\n",
        "for model in [\"Flux-Dev\", \"sd_2\", \"sdxl\"]:\n",
        "    cands = merged[model].fillna(\"\").astype(str).tolist()\n",
        "    refs = merged[\"Prompts\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    # --- ROUGE-L Recall ---\n",
        "    rouge_recall_total = 0\n",
        "    for ref, cand in zip(refs, cands):\n",
        "        try:\n",
        "            rouge_scores = rouge.get_scores(cand, ref)[0]\n",
        "            rouge_recall_total += rouge_scores[\"rouge-l\"][\"r\"]  # recall\n",
        "        except:\n",
        "            rouge_recall_total += 0.0\n",
        "    avg_rouge_recall = rouge_recall_total / num_images\n",
        "\n",
        "    # --- BERT Recall ---\n",
        "    P, R, F1 = score(cands, refs, lang=\"en\", verbose=True)\n",
        "    avg_bert_recall = R.mean().item()\n",
        "\n",
        "    # Save averages\n",
        "    avg_scores[model][\"ROUGE-L_Recall\"] = avg_rouge_recall\n",
        "    avg_scores[model][\"BERTScore_Recall\"] = avg_bert_recall\n",
        "\n",
        "# ===== Save to JSON =====\n",
        "out_path = base_path + \"rouge,bert_recall_scores.json\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(avg_scores, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\" Recall scores saved to {out_path}\")\n"
      ]
    }
  ]
}