{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk96UHuunP_R",
        "outputId": "10ea3069-59e2-467d-b9d9-e238fd9022ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flux-Dev → Average ALOHA: 0.1223\n",
            "sd_2 → Average ALOHA: 0.0672\n",
            "sdxl → Average ALOHA: 0.0811\n",
            " ALOHA scores saved to /content/drive/MyDrive/phi-3-mini/aloha_scores.json\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# ===== File paths =====\n",
        "folder_path = \"/content/drive/MyDrive/phi-3-mini\"\n",
        "files = {\n",
        "    \"Flux-Dev\": \"meta_captions_Flux-Dev_entities.csv\",\n",
        "    \"sd_2\": \"meta_captions_sd_2_entities.csv\",\n",
        "    \"sdxl\": \"meta_captions_sdxl_entities.csv\"\n",
        "}\n",
        "baseline_file = \"DrawBenchPrompts_entities.csv\"\n",
        "\n",
        "# ===== Load baseline =====\n",
        "baseline_df = pd.read_csv(f\"{folder_path}/{baseline_file}\")\n",
        "baseline_df[\"Prompts_entities\"] = baseline_df[\"Prompts_entities\"].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
        "\n",
        "# ===== Function to compute ALOHA for one model =====\n",
        "def compute_aloha(model_file, model_name):\n",
        "    df = pd.read_csv(f\"{folder_path}/{model_file}\")\n",
        "    df[\"Meta Caption_entities\"] = df[\"Meta Caption_entities\"].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
        "\n",
        "    aloha_scores = []\n",
        "    for i, row in baseline_df.iterrows():\n",
        "        baseline_entities = set(row[\"Prompts_entities\"])\n",
        "        model_entities = set(df.loc[i, \"Meta Caption_entities\"])\n",
        "\n",
        "        if len(baseline_entities) == 0:\n",
        "            score = 1.0 if len(model_entities) == 0 else 0.0\n",
        "        else:\n",
        "            correct = baseline_entities & model_entities\n",
        "            score = len(correct) / len(baseline_entities)\n",
        "        aloha_scores.append(score)\n",
        "\n",
        "    avg_aloha = sum(aloha_scores) / len(aloha_scores)\n",
        "    print(f\"{model_name} → Average ALOHA: {avg_aloha:.4f}\")\n",
        "    return avg_aloha\n",
        "\n",
        "# ===== Compute for all models =====\n",
        "results = {}\n",
        "for model_name, model_file in files.items():\n",
        "    results[model_name] = compute_aloha(model_file, model_name)\n",
        "\n",
        "# ===== Save to JSON =====\n",
        "out_path = f\"{folder_path}/aloha_scores.json\"\n",
        "with open(out_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\" ALOHA scores saved to {out_path}\")\n"
      ]
    }
  ]
}