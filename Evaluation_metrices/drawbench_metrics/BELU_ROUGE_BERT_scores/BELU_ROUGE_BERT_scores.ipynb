{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "!pip install bert-score\n"
      ],
      "metadata": {
        "id": "5Rxcs2ro20MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "from bert_score import score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===== File paths (Google Drive) =====\n",
        "base_path = \"/content/drive/MyDrive/\"\n",
        "ref_df = pd.read_csv(base_path + \"DrawBenchPrompts.csv\")\n",
        "flux_df = pd.read_csv(base_path + \"meta_captions_Flux-Dev.csv\")\n",
        "sd2_df = pd.read_csv(base_path + \"meta_captions_sd_2.csv\")\n",
        "sdxl_df = pd.read_csv(base_path + \"meta_captions_sdxl.csv\")\n",
        "\n",
        "# ===== Merge on image_name =====\n",
        "merged = ref_df[[\"image_name\", \"Prompts\", \"Category\"]].merge(\n",
        "    flux_df[[\"image_name\", \"Meta Caption\"]].rename(columns={\"Meta Caption\": \"Flux-Dev\"}),\n",
        "    on=\"image_name\"\n",
        ").merge(\n",
        "    sd2_df[[\"image_name\", \"Meta Caption\"]].rename(columns={\"Meta Caption\": \"sd_2\"}),\n",
        "    on=\"image_name\"\n",
        ").merge(\n",
        "    sdxl_df[[\"image_name\", \"Meta Caption\"]].rename(columns={\"Meta Caption\": \"sdxl\"}),\n",
        "    on=\"image_name\"\n",
        ")\n",
        "\n",
        "# ===== Metric Setup =====\n",
        "rouge = Rouge()\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "# ===== Initialize accumulators =====\n",
        "avg_scores = {\n",
        "    \"Flux-Dev\": {\"BLEU-1\": 0, \"BLEU-4\": 0, \"ROUGE-L_F1\": 0, \"BERTScore_F1\": 0},\n",
        "    \"sd_2\": {\"BLEU-1\": 0, \"BLEU-4\": 0, \"ROUGE-L_F1\": 0, \"BERTScore_F1\": 0},\n",
        "    \"sdxl\": {\"BLEU-1\": 0, \"BLEU-4\": 0, \"ROUGE-L_F1\": 0, \"BERTScore_F1\": 0}\n",
        "}\n",
        "\n",
        "num_images = len(merged)\n",
        "\n",
        "# ===== Compute BLEU + ROUGE =====\n",
        "for model in [\"Flux-Dev\", \"sd_2\", \"sdxl\"]:\n",
        "    cands = merged[model].fillna(\"\").astype(str).tolist()\n",
        "    refs = merged[\"Prompts\"].fillna(\"\").astype(str).tolist()\n",
        "\n",
        "    bleu1_total, bleu4_total, rougeL_total = 0, 0, 0\n",
        "\n",
        "    for ref, cand in zip(refs, cands):\n",
        "        # BLEU-1\n",
        "        try:\n",
        "            bleu1 = sentence_bleu([ref.split()], cand.split(), weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
        "        except:\n",
        "            bleu1 = 0.0\n",
        "        bleu1_total += bleu1\n",
        "\n",
        "        # BLEU-4\n",
        "        try:\n",
        "            bleu4 = sentence_bleu([ref.split()], cand.split(), weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
        "        except:\n",
        "            bleu4 = 0.0\n",
        "        bleu4_total += bleu4\n",
        "\n",
        "        # ROUGE-L F1\n",
        "        try:\n",
        "            rouge_scores = rouge.get_scores(cand, ref)[0]\n",
        "            rougeL_total += rouge_scores[\"rouge-l\"][\"f\"]\n",
        "        except:\n",
        "            rougeL_total += 0.0\n",
        "\n",
        "    # BERTScore (batch)\n",
        "    P, R, F1 = score(cands, refs, lang=\"en\", verbose=True)\n",
        "    avg_bert_f1 = F1.mean().item()\n",
        "\n",
        "    # Save averages\n",
        "    avg_scores[model][\"BLEU-1\"] = bleu1_total / num_images\n",
        "    avg_scores[model][\"BLEU-4\"] = bleu4_total / num_images\n",
        "    avg_scores[model][\"ROUGE-L_F1\"] = rougeL_total / num_images\n",
        "    avg_scores[model][\"BERTScore_F1\"] = avg_bert_f1\n",
        "\n",
        "# ===== Save to JSON =====\n",
        "out_path = base_path + \"caption_eval_avg_scores.json\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(avg_scores, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(f\" Average scores saved to {out_path}\")\n"
      ],
      "metadata": {
        "id": "nNQlF9QzFzR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}