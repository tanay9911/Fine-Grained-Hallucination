{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/salaniz/pycocoevalcap.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEqAm26yZiF0",
        "outputId": "7ba1585a-f832-465c-a6a3-30a0d1442521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/salaniz/pycocoevalcap.git\n",
            "  Cloning https://github.com/salaniz/pycocoevalcap.git to /tmp/pip-req-build-5t8407uv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap.git /tmp/pip-req-build-5t8407uv\n",
            "  Resolved https://github.com/salaniz/pycocoevalcap.git to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from pycocoevalcap==1.2) (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (2.0.2)\n",
            "Building wheels for collected packages: pycocoevalcap\n",
            "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312245 sha256=d4e3c082772f93fdb603ecc1e7e7c8f51f8fdeb2f58b79c26f4f04dbd2fb6a2f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5mthzao7/wheels/b7/7d/bd/ee763c30e9e11deccc65814eb10c3f58b155fee1d68c0a392f\n",
            "Successfully built pycocoevalcap\n",
            "Installing collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBOQJFCHZalo",
        "outputId": "acb884df-41eb-45f3-ed22-51947609a2c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved CIDEr dictionaries for Flux-Dev:\n",
            "  refs -> /content/drive/MyDrive/csvs/cider_dicts/Flux-Dev_refs.json\n",
            "  cands -> /content/drive/MyDrive/csvs/cider_dicts/Flux-Dev_cands.json\n",
            "Saved CIDEr dictionaries for SDXL:\n",
            "  refs -> /content/drive/MyDrive/csvs/cider_dicts/SDXL_refs.json\n",
            "  cands -> /content/drive/MyDrive/csvs/cider_dicts/SDXL_cands.json\n",
            "Saved CIDEr dictionaries for SD2:\n",
            "  refs -> /content/drive/MyDrive/csvs/cider_dicts/SD2_refs.json\n",
            "  cands -> /content/drive/MyDrive/csvs/cider_dicts/SD2_cands.json\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "\n",
        "# ===== Paths =====\n",
        "root_csv = \"/content/drive/MyDrive/csvs\"\n",
        "generated_files = {\n",
        "    \"Flux-Dev\": \"meta_captions_Flux-Dev.csv\",\n",
        "    \"SDXL\": \"meta_captions_sdxl.csv\",\n",
        "    \"SD2\": \"meta_captions_sd_2.csv\"\n",
        "}\n",
        "\n",
        "# Output folder for JSON dictionaries\n",
        "output_json_folder = os.path.join(root_csv, \"cider_dicts\")\n",
        "os.makedirs(output_json_folder, exist_ok=True)\n",
        "\n",
        "# ===== Text cleaning function =====\n",
        "def clean_text(text):\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        return None\n",
        "    text = str(text).lower().strip()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "# ===== Function to convert CSV to CIDEr dicts =====\n",
        "def csv_to_cider_dicts(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df.dropna(subset=[\"Prompts\", \"Meta Caption\"])\n",
        "\n",
        "    refs = {}\n",
        "    cands = {}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        image_id = str(row[\"image_name\"])\n",
        "        ref_text = clean_text(row[\"Prompts\"])\n",
        "        cand_text = clean_text(row[\"Meta Caption\"])\n",
        "        if ref_text and cand_text:\n",
        "            refs[image_id] = [ref_text]   # refs must be a list\n",
        "            cands[image_id] = [cand_text]   # candidate is single string\n",
        "\n",
        "    return refs, cands\n",
        "\n",
        "# ===== Process all CSVs and save JSONs =====\n",
        "for model_name, file_name in generated_files.items():\n",
        "    file_path = os.path.join(root_csv, file_name)\n",
        "    refs, cands = csv_to_cider_dicts(file_path)\n",
        "\n",
        "    refs_json_path = os.path.join(output_json_folder, f\"{model_name}_refs.json\")\n",
        "    cands_json_path = os.path.join(output_json_folder, f\"{model_name}_cands.json\")\n",
        "\n",
        "    with open(refs_json_path, \"w\") as f:\n",
        "        json.dump(refs, f, indent=4)\n",
        "    with open(cands_json_path, \"w\") as f:\n",
        "        json.dump(cands, f, indent=4)\n",
        "\n",
        "    print(f\"Saved CIDEr dictionaries for {model_name}:\")\n",
        "    print(f\"  refs -> {refs_json_path}\")\n",
        "    print(f\"  cands -> {cands_json_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "# ===== Paths =====\n",
        "root_csv = \"/content/drive/MyDrive/csvs\"\n",
        "cider_json_folder = os.path.join(root_csv, \"cider_dicts\")\n",
        "output_json = os.path.join(root_csv, \"cider_average_scores.json\")\n",
        "\n",
        "# Models\n",
        "models = [\"Flux-Dev\", \"SDXL\", \"SD2\"]\n",
        "\n",
        "# ===== Compute CIDEr =====\n",
        "average_scores = {}\n",
        "\n",
        "for model_name in models:\n",
        "    refs_path = os.path.join(cider_json_folder, f\"{model_name}_refs.json\")\n",
        "    cands_path = os.path.join(cider_json_folder, f\"{model_name}_cands.json\")\n",
        "\n",
        "    # Load JSON dictionaries\n",
        "    with open(refs_path) as f:\n",
        "        refs = json.load(f)\n",
        "    with open(cands_path) as f:\n",
        "        cands = json.load(f)\n",
        "\n",
        "    try:\n",
        "        scorer = Cider()\n",
        "        score, _ = scorer.compute_score(refs, cands)\n",
        "        average_scores[model_name] = score\n",
        "        print(f\"CIDEr score for {model_name}: {score}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing CIDEr for {model_name}: {e}\")\n",
        "        average_scores[model_name] = None\n",
        "\n",
        "# ===== Save averages to JSON =====\n",
        "with open(output_json, \"w\") as f:\n",
        "    json.dump(average_scores, f, indent=4)\n",
        "\n",
        "print(\"Average CIDEr scores saved to:\", output_json)\n",
        "print(average_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM0ebKXGcSkK",
        "outputId": "b374df2a-dcaa-4ede-99e0-ab5afa963809"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIDEr score for Flux-Dev: 0.037878950842012274\n",
            "CIDEr score for SDXL: 0.024955449323365252\n",
            "CIDEr score for SD2: 0.02940295592983221\n",
            "Average CIDEr scores saved to: /content/drive/MyDrive/csvs/cider_average_scores.json\n",
            "{'Flux-Dev': np.float64(0.037878950842012274), 'SDXL': np.float64(0.024955449323365252), 'SD2': np.float64(0.02940295592983221)}\n"
          ]
        }
      ]
    }
  ]
}