{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download necessary NLTK resources for METEOR\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # WordNet multilingual data, sometimes needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d1KzM6CWqps",
        "outputId": "b3be9d8b-12c9-4a40-88a7-eeafd853bb52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxBey4czVmoT",
        "outputId": "9984c4c2-4f90-4a83-ad0d-07255cf587bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Calculating METEOR for meta_captions_Flux-Dev.csv: 100%|██████████| 170/170 [00:00<00:00, 520.32it/s]\n",
            "Calculating METEOR for meta_captions_sdxl.csv: 100%|██████████| 200/200 [00:00<00:00, 246.98it/s]\n",
            "Calculating METEOR for meta_captions_sd_2.csv: 100%|██████████| 200/200 [00:00<00:00, 272.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average METEOR scores saved to: /content/drive/MyDrive/csvs/meteor_average_scores.json\n",
            "{'Flux-Dev': 0.18172076704307052, 'SDXL': 0.15261947975059628, 'SD2': 0.14240416027352182}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "# Make sure NLTK wordnet is downloaded\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# ===== Paths =====\n",
        "root_csv = \"/content/drive/MyDrive/csvs\"\n",
        "baseline_file = os.path.join(root_csv, \"DrawBenchPrompts.csv\")\n",
        "\n",
        "# Generated captions (full paths)\n",
        "generated_files = {\n",
        "    \"Flux-Dev\": os.path.join(root_csv, \"meta_captions_Flux-Dev.csv\"),\n",
        "    \"SDXL\": os.path.join(root_csv, \"meta_captions_sdxl.csv\"),\n",
        "    \"SD2\": os.path.join(root_csv, \"meta_captions_sd_2.csv\")\n",
        "}\n",
        "\n",
        "# Output JSON file\n",
        "output_json = os.path.join(root_csv, \"meteor_average_scores.json\")\n",
        "\n",
        "# ===== Load baseline prompts =====\n",
        "baseline_df = pd.read_csv(baseline_file)\n",
        "baseline_prompts_dict = dict(zip(baseline_df[\"image_name\"], baseline_df[\"Prompts\"]))\n",
        "\n",
        "# ===== Function to compute METEOR for one model =====\n",
        "def compute_meteor(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    scores = []\n",
        "\n",
        "    # Drop rows where either 'Prompts' or 'Meta Caption' is empty or NaN\n",
        "    df = df.dropna(subset=[\"Prompts\", \"Meta Caption\"])\n",
        "    df = df[(df[\"Prompts\"].str.strip() != \"\") & (df[\"Meta Caption\"].str.strip() != \"\")]\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Calculating METEOR for {os.path.basename(file_path)}\"):\n",
        "        ref_prompt = str(row[\"Prompts\"]).split()          # tokenize reference\n",
        "        candidate_caption = str(row[\"Meta Caption\"]).split()  # tokenize candidate\n",
        "\n",
        "        # Compute METEOR\n",
        "        score = meteor_score([ref_prompt], candidate_caption)\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# ===== Compute for all models =====\n",
        "average_scores = {}\n",
        "for model_name, file_path in generated_files.items():\n",
        "    scores = compute_meteor(file_path)\n",
        "    avg = sum(scores) / len(scores) if scores else 0\n",
        "    average_scores[model_name] = avg\n",
        "\n",
        "# ===== Save averages to JSON =====\n",
        "with open(output_json, \"w\") as f:\n",
        "    json.dump(average_scores, f, indent=4)\n",
        "\n",
        "print(\"Average METEOR scores saved to:\", output_json)\n",
        "print(average_scores)\n"
      ]
    }
  ]
}