{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79U97uhmqSHe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "# ===== File paths =====\n",
        "folder_path = \"/content/drive/MyDrive/phi-3-mini\"\n",
        "files = {\n",
        "    \"Flux-Dev\": \"meta_captions_Flux-Dev_entities.csv\",\n",
        "    \"sd_2\": \"meta_captions_sd_2_entities.csv\",\n",
        "    \"sdxl\": \"meta_captions_sdxl_entities.csv\"\n",
        "}\n",
        "baseline_file = \"DrawBenchPrompts_entities.csv\"\n",
        "\n",
        "# ===== Load baseline =====\n",
        "baseline_df = pd.read_csv(f\"{folder_path}/{baseline_file}\")\n",
        "baseline_df[\"Prompts_entities\"] = baseline_df[\"Prompts_entities\"].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
        "\n",
        "# ===== Function to compute MHalDetect for one model =====\n",
        "def compute_mhal(model_file, model_name):\n",
        "    df = pd.read_csv(f\"{folder_path}/{model_file}\")\n",
        "    df[\"Meta Caption_entities\"] = df[\"Meta Caption_entities\"].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
        "\n",
        "    mhal_scores = []\n",
        "    for i, row in baseline_df.iterrows():\n",
        "        baseline_entities = set(row[\"Prompts_entities\"])\n",
        "        model_entities = set(df.loc[i, \"Meta Caption_entities\"])\n",
        "\n",
        "        if len(model_entities) == 0:\n",
        "            score = 0.0\n",
        "        else:\n",
        "            # Hallucinated entities = model entities NOT in baseline\n",
        "            hallucinated = model_entities - baseline_entities\n",
        "            score = len(hallucinated) / len(model_entities)\n",
        "        mhal_scores.append(score)\n",
        "\n",
        "    avg_mhal = sum(mhal_scores) / len(mhal_scores)\n",
        "    print(f\"{model_name} â†’ Average MHalDetect: {avg_mhal:.4f}\")\n",
        "    return avg_mhal\n",
        "\n",
        "# ===== Compute for all models =====\n",
        "results = {}\n",
        "for model_name, model_file in files.items():\n",
        "    results[model_name] = compute_mhal(model_file, model_name)\n",
        "\n",
        "# ===== Save to JSON =====\n",
        "out_path = f\"{folder_path}/MHalDetect_scores.json\"\n",
        "with open(out_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "print(f\"MHalDetect scores saved to {out_path}\")\n"
      ]
    }
  ]
}