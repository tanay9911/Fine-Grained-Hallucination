{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating the captions from Transformer models"
      ],
      "metadata": {
        "id": "ILvmGAhfYqN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, ViTImageProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining the folder path where images are stored\n",
        "image_folder = \"/content/drive/MyDrive/Fine-Grained-Hallucination-main/sd_2_outputs\"\n",
        "\n",
        "# Defining the output CSV file path where captions will be saved\n",
        "csv_output_path = \"/content/drive/MyDrive/image_captions.csv\"\n",
        "\n",
        "# Loading the BLIP and ViT models along with their processors and tokenizers\n",
        "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "vit_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "vit_model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "vit_tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "# Defining a function that is generating captions for a given image\n",
        "def generate_caption(image_path, processor, model, tokenizer=None):\n",
        "    # Opening and converting the image to RGB format for processing\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Checking if the processor belongs to ViT and preparing the pixel values accordingly\n",
        "    if isinstance(processor, ViTImageProcessor):\n",
        "        pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "        # Generating the caption using the ViT model while ensuring no gradient computation for efficiency\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(pixel_values)\n",
        "\n",
        "        # Decoding the generated token IDs to get the final caption\n",
        "        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    else:\n",
        "        # Preparing inputs for BLIP model\n",
        "        inputs = processor(image, return_tensors=\"pt\")\n",
        "\n",
        "        # Generating the caption using BLIP while ensuring efficient computation\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs)\n",
        "\n",
        "        # Decoding the generated output into a readable caption\n",
        "        caption = processor.batch_decode(output, skip_special_tokens=True)[0]\n",
        "\n",
        "    return caption\n",
        "\n",
        "# Initializing an empty list for storing the image captions\n",
        "captions_data = []\n",
        "\n",
        "# Iterating through each image file in the specified folder\n",
        "for image_name in os.listdir(image_folder):\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "    # Checking if the file is an image format (PNG, JPG, JPEG)\n",
        "    if image_path.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "        try:\n",
        "            # Generating captions using BLIP and ViT models\n",
        "            blip_caption = generate_caption(image_path, blip_processor, blip_model)\n",
        "            vit_caption = generate_caption(image_path, vit_processor, vit_model, tokenizer=vit_tokenizer)\n",
        "\n",
        "            # Appending the generated captions and image ID to the list\n",
        "            captions_data.append({\n",
        "                \"image_id\": image_name,\n",
        "                \"blip_caption\": blip_caption,\n",
        "                \"vit_caption\": vit_caption\n",
        "            })\n",
        "\n",
        "        # Handling any exceptions that might occur during the processing\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {image_name}: {e}\")\n",
        "\n",
        "# Creating a DataFrame to store the collected captions\n",
        "df = pd.DataFrame(captions_data)\n",
        "\n",
        "# Saving the processed captions into a CSV file\n",
        "df.to_csv(csv_output_path, index=False)\n",
        "\n",
        "# Printing a confirmation message to indicate successful saving of captions\n",
        "print(f\"Captions saved to {csv_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUnERhE1Luy0",
        "outputId": "f7cf9664-3d04-45ec-fa7d-2cc05e4652c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
            "  \"architectures\": [\n",
            "    \"ViTModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"pooler_act\": \"tanh\",\n",
            "  \"pooler_output_size\": 768,\n",
            "  \"qkv_bias\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"pad_token_id\": 50256,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Captions saved to /content/drive/MyDrive/image_captions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')  # Explicitly download punkt_tab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p-Bb7iehRWU",
        "outputId": "eab9678b-e39c-4637-fe23-a6313eecd755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PLELuj8h3l-",
        "outputId": "7dfc33a6-775a-4d59-963a-5bccd0b1f463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=40932ebdb7f28c646098ec774315d89efda413237416ff6c40f89f1e9fe88f4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the CSV file\n",
        "csv_path = \"/content/drive/MyDrive/image_captions.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Extract numeric part of image_id and convert to integer for sorting\n",
        "df['numeric_id'] = df['image_id'].str.extract(r'(\\d+)').astype(int)\n",
        "\n",
        "# Sort by numeric_id in ascending order\n",
        "df_sorted = df.sort_values(by=\"numeric_id\", ascending=True).drop(columns=['numeric_id'])\n",
        "\n",
        "# Save the sorted CSV as a new file\n",
        "sorted_csv_path = \"/content/drive/MyDrive/sorted_images_captions.csv\"\n",
        "df_sorted.to_csv(sorted_csv_path, index=False)\n",
        "\n",
        "print(f\"Sorted CSV saved successfully at {sorted_csv_path}\")\n"
      ],
      "metadata": {
        "id": "v-55WUgAlHrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we loaded the original prompts containing .csv file and renamed it for ease of use\n",
        "# Load the CSV file\n",
        "csv_path = \"/content/drive/MyDrive/Fine-Grained-Hallucination-main/DrawBenchPrompts.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Generate image IDs in the format '0.jpg', '1.jpg', ..., '199.jpg'\n",
        "df.insert(0, 'image_id', [f\"{i}.jpg\" for i in range(len(df))])\n",
        "\n",
        "# Save the updated CSV with the new name\n",
        "new_csv_path = \"/content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv\"\n",
        "df.to_csv(new_csv_path, index=False)\n",
        "\n",
        "print(f\"Updated CSV saved successfully as {new_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae1M5KlKvXoP",
        "outputId": "98281cb9-d9c6-4cab-fce5-d8092848fe53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated CSV saved successfully as /content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all necessary libraries\n",
        "import os\n",
        "import time\n",
        "import string\n",
        "import nltk\n",
        "from ast import literal_eval\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "# NLTK-related imports\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# ROUGE scoring\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Sentence similarity tools\n",
        "from sentence_transformers import SentenceTransformer, util\n"
      ],
      "metadata": {
        "id": "VSJByBwmc8-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating various metrics"
      ],
      "metadata": {
        "id": "eLOV4DfKZ4D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CSV files that contain captions and corresponding prompts\n",
        "captions_df = pd.read_csv('/content/drive/MyDrive/sorted_images_captions.csv')\n",
        "prompts_df = pd.read_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv')\n",
        "\n",
        "# Initializing the ROUGE scorer to calculate similarity scores based on different ROUGE metrics\n",
        "# Also, initializing the BLEU smoothing function to improve BLEU score calculations\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "smoothing = SmoothingFunction()\n",
        "\n",
        "# Defining a function that is computing the BLEU, METEOR, and ROUGE scores for a given prompt and caption\n",
        "def compute_scores(prompt, caption):\n",
        "    # Tokenizing both the prompt and the caption to prepare them for BLEU and METEOR calculations\n",
        "    tokenized_prompt = word_tokenize(prompt)\n",
        "    tokenized_caption = word_tokenize(caption)\n",
        "\n",
        "    # Calculating the BLEU score by comparing the caption against the reference prompt\n",
        "    bleu = sentence_bleu([tokenized_prompt], tokenized_caption, smoothing_function=smoothing.method1)\n",
        "\n",
        "    # Calculating the METEOR score which considers synonyms and paraphrases for evaluation\n",
        "    meteor = meteor_score([tokenized_prompt], tokenized_caption)\n",
        "\n",
        "    # Calculating the ROUGE score to measure the overlap of words and phrases between the prompt and caption\n",
        "    rouge_scores = rouge.score(prompt, caption)\n",
        "\n",
        "    return bleu, meteor, rouge_scores['rouge1'].fmeasure\n",
        "\n",
        "# Merging the captions dataset with the prompts dataset based on the 'image_id' column\n",
        "merged_df = captions_df.merge(prompts_df, on='image_id')\n",
        "\n",
        "# Initializing an empty list to store the computed similarity scores for each image caption\n",
        "results = []\n",
        "\n",
        "# Iterating over each row in the merged dataset to compute the similarity scores\n",
        "for index, row in merged_df.iterrows():\n",
        "    prompt = row['Prompts']\n",
        "    blip_caption = row['blip_caption']\n",
        "    vit_caption = row['vit_caption']\n",
        "\n",
        "    # Computing BLEU, METEOR, and ROUGE scores for BLIP-generated captions\n",
        "    bleu_blip, meteor_blip, rouge_blip = compute_scores(prompt, blip_caption)\n",
        "\n",
        "    # Computing BLEU, METEOR, and ROUGE scores for ViT-generated captions\n",
        "    bleu_vit, meteor_vit, rouge_vit = compute_scores(prompt, vit_caption)\n",
        "\n",
        "    # Storing the computed scores in a structured dictionary for later analysis\n",
        "    results.append({\n",
        "        'image_id': row['image_id'],\n",
        "        'bleu_blip': bleu_blip,\n",
        "        'bleu_vit': bleu_vit,\n",
        "        'meteor_blip': meteor_blip,\n",
        "        'meteor_vit': meteor_vit,\n",
        "        'rouge_blip': rouge_blip,\n",
        "        'rouge_vit': rouge_vit\n",
        "    })\n",
        "\n",
        "# Creating a DataFrame from the results and saving it as a CSV file for further analysis\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/1final.csv', index=False)\n",
        "\n",
        "# Calculating and printing the average scores for BLEU, METEOR, and ROUGE metrics across all captions\n",
        "averages = final_df.mean(numeric_only=True)\n",
        "print(\"Average Scores:\\n\", averages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQMlATlbrcv3",
        "outputId": "0cf1308d-3c8b-4779-bff4-e596854625fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Scores:\n",
            " bleu_blip      0.038710\n",
            "bleu_vit       0.033498\n",
            "meteor_blip    0.227122\n",
            "meteor_vit     0.186605\n",
            "rouge_blip     0.314312\n",
            "rouge_vit      0.289201\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCuL41IbxDb_",
        "outputId": "98f2cb61-26f4-4048-f8be-cc65e18ed31d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating similarity scores between the original prompts and the ones generated from transformer models"
      ],
      "metadata": {
        "id": "XNkyPpxFaHrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the required NLTK datasets for tokenization, stopword removal, and lemmatization\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Loading the CSV files containing image captions and their corresponding prompts\n",
        "captions_df = pd.read_csv('/content/drive/MyDrive/sorted_images_captions.csv')\n",
        "prompts_df = pd.read_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv')\n",
        "\n",
        "# Initializing the evaluation tools: ROUGE scorer for textual overlap, smoothing function for BLEU, and Sentence Transformer for semantic similarity\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "smoothing = SmoothingFunction()\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initializing the lemmatizer for text preprocessing and setting up the stopword removal set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Defining a function that is computing the semantic similarity between a prompt and generated caption\n",
        "def compute_semantic_similarity(prompt, caption):\n",
        "    # Encoding the prompt and caption into vector representations\n",
        "    prompt_embedding = model.encode(prompt, convert_to_tensor=True)\n",
        "    caption_embedding = model.encode(caption, convert_to_tensor=True)\n",
        "\n",
        "    # Calculating cosine similarity between the two encoded vectors to assess semantic closeness\n",
        "    return util.cos_sim(prompt_embedding, caption_embedding).item()\n",
        "\n",
        "# Defining a function that is computing various evaluation metrics between a prompt and its corresponding caption\n",
        "def compute_scores(prompt, caption):\n",
        "    # Ensuring inputs are converted to strings if necessary\n",
        "    if not isinstance(prompt, str): prompt = str(prompt)\n",
        "    if not isinstance(caption, str): caption = str(caption)\n",
        "\n",
        "    # Tokenizing the prompt and caption for BLEU and METEOR score calculations\n",
        "    tokenized_prompt = word_tokenize(prompt)\n",
        "    tokenized_caption = word_tokenize(caption)\n",
        "\n",
        "    # Computing the BLEU score using a smoothing function to improve readability of comparisons\n",
        "    bleu = sentence_bleu([tokenized_prompt], tokenized_caption, smoothing_function=smoothing.method1)\n",
        "\n",
        "    # Computing the METEOR score, which takes synonym matching into account for better evaluation\n",
        "    meteor = meteor_score([tokenized_prompt], tokenized_caption)\n",
        "\n",
        "    # Computing the ROUGE score, which measures the amount of text overlap between the prompt and generated caption\n",
        "    rouge_scores = rouge.score(prompt, caption)\n",
        "\n",
        "    # Computing semantic similarity between the prompt and caption using cosine similarity\n",
        "    semantic_similarity = compute_semantic_similarity(prompt, caption)\n",
        "\n",
        "    return bleu, meteor, rouge_scores['rouge1'].fmeasure, semantic_similarity\n",
        "\n",
        "# Merging the captions dataset with the prompts dataset based on the image ID\n",
        "merged_df = captions_df.merge(prompts_df, on='image_id')\n",
        "\n",
        "# Initializing an empty list for storing evaluation results\n",
        "results = []\n",
        "\n",
        "# Iterating through each row in the merged dataset to compute evaluation scores\n",
        "for index, row in merged_df.iterrows():\n",
        "    prompt = row['Prompts']\n",
        "    blip_caption = row['blip_caption']\n",
        "    vit_caption = row['vit_caption']\n",
        "\n",
        "    # Computing evaluation scores for BLIP-generated captions\n",
        "    bleu_blip, meteor_blip, rouge_blip, semantic_blip = compute_scores(prompt, blip_caption)\n",
        "\n",
        "    # Computing evaluation scores for ViT-generated captions\n",
        "    bleu_vit, meteor_vit, rouge_vit, semantic_vit = compute_scores(prompt, vit_caption)\n",
        "\n",
        "    # Storing the computed scores for analysis\n",
        "    results.append({\n",
        "        'image_id': row['image_id'],\n",
        "        'bleu_blip': bleu_blip,\n",
        "        'bleu_vit': bleu_vit,\n",
        "        'meteor_blip': meteor_blip,\n",
        "        'meteor_vit': meteor_vit,\n",
        "        'rouge_blip': rouge_blip,\n",
        "        'rouge_vit': rouge_vit,\n",
        "        'semantic_blip': semantic_blip,\n",
        "        'semantic_vit': semantic_vit\n",
        "    })\n",
        "\n",
        "# Creating a DataFrame from the results and saving it as a CSV file for further analysis\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/modifies.csv', index=False)\n",
        "\n",
        "# Calculating and printing the average scores across all evaluated captions\n",
        "averages = final_df.mean(numeric_only=True)\n",
        "print(\"Average Scores:\\n\", averages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOTXTuiRxvHG",
        "outputId": "c375a1e8-696b-44d8-a6fb-799a73a5992f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Scores:\n",
            " bleu_blip        0.038710\n",
            "bleu_vit         0.033498\n",
            "meteor_blip      0.227122\n",
            "meteor_vit       0.186605\n",
            "rouge_blip       0.314312\n",
            "rouge_vit        0.289201\n",
            "semantic_blip    0.542584\n",
            "semantic_vit     0.427803\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating CHAIR score using a custom function"
      ],
      "metadata": {
        "id": "JBTGDb7rarQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the generated captions dataset and the ground truth prompts dataset\n",
        "captions_df = pd.read_csv('/content/drive/MyDrive/sorted_images_captions.csv')  # Generated captions\n",
        "prompts_df = pd.read_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv')  # Ground truth\n",
        "\n",
        "# Defining a function that is extracting objects from a given text\n",
        "def extract_objects(text):\n",
        "    # Tokenizing the input text and converting it to lowercase for consistency\n",
        "    tokens = nltk.word_tokenize(str(text).lower())\n",
        "    return tokens  # Adjusting the method if needed for better object detection\n",
        "\n",
        "# Defining a function that is identifying hallucinations by comparing caption and ground truth objects\n",
        "def compute_hallucinations(caption_objects, gt_objects):\n",
        "    # Converting both sets of objects into unique sets for comparison\n",
        "    caption_set = set(caption_objects)\n",
        "    gt_set = set(gt_objects)\n",
        "\n",
        "    # Determining objects that appear in the caption but are missing in the ground truth\n",
        "    hallucinated_objects = caption_set - gt_set\n",
        "    return hallucinated_objects\n",
        "\n",
        "# Merging the datasets on 'image_id' to align generated captions with their respective prompts\n",
        "merged_df = captions_df.merge(prompts_df, on='image_id')\n",
        "\n",
        "# Initializing an empty list for storing CHAIR scores and a counter for hallucinated captions\n",
        "chair_scores = []\n",
        "captions_with_hallucinations = 0\n",
        "\n",
        "# Iterating through each row in the merged dataset to compute CHAIR scores\n",
        "for _, row in merged_df.iterrows():\n",
        "    image_id = row['image_id']\n",
        "    prompt = row['Prompts']\n",
        "    blip_caption = row['blip_caption']\n",
        "    vit_caption = row['vit_caption']\n",
        "\n",
        "    # Extracting objects from the ground truth prompt\n",
        "    gt_objects = extract_objects(prompt)\n",
        "\n",
        "    # Extracting objects from the generated captions using the same method\n",
        "    blip_objects = extract_objects(blip_caption)\n",
        "    vit_objects = extract_objects(vit_caption)\n",
        "\n",
        "    # Identifying hallucinations by comparing generated captions to the ground truth objects\n",
        "    hallucinated_blip = compute_hallucinations(blip_objects, gt_objects)\n",
        "    hallucinated_vit = compute_hallucinations(vit_objects, gt_objects)\n",
        "\n",
        "    # Computing CHAIR_i scores, which measure the proportion of hallucinated objects in the generated caption\n",
        "    chair_i_blip = len(hallucinated_blip) / len(blip_objects) if len(blip_objects) > 0 else 0\n",
        "    chair_i_vit = len(hallucinated_vit) / len(vit_objects) if len(vit_objects) > 0 else 0\n",
        "\n",
        "    # Tracking captions that contain hallucinated objects for CHAIR_s score computation\n",
        "    if hallucinated_blip or hallucinated_vit:\n",
        "        captions_with_hallucinations += 1\n",
        "\n",
        "    # Storing computed CHAIR scores and hallucinated objects in a structured format\n",
        "    chair_scores.append({\n",
        "        'image_id': image_id,\n",
        "        'chair_i_blip': chair_i_blip,\n",
        "        'chair_i_vit': chair_i_vit,\n",
        "        'hallucinated_objects_blip': list(hallucinated_blip),\n",
        "        'hallucinated_objects_vit': list(hallucinated_vit)\n",
        "    })\n",
        "\n",
        "# Computing CHAIR_s score, which evaluates how frequently captions contain hallucinations\n",
        "total_captions = len(captions_df)\n",
        "chair_s = captions_with_hallucinations / total_captions\n",
        "\n",
        "# Creating a DataFrame with the computed CHAIR scores and saving the results as a CSV file\n",
        "final_df = pd.DataFrame(chair_scores)\n",
        "final_df.to_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/CHAIR_Score_Results.csv', index=False)\n",
        "\n",
        "# Printing the computed CHAIR_s score for summary analysis\n",
        "print(f\"CHAIR_s Score: {chair_s}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0QWQ3lhYRdh",
        "outputId": "5c244294-a1ca-482a-dfc5-4e11fbe98526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAIR_s Score: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating SPICE score"
      ],
      "metadata": {
        "id": "ZeoIKefBa7cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading datasets that contain generated captions and ground truth prompts for evaluation\n",
        "captions_df = pd.read_csv('/content/drive/MyDrive/sorted_images_captions.csv')  # Generated captions\n",
        "prompts_df = pd.read_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv')  # Ground truth prompts\n",
        "\n",
        "# Merging both datasets on 'image_id' to align generated captions with their respective reference prompts\n",
        "merged_df = captions_df.merge(prompts_df, on='image_id')\n",
        "\n",
        "# Initializing the sentence similarity model, which is being used for semantic evaluation\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Defining a function that is computing semantic similarity between the reference prompt and the generated caption\n",
        "def compute_spice(reference, candidate):\n",
        "    # Encoding both reference and candidate text into vector representations\n",
        "    reference_embedding = model.encode(reference, convert_to_tensor=True)\n",
        "    candidate_embedding = model.encode(candidate, convert_to_tensor=True)\n",
        "\n",
        "    # Calculating cosine similarity, which is serving as a proxy for SPICE score\n",
        "    return util.cos_sim(reference_embedding, candidate_embedding).item()\n",
        "\n",
        "# Initializing an empty list to store computed SPICE scores\n",
        "results = []\n",
        "\n",
        "# Iterating over each row in the merged dataset to compute SPICE scores for BLIP and ViT captions\n",
        "for _, row in merged_df.iterrows():\n",
        "    image_id = row['image_id']\n",
        "    prompt = row['Prompts']\n",
        "    blip_caption = row['blip_caption']\n",
        "    vit_caption = row['vit_caption']\n",
        "\n",
        "    # Computing SPICE scores for both BLIP and ViT-generated captions using semantic similarity\n",
        "    spice_blip = compute_spice(prompt, blip_caption)\n",
        "    spice_vit = compute_spice(prompt, vit_caption)\n",
        "\n",
        "    # Storing computed SPICE scores for later analysis\n",
        "    results.append({\n",
        "        \"image_id\": image_id,\n",
        "        \"spice_blip\": spice_blip,\n",
        "        \"spice_vit\": spice_vit\n",
        "    })\n",
        "\n",
        "# Creating a DataFrame from the computed SPICE scores and saving it as a CSV file\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/Custom_SPICE_Scores.csv', index=False)\n",
        "\n",
        "# Computing and displaying the average SPICE scores across all evaluated captions\n",
        "averages = final_df.mean(numeric_only=True)\n",
        "print(\"Average Custom SPICE Scores:\\n\", averages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBsuugwidNoy",
        "outputId": "0556090c-133b-450f-f916-0ccf88c88177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Custom SPICE Scores:\n",
            " spice_blip    0.542584\n",
            "spice_vit     0.427803\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating captions from gemini-1.5-flash"
      ],
      "metadata": {
        "id": "mg06_Rh3bJ6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the Gemini API by setting up the authentication key\n",
        "GOOGLE_API_KEY = \"AIzaSyBo_KdiOjai53nGeBylfcJzRI_3rNctiuk\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Defining configuration settings for the caption generation process\n",
        "CONFIG = {\n",
        "    \"image_folder\": \"/content/drive/MyDrive/sd_2_outputs\",  # Specifying the directory where images are stored\n",
        "    \"csv_output_path\": \"/content/drive/MyDrive/gemini_gen_captions.csv\",  # Setting the output file path for captions\n",
        "    \"resize_dim\": (512, 512),  # Resizing images to uniform dimensions for processing\n",
        "    \"retries\": 3,  # Setting the number of retries for failed API requests\n",
        "    \"base_delay\": 1,  # Defining the base delay in seconds before retrying failed requests\n",
        "    \"request_delay\": 10,  # Ensuring a fixed delay between API requests to respect rate limits\n",
        "    \"save_interval\": 5,  # Saving the progress after processing every N images\n",
        "    \"prompt\": \"Generate a single line caption describing what's in the image using fewer words. Keep it short and simple, and generate only the caption.\"\n",
        "}\n",
        "\n",
        "# Initializing the Gemini model that is being used for caption generation\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "# Defining a function that is loading existing caption data if available\n",
        "def load_existing_data():\n",
        "    \"\"\"Loading previously processed captions to avoid re-processing images\"\"\"\n",
        "    if os.path.exists(CONFIG[\"csv_output_path\"]):  # Checking if the CSV file already exists\n",
        "        df_existing = pd.read_csv(CONFIG[\"csv_output_path\"])  # Reading existing captions\n",
        "        processed_images = set(df_existing[\"image_id\"])  # Extracting IDs of images already processed\n",
        "        captions_data = df_existing.to_dict(\"records\")  # Converting existing data to a dictionary format\n",
        "    else:\n",
        "        processed_images = set()  # Initializing an empty set if no previous data exists\n",
        "        captions_data = []  # Initializing an empty list for new captions\n",
        "    return processed_images, captions_data\n",
        "\n",
        "# Defining a function that is generating a caption for a given image\n",
        "def generate_caption(image_path):\n",
        "    \"\"\"Generating a single-line caption describing the content of the image\"\"\"\n",
        "    for attempt in range(CONFIG[\"retries\"]):  # Attempting multiple retries in case of failure\n",
        "        try:\n",
        "            with Image.open(image_path) as img:  # Opening the image for processing\n",
        "                img = img.convert(\"RGB\")  # Ensuring the image is in RGB format\n",
        "                img = img.resize(CONFIG[\"resize_dim\"])  # Resizing the image before sending it to the API\n",
        "\n",
        "                # Sending the image and prompt to Gemini API for caption generation\n",
        "                response = model.generate_content(\n",
        "                    [img, CONFIG[\"prompt\"]],\n",
        "                    generation_config={\"temperature\": 0.2}  # Setting temperature to control response randomness\n",
        "                )\n",
        "\n",
        "                # Checking if the API response contains a valid caption and returning it\n",
        "                if response.text:\n",
        "                    return response.text.strip()\n",
        "                return \"No caption generated\"  # Returning a default message if caption generation fails\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAttempt {attempt + 1} failed: {str(e)}\")  # Logging error messages\n",
        "            if attempt < CONFIG[\"retries\"] - 1:  # Implementing exponential backoff for retries\n",
        "                time.sleep(CONFIG[\"base_delay\"] * (2 ** attempt))  # Waiting before retrying\n",
        "            else:\n",
        "                return f\"Error: {str(e)}\"  # Returning an error message if all retry attempts fail\n",
        "\n",
        "# Defining the main function that is processing images and generating captions\n",
        "def main():\n",
        "    \"\"\"Executing the main workflow for image caption generation\"\"\"\n",
        "    processed_images, captions_data = load_existing_data()  # Loading previously processed data\n",
        "    image_files = [f for f in os.listdir(CONFIG[\"image_folder\"])  # Listing image files in the folder\n",
        "                  if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]  # Filtering for valid image formats\n",
        "\n",
        "    # Calculating the number of images to be processed\n",
        "    total_images = len(image_files)\n",
        "    processed_count = len(processed_images)\n",
        "    remaining_images = total_images - processed_count\n",
        "\n",
        "    # Printing summary of images found and processing status\n",
        "    print(f\"Found {total_images} images total\")\n",
        "    print(f\"{processed_count} already processed\")\n",
        "    print(f\"{remaining_images} remaining to process\\n\")\n",
        "\n",
        "    # Creating a progress bar to track the processing of images\n",
        "    progress_bar = tqdm(\n",
        "        [img for img in image_files if img not in processed_images],\n",
        "        desc=\"Processing images\",\n",
        "        unit=\"image\"\n",
        "    )\n",
        "\n",
        "    # Iterating through each unprocessed image for caption generation\n",
        "    for i, image_name in enumerate(progress_bar):\n",
        "        image_path = os.path.join(CONFIG[\"image_folder\"], image_name)  # Constructing image file path\n",
        "\n",
        "        # Updating the progress bar description with the current image being processed\n",
        "        progress_bar.set_description(f\"Processing {image_name[:20]}...\")\n",
        "\n",
        "        # Generating the caption using the defined function\n",
        "        caption = generate_caption(image_path)\n",
        "\n",
        "        # Storing the generated caption along with the image ID\n",
        "        captions_data.append({\n",
        "            \"image_id\": image_name,\n",
        "            \"gemini_caption\": caption\n",
        "        })\n",
        "\n",
        "        # Saving the progress periodically based on the defined interval\n",
        "        if (i + 1) % CONFIG[\"save_interval\"] == 0:\n",
        "            pd.DataFrame(captions_data).to_csv(CONFIG[\"csv_output_path\"], index=False)\n",
        "            progress_bar.set_postfix({\"Saved\": \"✓\"})  # Indicating progress is saved\n",
        "\n",
        "        # Respecting API rate limits by adding a delay before the next request\n",
        "        time.sleep(CONFIG[\"request_delay\"])\n",
        "\n",
        "    # Saving the final set of generated captions\n",
        "    pd.DataFrame(captions_data).to_csv(CONFIG[\"csv_output_path\"], index=False)\n",
        "\n",
        "    # Printing completion message with final stats\n",
        "    print(f\"\\n✅ Completed processing all images\")\n",
        "    print(f\"Total processed: {len(captions_data)}\")\n",
        "    print(f\"Saved to: {CONFIG['csv_output_path']}\")\n",
        "\n",
        "# Executing the main function when the script is run\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "7Amk3--4PkYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "csv_path = \"/content/drive/MyDrive/gemini_gen_captions.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Extract numeric part of image_id and convert to integer for sorting\n",
        "df['numeric_id'] = df['image_id'].str.extract(r'(\\d+)').astype(int)\n",
        "\n",
        "# Sort by numeric_id in ascending order\n",
        "df_sorted = df.sort_values(by=\"numeric_id\", ascending=True).drop(columns=['numeric_id'])\n",
        "\n",
        "# Save the sorted CSV as a new file\n",
        "sorted_csv_path = \"/content/drive/MyDrive/gemini_sorted_images_captions.csv\"\n",
        "df_sorted.to_csv(sorted_csv_path, index=False)\n",
        "\n",
        "print(f\"Sorted CSV saved successfully at {sorted_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S5NImraREjB",
        "outputId": "72e45bcd-1849-457c-c568-7c419f01596a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted CSV saved successfully at /content/drive/MyDrive/gemini_sorted_images_captions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the various metrics between the original prompts and the gemini generated ones"
      ],
      "metadata": {
        "id": "r4JUbZDcbf6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading CSV files that contain Gemini-generated captions and their corresponding prompts\n",
        "captions_df = pd.read_csv('/content/drive/MyDrive/gemini_sorted_images_captions.csv')  # Gemini captions file\n",
        "prompts_df = pd.read_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/Prompts.csv')  # Reference prompts\n",
        "\n",
        "# Initializing tools used for various text similarity evaluations\n",
        "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)  # Setting up ROUGE scorer\n",
        "smoothing = SmoothingFunction()  # Initializing the smoothing function for BLEU score computation\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')  # Loading pre-trained SentenceTransformer model\n",
        "\n",
        "# Initializing the lemmatizer for text preprocessing and setting up the stopword removal set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Defining a function that is computing the semantic similarity between a prompt and generated caption\n",
        "def compute_semantic_similarity(prompt, caption):\n",
        "    # Encoding both prompt and caption into numerical vector representations\n",
        "    prompt_embedding = model.encode(prompt, convert_to_tensor=True)\n",
        "    caption_embedding = model.encode(caption, convert_to_tensor=True)\n",
        "\n",
        "    # Calculating cosine similarity between the two vectors to assess semantic closeness\n",
        "    return util.cos_sim(prompt_embedding, caption_embedding).item()\n",
        "\n",
        "# Defining a function that is computing evaluation metrics between a prompt and its corresponding caption\n",
        "def compute_scores(prompt, caption):\n",
        "    # Ensuring inputs are converted to strings if necessary\n",
        "    if not isinstance(prompt, str): prompt = str(prompt)\n",
        "    if not isinstance(caption, str): caption = str(caption)\n",
        "\n",
        "    # Tokenizing the prompt and caption for BLEU and METEOR score calculations\n",
        "    tokenized_prompt = word_tokenize(prompt)\n",
        "    tokenized_caption = word_tokenize(caption)\n",
        "\n",
        "    # Computing the BLEU score using a smoothing function for better readability\n",
        "    bleu = sentence_bleu([tokenized_prompt], tokenized_caption, smoothing_function=smoothing.method1)\n",
        "\n",
        "    # Computing the METEOR score, which considers synonyms and paraphrases for evaluation\n",
        "    meteor = meteor_score([tokenized_prompt], tokenized_caption)\n",
        "\n",
        "    # Computing the ROUGE score to measure text overlap between the prompt and generated caption\n",
        "    rouge_scores = rouge.score(prompt, caption)\n",
        "\n",
        "    # Computing semantic similarity between the prompt and caption\n",
        "    semantic_similarity = compute_semantic_similarity(prompt, caption)\n",
        "\n",
        "    return bleu, meteor, rouge_scores['rouge1'].fmeasure, semantic_similarity\n",
        "\n",
        "# Merging the captions dataset with the prompts dataset based on the image ID\n",
        "merged_df = captions_df.merge(prompts_df, on='image_id')\n",
        "\n",
        "# Initializing an empty list for storing evaluation results\n",
        "results = []\n",
        "\n",
        "# Iterating through each row in the merged dataset to compute evaluation scores for Gemini captions\n",
        "for _, row in merged_df.iterrows():\n",
        "    prompt = row['Prompts']\n",
        "    gemini_caption = row['gemini_caption']\n",
        "\n",
        "    # Computing BLEU, METEOR, ROUGE, and Semantic Similarity scores\n",
        "    bleu_gemini, meteor_gemini, rouge_gemini, semantic_gemini = compute_scores(prompt, gemini_caption)\n",
        "\n",
        "    # Storing computed evaluation scores in a structured format\n",
        "    results.append({\n",
        "        'image_id': row['image_id'],\n",
        "        'bleu_gemini': bleu_gemini,\n",
        "        'meteor_gemini': meteor_gemini,\n",
        "        'rouge_gemini': rouge_gemini,\n",
        "        'semantic_gemini': semantic_gemini\n",
        "    })\n",
        "\n",
        "# Creating a DataFrame from the results and saving it as a CSV file for further analysis\n",
        "final_df = pd.DataFrame(results)\n",
        "final_df.to_csv('/content/drive/MyDrive/Fine-Grained-Hallucination-main/gemini_scores.csv', index=False)\n",
        "\n",
        "# Computing and printing the average scores across all evaluated Gemini captions\n",
        "averages = final_df.mean(numeric_only=True)\n",
        "print(\"Average Scores:\\n\", averages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSoFBpbfRnQ5",
        "outputId": "f081bf4b-48f6-441b-9738-1d9d05e55bf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Scores:\n",
            " bleu_gemini        0.048525\n",
            "meteor_gemini      0.260639\n",
            "rouge_gemini       0.325015\n",
            "semantic_gemini    0.578047\n",
            "dtype: float64\n"
          ]
        }
      ]
    }
  ]
}