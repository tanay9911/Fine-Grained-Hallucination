{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-EG2QQSE0AG"
      },
      "outputs": [],
      "source": [
        "!pip install openai pandas aiohttp tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CL59pD3U0ZHt",
        "outputId": "0452ffd9-8f78-4c68-ddfe-5947271b4a9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADwXxcvUDlzq",
        "outputId": "76145a85-5ad2-48a9-c1b4-ea855191c1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 200 triples.\n",
            "Already done: 20\n",
            "Remaining images to process: 180\n",
            "Saved: 20\n",
            "[1/180] Processed image: 20 | Total saved: 1\n",
            "Saved: 21\n",
            "[2/180] Processed image: 21 | Total saved: 2\n",
            "Saved: 22\n",
            "[3/180] Processed image: 22 | Total saved: 3\n",
            "Saved: 23\n",
            "[4/180] Processed image: 23 | Total saved: 4\n",
            "Saved: 24\n",
            "[5/180] Processed image: 24 | Total saved: 5\n",
            "Saved: 25\n",
            "[6/180] Processed image: 25 | Total saved: 6\n",
            "Saved: 26\n",
            "[7/180] Processed image: 26 | Total saved: 7\n",
            "Saved: 27\n",
            "[8/180] Processed image: 27 | Total saved: 8\n",
            "Saved: 28\n",
            "[9/180] Processed image: 28 | Total saved: 9\n",
            "Saved: 29\n",
            "[10/180] Processed image: 29 | Total saved: 10\n",
            "Saved: 30\n",
            "[11/180] Processed image: 30 | Total saved: 11\n"
          ]
        }
      ],
      "source": [
        "# --- Step 3: Setup ---\n",
        "import os\n",
        "import base64\n",
        "import json\n",
        "import asyncio\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from openai import AsyncOpenAI\n",
        "import time\n",
        "import nest_asyncio\n",
        "\n",
        "# Patch asyncio for Jupyter/Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# GitHub token\n",
        "os.environ[\"GITHUB_TOKEN\"] = \"\"\n",
        "\n",
        "# Instantiate client with GitHub endpoint\n",
        "client = AsyncOpenAI(\n",
        "    base_url=\"https://models.github.ai/inference\",\n",
        "    api_key=os.environ[\"GITHUB_TOKEN\"],\n",
        ")\n",
        "\n",
        "MODEL = \"openai/gpt-4o-mini\"\n",
        "\n",
        "# Rate limits\n",
        "RPM = 15        # requests per minute\n",
        "RPD = 150       # requests per day\n",
        "CONCURRENT = 5  # max concurrent requests\n",
        "\n",
        "# --- Step 4: Utils & G-VEval style prompt ---\n",
        "def encode_image(image_path: Path):\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "async def gveval_score_async(prompt_text: str, image_path: Path, meta_caption: str):\n",
        "    image_b64 = encode_image(image_path)\n",
        "\n",
        "    system_prompt = \"\"\"You are a caption evaluation module following G-VEval from AAAI 2025.\n",
        "You will be given:\n",
        "- A reference caption.\n",
        "- A candidate caption (meta-caption) generated from the image.\n",
        "- The image itself.\n",
        "\n",
        "You must produce:\n",
        "- Four scores (Accuracy, Completeness, Conciseness, Relevance), each from 0 to 100.\n",
        "- An overall score from 0 to 100.\n",
        "- A short reasoning explaining the main strengths & weaknesses.\n",
        "\n",
        "Return strictly JSON, exactly this structure:\n",
        "\n",
        "{\n",
        "  \"accuracy\": <int 0-100>,\n",
        "  \"completeness\": <int 0-100>,\n",
        "  \"conciseness\": <int 0-100>,\n",
        "  \"relevance\": <int 0-100>,\n",
        "  \"overall\": <int 0-100>,\n",
        "  \"reason\": \"<short explanation>\"\n",
        "}\"\"\"\n",
        "\n",
        "    user_prompt = f\"Reference: {prompt_text}\\nCandidate: {meta_caption}\"\n",
        "\n",
        "    resp = await client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": user_prompt},\n",
        "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{image_b64}\"}}\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=400,\n",
        "    )\n",
        "\n",
        "    return resp.choices[0].message.content\n",
        "\n",
        "def safe_append_jsonl(out_path: str, record: dict):\n",
        "    \"\"\"Append record to jsonl safely (flush/sync) without overwriting previous content.\"\"\"\n",
        "    with open(out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "async def process_triple(triple, out_path: str):\n",
        "    img_name, prompt, meta_caption, img_path = triple\n",
        "    try:\n",
        "        json_result = await gveval_score_async(prompt, img_path, meta_caption)\n",
        "        record = {\n",
        "            \"image_name\": img_name,\n",
        "            \"result\": json.loads(json_result) if isinstance(json_result, str) else json_result\n",
        "        }\n",
        "        safe_append_jsonl(out_path, record)\n",
        "        print(f\"Saved: {img_name}\")\n",
        "        return record\n",
        "    except Exception as e:\n",
        "        print(f\"Error for {img_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "async def run_eval(triples, out_path: str):\n",
        "    sem = asyncio.Semaphore(CONCURRENT)\n",
        "    start_time = time.time()\n",
        "    completed = 0\n",
        "    results = []\n",
        "\n",
        "    total = len(triples)\n",
        "    for i, triple in enumerate(triples, start=1):\n",
        "        async with sem:\n",
        "            # enforce per-minute limit\n",
        "            elapsed = time.time() - start_time\n",
        "            if completed >= RPM and elapsed < 60:\n",
        "                await asyncio.sleep(60 - elapsed)\n",
        "                completed = 0\n",
        "                start_time = time.time()\n",
        "            completed += 1\n",
        "            rec = await process_triple(triple, out_path)\n",
        "            if rec:\n",
        "                results.append(rec)\n",
        "            # verbose progress\n",
        "            print(f\"[{i}/{total}] Processed image: {triple[0]} | Total saved: {len(results)}\")\n",
        "            if len(results) >= RPD:\n",
        "                print(\"Hit daily cap, stopping for today.\")\n",
        "                break\n",
        "    return results\n",
        "\n",
        "# --- Step 5: Load triples ---\n",
        "def load_triples(prompt_csv: str, meta_csv: str, image_folder: str):\n",
        "    df_prompts = pd.read_csv(prompt_csv)\n",
        "    df_meta = pd.read_csv(meta_csv)\n",
        "    triples = []\n",
        "    for _, row in df_meta.iterrows():\n",
        "        img_name = str(row[\"image_name\"]).strip()\n",
        "        prompt = row[\"Prompts\"]\n",
        "        meta_caption = row.get(\"Meta Caption\", \"\")\n",
        "        if not isinstance(meta_caption, str) or meta_caption.strip() == \"\":\n",
        "            continue\n",
        "        # find image\n",
        "        img_path = None\n",
        "        for ext in [\".png\", \".jpg\", \".jpeg\"]:\n",
        "            candidate = Path(image_folder) / f\"{img_name}{ext}\"\n",
        "            if candidate.exists():\n",
        "                img_path = candidate\n",
        "                break\n",
        "        if img_path is None:\n",
        "            continue\n",
        "        triples.append((img_name, prompt, meta_caption, img_path))\n",
        "    return triples\n",
        "\n",
        "# --- Step 6: Run for SD2 model ---\n",
        "base = \"/content/drive/MyDrive/gveval\"\n",
        "prompt_csv = f\"{base}/DrawBenchPrompts.csv\"\n",
        "meta_csv = f\"{base}/meta_captions_sd2.csv\"\n",
        "image_folder = f\"{base}/sd2\"\n",
        "out_file = f\"{base}/results_sd2_gveval.jsonl\"\n",
        "\n",
        "# Prepare data\n",
        "triples = load_triples(prompt_csv, meta_csv, image_folder)\n",
        "print(f\"Loaded {len(triples)} triples.\")\n",
        "\n",
        "# Load already processed IDs and **filter triples before sending to GPT**\n",
        "done_ids = set()\n",
        "if Path(out_file).exists():\n",
        "    with open(out_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            try:\n",
        "                rec = json.loads(line)\n",
        "                done_ids.add(rec[\"image_name\"])\n",
        "            except:\n",
        "                pass\n",
        "print(f\"Already done: {len(done_ids)}\")\n",
        "\n",
        "# Filter out already processed triples\n",
        "triples_to_process = [t for t in triples if t[0] not in done_ids]\n",
        "print(f\"Remaining images to process: {len(triples_to_process)}\")\n",
        "\n",
        "# Run evaluation (Colab-safe)\n",
        "loop = asyncio.get_event_loop()\n",
        "results = loop.run_until_complete(run_eval(triples_to_process, out_file))\n",
        "print(\"Evaluation completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
