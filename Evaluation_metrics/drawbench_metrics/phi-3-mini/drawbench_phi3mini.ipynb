{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ===== Paths =====\n",
        "base_path = \"/content/drive/MyDrive/\"\n",
        "ref_csv = base_path + \"DrawBenchPrompts.csv\"\n",
        "model_files = [\"meta_captions_Flux-Dev.csv\", \"meta_captions_sd_2.csv\", \"meta_captions_sdxl.csv\"]\n",
        "\n",
        "# ===== Load reference =====\n",
        "ref_df = pd.read_csv(ref_csv)\n",
        "\n",
        "for model_file in model_files:\n",
        "    model_path = base_path + model_file\n",
        "    model_df = pd.read_csv(model_path)\n",
        "\n",
        "    # Find missing image_names\n",
        "    missing_images = list(set(ref_df[\"image_name\"]) - set(model_df[\"image_name\"]))\n",
        "    print(f\"{model_file}: {len(missing_images)} missing images\")\n",
        "    if missing_images:\n",
        "        print(\"Missing image_names:\", missing_images)\n",
        "\n",
        "        # Create new rows for missing images\n",
        "        new_rows = ref_df[ref_df[\"image_name\"].isin(missing_images)].copy()\n",
        "        new_rows[\"Meta Caption\"] = \"\"  # empty caption\n",
        "        # Keep other columns same as ref_df\n",
        "        model_df = pd.concat([model_df, new_rows[model_df.columns]], ignore_index=True)\n",
        "\n",
        "    # Optional: sort by image_name to match reference\n",
        "    model_df = model_df.set_index(\"image_name\").reindex(ref_df[\"image_name\"]).reset_index()\n",
        "\n",
        "    # Save back\n",
        "    model_df.to_csv(model_path, index=False)\n",
        "    print(f\"{model_file} updated with missing rows.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbZlGsxqTYWu",
        "outputId": "971fa36d-b69f-4ca6-dcc9-16716cd24ed1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "meta_captions_Flux-Dev.csv: 29 missing images\n",
            "Missing image_names: [8, 9, 168, 67, 70, 72, 73, 74, 75, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 98, 99, 109]\n",
            "meta_captions_Flux-Dev.csv updated with missing rows.\n",
            "\n",
            "meta_captions_sd_2.csv: 0 missing images\n",
            "meta_captions_sd_2.csv updated with missing rows.\n",
            "\n",
            "meta_captions_sdxl.csv: 0 missing images\n",
            "meta_captions_sdxl.csv updated with missing rows.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# ===============================\n",
        "# 1. Load Phi-3-mini\n",
        "# ===============================\n",
        "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
        "\n",
        "print(\"ðŸ”¹ Loading Phi-3-mini model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 2. Function to extract noun phrases\n",
        "# ===============================\n",
        "def get_entities(caption: str):\n",
        "    \"\"\"Return a list of noun phrases for a single caption.\"\"\"\n",
        "    if not caption or caption.strip() == \"\":\n",
        "        return []\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": (\n",
        "            \"Extract all the noun phrases in the given sentence. \"\n",
        "            \"Return them separated by commas, without rephrasing or extra text. \"\n",
        "            \"Only keep phrases that contain a noun. \"\n",
        "            f\"\\nSentence: {caption}\\nEntities:\"\n",
        "        )}\n",
        "    ]\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 50,\n",
        "        \"return_full_text\": False,\n",
        "        \"temperature\": 0.0,\n",
        "        \"do_sample\": False\n",
        "    }\n",
        "    try:\n",
        "        output = pipe(messages, **generation_args)\n",
        "        text = output[0]['generated_text'].strip()\n",
        "        entities = [ent.strip() for ent in text.split(\",\") if ent.strip()]\n",
        "        return entities\n",
        "    except Exception as e:\n",
        "        print(\"Error processing caption:\", caption, e)\n",
        "        return []\n",
        "\n",
        "# ===============================\n",
        "# 3. Function to process a single CSV\n",
        "# ===============================\n",
        "def process_csv(file_path: str, caption_column: str):\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"ðŸ”¹ Processing {file_path} ({len(df)} rows)\")\n",
        "\n",
        "    entities_list = []\n",
        "\n",
        "    # Parallel processing using ThreadPoolExecutor\n",
        "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
        "        futures = {executor.submit(get_entities, str(caption)): i for i, caption in enumerate(df[caption_column])}\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Extracting entities\"):\n",
        "            idx = futures[future]\n",
        "            try:\n",
        "                entities_list.append((idx, future.result()))\n",
        "            except Exception as e:\n",
        "                entities_list.append((idx, []))\n",
        "                print(\"Error at row\", idx, e)\n",
        "\n",
        "    # Sort back to original order\n",
        "    entities_list.sort(key=lambda x: x[0])\n",
        "    df[f\"{caption_column}_entities\"] = [e for _, e in entities_list]\n",
        "\n",
        "    out_csv = file_path.replace(\".csv\", \"_entities.csv\")\n",
        "    df.to_csv(out_csv, index=False)\n",
        "    print(f\"Saved entities to {out_csv}\")\n",
        "\n",
        "# ===============================\n",
        "# 4. Run for all files\n",
        "# ===============================\n",
        "base_path = \"/content/drive/MyDrive/\"\n",
        "\n",
        "files_to_process = {\n",
        "    \"Prompts\": base_path + \"DrawBenchPrompts.csv\",\n",
        "    \"Flux-Dev\": base_path + \"meta_captions_Flux-Dev.csv\",\n",
        "    \"sd_2\": base_path + \"meta_captions_sd_2.csv\",\n",
        "    \"sdxl\": base_path + \"meta_captions_sdxl.csv\"\n",
        "}\n",
        "\n",
        "for col_name, file_path in files_to_process.items():\n",
        "    # Determine caption column\n",
        "    caption_col = \"Prompts\" if col_name == \"Prompts\" else \"Meta Caption\"\n",
        "    process_csv(file_path, caption_col)\n"
      ],
      "metadata": {
        "id": "NzNqshvtVCJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}