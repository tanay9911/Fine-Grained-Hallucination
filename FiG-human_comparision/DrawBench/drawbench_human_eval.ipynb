{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPJjSfLVuCZI",
        "outputId": "ad2253bd-10cf-43df-be2f-03f6651ac3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved -> drawbench_all_models_fig_detailed.csv\n",
            "Saved -> drawbench_avg_fig_consistency_per_model.csv\n",
            "      Model  Object FiG_Consistency  Colour FiG_Consistency  \\\n",
            "0  Flux-Dev                    0.75                   0.000   \n",
            "1       SD2                    0.95                   0.500   \n",
            "2      SDXL                    0.90                   0.667   \n",
            "\n",
            "   Number FiG_Consistency  Positional FiG_Consistency  Text FiG_Consistency  \n",
            "0                   0.562                       0.800                  1.00  \n",
            "1                   0.286                       1.000                  1.00  \n",
            "2                   0.357                       0.909                  0.75  \n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "#          IMPORTS\n",
        "# ==============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from typing import Set, Dict\n",
        "import re\n",
        "\n",
        "# ==============================\n",
        "#        LOAD SPACY MODEL\n",
        "# ==============================\n",
        "try:\n",
        "    # Loading the English language model for NLP processing\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # Downloading the model if it's not found locally\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Merging entities to treat multi-word entities as single tokens\n",
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "# ==============================\n",
        "#        FINE-GRAINED METRICS\n",
        "# ==============================\n",
        "class FineGrainedMetrics:\n",
        "    \"\"\"\n",
        "    Defining a class that is calculating fine-grained similarity metrics\n",
        "    between original and generated captions\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def related_to_noun(doc, attribute: str, noun: str) -> bool:\n",
        "        \"\"\"\n",
        "        Checking if a specific attribute is grammatically related to a given noun in the document\n",
        "        by examining the noun's syntactic subtree\n",
        "        \"\"\"\n",
        "        for token in doc:\n",
        "            if token.text == noun:\n",
        "                # Looking for the attribute within the noun's grammatical subtree\n",
        "                if attribute in [t.text for t in token.subtree]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @classmethod\n",
        "    def color(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculating color attribute matching score by finding color adjectives\n",
        "        that are grammatically connected to common nouns\n",
        "        \"\"\"\n",
        "        # Defining a comprehensive set of color terms\n",
        "        COLORS = {'red','blue','green','yellow','black','white','gray','grey',\n",
        "                  'orange','pink','purple','brown','violet','indigo','turquoise',\n",
        "                  'cyan','magenta'}\n",
        "        score, original_colors = 0.0, set()\n",
        "\n",
        "        for noun in generated_nouns:\n",
        "            # Finding colors in metadata caption that are modifying this noun\n",
        "            meta_colors = {t.text for t in meta\n",
        "                           if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            # Finding colors in original caption that are modifying this noun\n",
        "            orig_colors = {t.text for t in orig\n",
        "                           if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            original_colors.update(orig_colors)\n",
        "\n",
        "            # Adding to score for each matching color\n",
        "            if orig_colors:\n",
        "                score += len(orig_colors & meta_colors)\n",
        "\n",
        "        # Returning -1 if no colors found in original, otherwise calculating precision score\n",
        "        return -1 if not original_colors else score / len(original_colors)\n",
        "\n",
        "    @classmethod\n",
        "    def number(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculating number/quantity attribute matching by finding numeric modifiers\n",
        "        and quantifiers connected to common nouns\n",
        "        \"\"\"\n",
        "        # Mapping textual quantity words to their numeric equivalents\n",
        "        QUANTITY_MAP = {'a':'1','an':'1','the':'1','one':'1','two':'2','three':'3',\n",
        "                        'couple':'2','few':'3','several':'4','many':'5','dozen':'12'}\n",
        "        score, original_numbers = 0.0, set()\n",
        "\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting numeric modifiers from metadata caption\n",
        "            meta_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in meta\n",
        "                         if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            # Extracting numeric modifiers from original caption\n",
        "            orig_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in orig\n",
        "                         if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            original_numbers.update(orig_nums)\n",
        "\n",
        "            # Adding to score for each matching number\n",
        "            if orig_nums:\n",
        "                score += len(orig_nums & meta_nums)\n",
        "\n",
        "        if not original_numbers:\n",
        "            return -1\n",
        "\n",
        "        # Calculating ratio and ensuring it doesn't exceed 1.0\n",
        "        raw_val = score / len(original_numbers)\n",
        "        return min(raw_val, 1.0)\n",
        "\n",
        "    @classmethod\n",
        "    def text(cls, meta, orig, _: Set[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculating text/quote matching score by extracting quoted text\n",
        "        when text indicators are present\n",
        "        \"\"\"\n",
        "        TEXT_INDICATORS = {'written','saying','says','reading','text'}\n",
        "        QUOTE_PATTERN = r'[\\\"\\'«»\"]([^\"\\'«»\"]*)[\\\"\\'«»\"]'\n",
        "\n",
        "        # Checking if original caption contains text indicators\n",
        "        if any(t.text in TEXT_INDICATORS for t in orig):\n",
        "            # Extracting all quoted text from both captions\n",
        "            orig_matches = re.findall(QUOTE_PATTERN, orig.text)\n",
        "            meta_matches = re.findall(QUOTE_PATTERN, meta.text)\n",
        "\n",
        "            if not orig_matches:\n",
        "                return -1\n",
        "\n",
        "            # Normalizing text by removing spaces and converting to lowercase\n",
        "            orig_norm = [''.join(s.lower().split()) for s in orig_matches]\n",
        "            meta_norm = [''.join(s.lower().split()) for s in meta_matches]\n",
        "\n",
        "            # Counting how many original quotes appear in metadata quotes\n",
        "            matches = sum(any(o in m for m in meta_norm) for o in orig_norm)\n",
        "            return matches / len(orig_matches)\n",
        "\n",
        "        return -1\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_spatial_relations(doc):\n",
        "        \"\"\"\n",
        "        Extracting spatial relationships in the form of (subject, preposition, object) tuples\n",
        "        to understand positional information\n",
        "        \"\"\"\n",
        "        rels = set()\n",
        "        for token in doc:\n",
        "            if token.dep_ == 'prep':  # Looking for preposition tokens\n",
        "                # Finding prepositional objects\n",
        "                pobjects = [child for child in token.children if child.dep_ == 'pobj']\n",
        "                if pobjects:\n",
        "                    pobj = pobjects[0].text\n",
        "                    subj = None\n",
        "                    # Finding the subject related to this preposition\n",
        "                    for anc in token.ancestors:\n",
        "                        if anc.dep_ in {'nsubj','nsubjpass','dobj','pobj'}:\n",
        "                            subj = anc.text\n",
        "                            break\n",
        "                    if subj:\n",
        "                        # Storing the spatial relation triple\n",
        "                        rels.add((subj, token.text, pobj))\n",
        "        return rels\n",
        "\n",
        "    @classmethod\n",
        "    def position(cls, meta, orig, _: Set[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calculating positional relationship matching score by comparing\n",
        "        spatial relation tuples between captions\n",
        "        \"\"\"\n",
        "        orig_rel = cls.extract_spatial_relations(orig)\n",
        "        meta_rel = cls.extract_spatial_relations(meta)\n",
        "\n",
        "        # Returning -1 if no spatial relations in original, otherwise calculating overlap ratio\n",
        "        return -1 if not orig_rel else len(orig_rel & meta_rel) / len(orig_rel)\n",
        "\n",
        "# ==============================\n",
        "#        ANALYZE CAPTION PAIR\n",
        "# ==============================\n",
        "def analyze_caption_pair(meta_caption: str, orig_caption: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Analyzing a pair of captions (metadata vs original) and calculating\n",
        "    multiple fine-grained similarity metrics\n",
        "    \"\"\"\n",
        "    # Handling NaN values by converting to empty strings\n",
        "    meta_caption = str(meta_caption) if pd.notna(meta_caption) else \"\"\n",
        "    orig_caption = str(orig_caption) if pd.notna(orig_caption) else \"\"\n",
        "\n",
        "    # Processing both captions with spaCy NLP pipeline\n",
        "    meta_doc, orig_doc = nlp(meta_caption), nlp(orig_caption)\n",
        "\n",
        "    # Extracting nouns and proper nouns from both captions\n",
        "    meta_nouns = {t.text for t in meta_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    orig_nouns = {t.text for t in orig_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "\n",
        "    # Finding common nouns between both captions\n",
        "    common_nouns = orig_nouns & meta_nouns\n",
        "\n",
        "    # Calculating noun recall (object-level similarity)\n",
        "    noun_recall = len(common_nouns) / len(orig_nouns) if orig_nouns else 0\n",
        "\n",
        "    # Returning dictionary of all fine-grained metrics\n",
        "    return {\n",
        "        \"Object FiG\": noun_recall,\n",
        "        \"Colour FiG\": FineGrainedMetrics.color(meta_doc, orig_doc, common_nouns),\n",
        "        \"Number FiG\": FineGrainedMetrics.number(meta_doc, orig_doc, common_nouns),\n",
        "        \"Positional FiG\": FineGrainedMetrics.position(meta_doc, orig_doc, common_nouns),\n",
        "        \"Text FiG\": FineGrainedMetrics.text(meta_doc, orig_doc, common_nouns)\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "#       PROCESS MODEL CSV\n",
        "# ==============================\n",
        "def process_model_csv(file_path: str):\n",
        "    \"\"\"\n",
        "    Processing a CSV file containing model outputs and calculating\n",
        "    fine-grained metrics for each caption pair\n",
        "    \"\"\"\n",
        "    # Reading CSV and removing rows with missing prompts or metadata captions\n",
        "    df = pd.read_csv(file_path).dropna(subset=[\"Prompts\", \"Meta Caption\"]).reset_index(drop=True)\n",
        "\n",
        "    # Analyzing each caption pair and storing results\n",
        "    results = [analyze_caption_pair(row[\"Meta Caption\"], row[\"Prompts\"]) for _, row in df.iterrows()]\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "\n",
        "    # Combining original data with calculated metrics\n",
        "    return pd.concat([df, metrics_df], axis=1)\n",
        "\n",
        "# ==============================\n",
        "#      PARSE HUMAN RESPONSES\n",
        "# ==============================\n",
        "def parse_human_responses(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parsing human evaluation data from Excel file and extracting\n",
        "    alignment judgments from annotators\n",
        "    \"\"\"\n",
        "    df = pd.read_excel(file_path, header=None)\n",
        "    rows = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Parsing the caption information cell\n",
        "        caption_cell = str(df.iloc[0, col])\n",
        "        match = re.match(r\"\\[(.*?)\\] Image: (\\d+) \\| Prompt: (.*)\", caption_cell)\n",
        "        if not match:\n",
        "            continue\n",
        "\n",
        "        model, image, caption = match.groups()\n",
        "        human_labels = []\n",
        "\n",
        "        # Processing responses from two annotators\n",
        "        for annot_idx in range(1, 3):\n",
        "            resp_cell = str(df.iloc[annot_idx, col]).strip()\n",
        "            # Determining if response indicates alignment\n",
        "            human_label = \"Yes\" if resp_cell.startswith(\"Yes\") else \"No\"\n",
        "            # Extracting which aspects are not aligning (for \"No\" responses)\n",
        "            not_aligning = \", \".join(re.findall(r\"(Color|Position|Text|Number|Object|Others)\", resp_cell)) if human_label==\"No\" else \"\"\n",
        "            human_labels.append(f\"{human_label} ({not_aligning})\" if not_aligning else human_label)\n",
        "\n",
        "        rows.append({\n",
        "            \"Model\": model,\n",
        "            \"Image\": image,\n",
        "            \"Caption\": caption,\n",
        "            \"Human_Responses\": \"; \".join(human_labels)\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ==============================\n",
        "#          HUMAN MAJORITY\n",
        "# ==============================\n",
        "def compute_human_majority(human_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computing majority vote from human annotators for each caption evaluation\n",
        "    \"\"\"\n",
        "    def majority_vote(responses):\n",
        "        # Counting \"Yes\" responses and taking majority (>=2 out of 3)\n",
        "        return 'Yes' if sum([r.startswith('Yes') for r in responses.split(\"; \")]) >= 2 else 'No'\n",
        "\n",
        "    human_df['Human_Majority'] = human_df['Human_Responses'].apply(majority_vote)\n",
        "    return human_df\n",
        "\n",
        "def merge_with_human(model_df: pd.DataFrame, human_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Merging model output data with human evaluation data and calculating\n",
        "    consistency between fine-grained metrics and human judgments\n",
        "    \"\"\"\n",
        "    # Ensuring consistent data types for merging\n",
        "    model_df['image_name'] = model_df['image_name'].astype(str)\n",
        "    human_df['Image'] = human_df['Image'].astype(str)\n",
        "\n",
        "    # Merging model data with human evaluation data\n",
        "    merged = pd.merge(\n",
        "        model_df,\n",
        "        human_df[human_df['Model']==model_name][['Image','Caption','Human_Responses','Human_Majority']],\n",
        "        left_on=['image_name','Prompts'],\n",
        "        right_on=['Image','Caption'],\n",
        "        how='left'\n",
        "    ).drop(columns=['Image','Caption'])\n",
        "\n",
        "    # Defining the fine-grained metric columns\n",
        "    fig_columns = ['Object FiG','Colour FiG','Number FiG','Positional FiG','Text FiG']\n",
        "\n",
        "    def check_consistency(row):\n",
        "        \"\"\"\n",
        "        Checking consistency between fine-grained metrics and human majority vote:\n",
        "        - If humans say \"Yes\" (aligned), expecting all FiG scores = 1\n",
        "        - If humans say \"No\" (not aligned), expecting at least one FiG score < 1\n",
        "        \"\"\"\n",
        "        consistency = {}\n",
        "        for col in fig_columns:\n",
        "            val = row.get(col, None)\n",
        "            if val == -1 or pd.isna(val):\n",
        "                # Marking as None if metric is not applicable\n",
        "                consistency[col+'_Consistency'] = None\n",
        "            else:\n",
        "                # Handling Number FiG separately since it can exceed 1.0\n",
        "                val_to_check = min(val, 1) if col == 'Number FiG' else val\n",
        "\n",
        "                if row['Human_Majority'] == 'Yes':\n",
        "                    # Expecting perfect scores for human-approved alignments\n",
        "                    consistency[col+'_Consistency'] = 1 if val_to_check == 1 else 0\n",
        "                else:\n",
        "                    # Expecting imperfect scores for human-rejected alignments\n",
        "                    consistency[col+'_Consistency'] = 1 if val_to_check < 1 else 0\n",
        "        return pd.Series(consistency)\n",
        "\n",
        "    # Applying consistency check to all rows\n",
        "    merged[[c+'_Consistency' for c in fig_columns]] = merged.apply(check_consistency, axis=1)\n",
        "    merged['Model'] = model_name\n",
        "    return merged\n",
        "\n",
        "# ==============================\n",
        "#            MAIN PIPELINE\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Executing the main analysis pipeline that processes model outputs,\n",
        "    combines with human evaluations, and generates final metrics\n",
        "    \"\"\"\n",
        "\n",
        "    # Processing fine-grained metrics for each model's outputs\n",
        "    sd2_df = process_model_csv(\"drawbench_sd2.csv\")\n",
        "    sdxl_df = process_model_csv(\"drawbench_sdxl.csv\")\n",
        "    flux_df = process_model_csv(\"drawbench_fluxdev.csv\")\n",
        "\n",
        "    # Parsing and processing human evaluation data\n",
        "    human_df = parse_human_responses(\"human_response.xlsx\")\n",
        "    human_df = compute_human_majority(human_df)\n",
        "\n",
        "    # Merging model metrics with human evaluations\n",
        "    sd2_merged = merge_with_human(sd2_df, human_df, \"SD2\")\n",
        "    sdxl_merged = merge_with_human(sdxl_df, human_df, \"SDXL\")\n",
        "    flux_merged = merge_with_human(flux_df, human_df, \"Flux-Dev\")\n",
        "\n",
        "    # Combining all model results into final dataframe\n",
        "    final_df = pd.concat([sd2_merged, sdxl_merged, flux_merged], ignore_index=True)\n",
        "    final_df.to_csv(\"drawbench_all_models_fig_detailed.csv\", index=False)\n",
        "    print(\"Saved -> drawbench_all_models_fig_detailed.csv\")\n",
        "\n",
        "    # Calculating average consistency metrics per model\n",
        "    fig_consistency_cols = ['Object FiG_Consistency','Colour FiG_Consistency','Number FiG_Consistency',\n",
        "                            'Positional FiG_Consistency','Text FiG_Consistency']\n",
        "    avg_consistency = final_df.groupby('Model')[fig_consistency_cols].mean().round(3).reset_index()\n",
        "    avg_consistency.to_csv(\"drawbench_avg_fig_consistency_per_model.csv\", index=False)\n",
        "    print(\"Saved -> drawbench_avg_fig_consistency_per_model.csv\")\n",
        "    print(avg_consistency)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==============================\n",
        "#      LOAD DETAILED CSV\n",
        "# ==============================\n",
        "df = pd.read_csv(\"drawbench_all_models_fig_detailed.csv\")  # Use your detailed CSV\n",
        "\n",
        "# ==============================\n",
        "#      LIST OF FIG COLUMNS\n",
        "# ==============================\n",
        "fig_cols = ['Object FiG','Colour FiG','Number FiG','Positional FiG','Text FiG']\n",
        "\n",
        "# Replace -1 with NaN since -1 indicates metric could not be computed\n",
        "df[fig_cols] = df[fig_cols].replace(-1, pd.NA)\n",
        "\n",
        "# ==============================\n",
        "#      COMPUTE AVERAGE FIG SCORES PER MODEL\n",
        "# ==============================\n",
        "avg_fig_scores = df.groupby('Model')[fig_cols].mean().round(3).reset_index()\n",
        "\n",
        "# ==============================\n",
        "#      SAVE TO CSV\n",
        "# ==============================\n",
        "avg_fig_scores.to_csv(\"drawbench_avg_fig_scores_per_model.csv\", index=False)\n",
        "\n",
        "# ==============================\n",
        "#      PRINT RESULTS\n",
        "# ==============================\n",
        "print(\"Average FiG scores per model saved as drawbench_avg_fig_scores_per_model.csv\")\n",
        "print(avg_fig_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9SO11Ulx71A",
        "outputId": "31d6df4a-1d91-445d-b41e-871fafca37eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average FiG scores per model saved as drawbench_avg_fig_scores_per_model.csv\n",
            "      Model  Object FiG Colour FiG Number FiG Positional FiG Text FiG\n",
            "0  Flux-Dev       0.431       0.75     0.8125            0.0     0.25\n",
            "1       SD2       0.307        0.5   0.642857            0.0      0.0\n",
            "2      SDXL       0.356        1.0   0.821429            0.0     0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "#      CHAIR SCORE FUNCTION\n",
        "# ==============================\n",
        "# CHAIR score measures object hallucination: fraction of objects in generated caption that are *not* in ground-truth\n",
        "def compute_chair(meta_caption: str, orig_caption: str) -> float:\n",
        "    meta_caption = str(meta_caption).lower()\n",
        "    orig_caption = str(orig_caption).lower()\n",
        "\n",
        "    # Extract nouns (simplified as words with letters only)\n",
        "    meta_nouns = set(re.findall(r'\\b[a-z]+\\b', meta_caption))\n",
        "    orig_nouns = set(re.findall(r'\\b[a-z]+\\b', orig_caption))\n",
        "\n",
        "    if not meta_nouns:\n",
        "        return -1  # cannot compute\n",
        "    # Objects in meta that are NOT in original (hallucinated)\n",
        "    hallucinated = meta_nouns - orig_nouns\n",
        "    return len(hallucinated) / len(meta_nouns)\n",
        "\n",
        "# ==============================\n",
        "#       PROCESS MODEL CSV\n",
        "# ==============================\n",
        "def add_chair_score(file_path: str, model_name: str):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['CHAIR'] = df.apply(lambda row: compute_chair(row['Meta Caption'], row['Prompts']), axis=1)\n",
        "    df['Model'] = model_name\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "#         MAIN\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    # Process each DrawBench model\n",
        "    sd2_df = add_chair_score(\"drawbench_sd2.csv\", \"SD2\")\n",
        "    sdxl_df = add_chair_score(\"drawbench_sdxl.csv\", \"SDXL\")\n",
        "    flux_df = add_chair_score(\"drawbench_fluxdev.csv\", \"Flux-Dev\")\n",
        "\n",
        "    # Combine all models\n",
        "    all_models_chair = pd.concat([sd2_df, sdxl_df, flux_df], ignore_index=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    all_models_chair.to_csv(\"drawbench_all_models_chair_scores.csv\", index=False)\n",
        "    print(\"CHAIR scores computed and saved to drawbench_all_models_chair_scores.csv\")\n",
        "\n",
        "    # Optional: average CHAIR per model\n",
        "    avg_chair = all_models_chair.groupby(\"Model\")['CHAIR'].mean().round(3).reset_index()\n",
        "    print(\"Average CHAIR scores per model:\")\n",
        "    print(avg_chair)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZR5v-ZFx84P",
        "outputId": "08955582-2e91-466d-db18-7dcfc952ce02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAIR scores computed and saved to drawbench_all_models_chair_scores.csv\n",
            "Average CHAIR scores per model:\n",
            "      Model  CHAIR\n",
            "0  Flux-Dev  0.858\n",
            "1       SD2  0.885\n",
            "2      SDXL  0.880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "# ==============================\n",
        "#       LOAD CSV\n",
        "# ==============================\n",
        "df = pd.read_csv(\"drawbench_all_models_fig_detailed.csv\")\n",
        "\n",
        "# ==============================\n",
        "#       FiG CATEGORIES\n",
        "# ==============================\n",
        "fig_cols = ['Object', 'Colour', 'Number', 'Positional', 'Text']\n",
        "\n",
        "# ==============================\n",
        "#   HELPER: PARSE Human_Responses\n",
        "# ==============================\n",
        "def parse_human_response(resp):\n",
        "    if pd.isna(resp) or resp.strip() == \"\":\n",
        "        return []\n",
        "    parts = [p.strip().split('(')[-1].replace(')', '').strip().lower() for p in resp.split(';')]\n",
        "    return parts\n",
        "\n",
        "# ==============================\n",
        "#   FUNCTION TO COMPUTE PRECISION / RECALL / F1\n",
        "# ==============================\n",
        "def compute_model_metrics(df, model_name, fig_cols):\n",
        "    model_df = df[df['Model'] == model_name]\n",
        "    metrics = {}\n",
        "\n",
        "    for col in fig_cols:\n",
        "        col_lower = col.lower()\n",
        "\n",
        "        # y_true: 1 = no hallucination, 0 = hallucination\n",
        "        y_true = model_df['Human_Responses'].apply(\n",
        "            lambda x: 0 if col_lower in parse_human_response(x) else 1\n",
        "        )\n",
        "\n",
        "        # y_pred: 1 = model score = 1 (perfect), 0 = anything < 1\n",
        "        y_pred = model_df[col + ' FiG_Consistency'].apply(\n",
        "            lambda x: 1 if pd.notna(x) and x == 1 else 0\n",
        "        )\n",
        "\n",
        "        # Filter out NaNs\n",
        "        mask = y_pred.notna() & y_true.notna()\n",
        "        y_true_filtered = y_true[mask]\n",
        "        y_pred_filtered = y_pred[mask]\n",
        "\n",
        "        # Compute metrics\n",
        "        metrics[col + '_Precision'] = round(\n",
        "            precision_score(y_true_filtered, y_pred_filtered, zero_division=0), 3\n",
        "        ) if len(y_true_filtered) > 0 else np.nan\n",
        "\n",
        "        metrics[col + '_Recall'] = round(\n",
        "            recall_score(y_true_filtered, y_pred_filtered, zero_division=0), 3\n",
        "        ) if len(y_true_filtered) > 0 else np.nan\n",
        "\n",
        "        metrics[col + '_F1'] = round(\n",
        "            f1_score(y_true_filtered, y_pred_filtered, zero_division=0), 3\n",
        "        ) if len(y_true_filtered) > 0 else np.nan\n",
        "\n",
        "    # Overall metrics (mean across columns)\n",
        "    metrics['Overall_Precision'] = round(np.nanmean([metrics[c+'_Precision'] for c in fig_cols]), 3)\n",
        "    metrics['Overall_Recall'] = round(np.nanmean([metrics[c+'_Recall'] for c in fig_cols]), 3)\n",
        "    metrics['Overall_F1'] = round(np.nanmean([metrics[c+'_F1'] for c in fig_cols]), 3)\n",
        "\n",
        "    metrics['Model'] = model_name\n",
        "    return metrics\n",
        "\n",
        "# ==============================\n",
        "#   BUILD TABLES FOR PRECISION / RECALL / F1\n",
        "# ==============================\n",
        "all_models = df['Model'].unique()\n",
        "results = [compute_model_metrics(df, m, fig_cols) for m in all_models]\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Overall average across all models\n",
        "numeric_cols = [c for c in results_df.columns if c != 'Model']\n",
        "overall_avg = pd.DataFrame([{**results_df[numeric_cols].mean(numeric_only=True).round(3), 'Model': 'Overall Avg (All Models)'}])\n",
        "results_df = pd.concat([results_df, overall_avg], ignore_index=True)\n",
        "\n",
        "# ==============================\n",
        "#   ACCURACY TABLE (original mean of FiG consistency)\n",
        "# ==============================\n",
        "fig_consistency_cols = [c + ' FiG_Consistency' for c in fig_cols]\n",
        "\n",
        "avg_consistency = (\n",
        "    df.groupby('Model')[fig_consistency_cols]\n",
        "      .mean(numeric_only=True)\n",
        "      .round(3)\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "avg_consistency['Overall_Accuracy'] = avg_consistency[fig_consistency_cols].mean(axis=1, skipna=True).round(3)\n",
        "\n",
        "overall_summary = pd.DataFrame({\n",
        "    'Model': ['Overall Avg (All Models)'],\n",
        "    **{col: [avg_consistency[col].mean().round(3)] for col in fig_consistency_cols},\n",
        "    'Overall_Accuracy': [avg_consistency['Overall_Accuracy'].mean().round(3)]\n",
        "})\n",
        "\n",
        "# Merge per-model and overall into one table\n",
        "combined_accuracy = pd.concat([avg_consistency, overall_summary], ignore_index=True)\n",
        "\n",
        "# ==============================\n",
        "#   PRINT TABLES\n",
        "# ==============================\n",
        "# Accuracy Table\n",
        "acc_cols = ['Model'] + fig_consistency_cols + ['Overall_Accuracy']\n",
        "print(\"\\n=== DrawBench FiG Accuracy vs Human Responses ===\")\n",
        "print(tabulate(combined_accuracy[acc_cols].fillna(\"-\"), headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# Precision Table\n",
        "prec_cols = ['Model'] + [c+'_Precision' for c in fig_cols] + ['Overall_Precision']\n",
        "print(\"\\n=== DrawBench FiG Precision vs Human Responses ===\")\n",
        "print(tabulate(results_df[prec_cols].fillna(\"-\"), headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# Recall Table\n",
        "rec_cols = ['Model'] + [c+'_Recall' for c in fig_cols] + ['Overall_Recall']\n",
        "print(\"\\n=== DrawBench FiG Recall vs Human Responses ===\")\n",
        "print(tabulate(results_df[rec_cols].fillna(\"-\"), headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "\n",
        "# F1 Table\n",
        "f1_cols = ['Model'] + [c+'_F1' for c in fig_cols] + ['Overall_F1']\n",
        "print(\"\\n=== DrawBench FiG F1 Score vs Human Responses ===\")\n",
        "print(tabulate(results_df[f1_cols].fillna(\"-\"), headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--DdilxS2xNd",
        "outputId": "0cf49b22-2ff0-40cf-8d3b-52542b52093c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== DrawBench FiG Accuracy vs Human Responses ===\n",
            "+--------------------------+------------------------+------------------------+------------------------+----------------------------+----------------------+------------------+\n",
            "|          Model           | Object FiG_Consistency | Colour FiG_Consistency | Number FiG_Consistency | Positional FiG_Consistency | Text FiG_Consistency | Overall_Accuracy |\n",
            "+--------------------------+------------------------+------------------------+------------------------+----------------------------+----------------------+------------------+\n",
            "|         Flux-Dev         |          0.75          |          0.0           |         0.562          |            0.8             |         1.0          |      0.622       |\n",
            "|           SD2            |          0.95          |          0.5           |         0.286          |            1.0             |         1.0          |      0.747       |\n",
            "|           SDXL           |          0.9           |         0.667          |         0.357          |           0.909            |         0.75         |      0.717       |\n",
            "| Overall Avg (All Models) |         0.867          |         0.389          |         0.402          |           0.903            |        0.917         |      0.695       |\n",
            "+--------------------------+------------------------+------------------------+------------------------+----------------------------+----------------------+------------------+\n",
            "\n",
            "=== DrawBench FiG Precision vs Human Responses ===\n",
            "+--------------------------+------------------+------------------+------------------+----------------------+----------------+-------------------+\n",
            "|          Model           | Object_Precision | Colour_Precision | Number_Precision | Positional_Precision | Text_Precision | Overall_Precision |\n",
            "+--------------------------+------------------+------------------+------------------+----------------------+----------------+-------------------+\n",
            "|           SD2            |      0.526       |       1.0        |       0.75       |         1.0          |      0.0       |       0.655       |\n",
            "|           SDXL           |      0.833       |       1.0        |       0.8        |         1.0          |     0.333      |       0.793       |\n",
            "|         Flux-Dev         |      0.867       |       0.0        |       1.0        |         1.0          |      0.25      |       0.623       |\n",
            "| Overall Avg (All Models) |      0.742       |      0.667       |       0.85       |         1.0          |     0.194      |       0.69        |\n",
            "+--------------------------+------------------+------------------+------------------+----------------------+----------------+-------------------+\n",
            "\n",
            "=== DrawBench FiG Recall vs Human Responses ===\n",
            "+--------------------------+---------------+---------------+---------------+-------------------+-------------+----------------+\n",
            "|          Model           | Object_Recall | Colour_Recall | Number_Recall | Positional_Recall | Text_Recall | Overall_Recall |\n",
            "+--------------------------+---------------+---------------+---------------+-------------------+-------------+----------------+\n",
            "|           SD2            |     0.909     |     0.05      |     0.158     |       0.55        |     0.0     |     0.333      |\n",
            "|           SDXL           |     0.882     |      0.1      |     0.235     |        0.5        |    0.062    |     0.356      |\n",
            "|         Flux-Dev         |     0.722     |      0.0      |     0.45      |        0.2        |    0.062    |     0.287      |\n",
            "| Overall Avg (All Models) |     0.838     |     0.05      |     0.281     |       0.417       |    0.041    |     0.325      |\n",
            "+--------------------------+---------------+---------------+---------------+-------------------+-------------+----------------+\n",
            "\n",
            "=== DrawBench FiG F1 Score vs Human Responses ===\n",
            "+--------------------------+-----------+-----------+-----------+---------------+---------+------------+\n",
            "|          Model           | Object_F1 | Colour_F1 | Number_F1 | Positional_F1 | Text_F1 | Overall_F1 |\n",
            "+--------------------------+-----------+-----------+-----------+---------------+---------+------------+\n",
            "|           SD2            |   0.667   |   0.095   |   0.261   |     0.71      |   0.0   |   0.347    |\n",
            "|           SDXL           |   0.857   |   0.182   |   0.364   |     0.667     |  0.105  |   0.435    |\n",
            "|         Flux-Dev         |   0.788   |    0.0    |   0.621   |     0.333     |   0.1   |   0.368    |\n",
            "| Overall Avg (All Models) |   0.771   |   0.092   |   0.415   |     0.57      |  0.068  |   0.383    |\n",
            "+--------------------------+-----------+-----------+-----------+---------------+---------+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Name of the output zip\n",
        "zip_name = \"Drawbench-FiG-human_comparision.zip\"\n",
        "\n",
        "# Find all CSV files in the current directory\n",
        "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "\n",
        "# Create the zip and add all CSVs\n",
        "with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
        "    for f in csv_files:\n",
        "        zipf.write(f)\n",
        "\n",
        "# Trigger download in Colab\n",
        "files.download(zip_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9_XYdBtzyTMa",
        "outputId": "3ca9634a-6ca6-427d-9a82-97e31f9b0dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_35b30a7c-1b53-410c-b834-f22a75fbbb84\", \"Drawbench-FiG-human_comparision.zip\", 72579)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}