{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "#          IMPORTS\n",
        "# ==============================\n",
        "# Importing necessary libraries for data processing and analysis\n",
        "import pandas as pd  # Using pandas for data manipulation and DataFrame operations\n",
        "import numpy as np  # Using numpy for numerical computations and array operations\n",
        "import spacy  # Using spaCy for natural language processing tasks\n",
        "import re  # Using regular expressions for text pattern matching\n",
        "from typing import Set, Dict  # Using type hints for better code documentation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score  # Importing evaluation metrics\n",
        "from tabulate import tabulate  # Using tabulate for creating formatted tables\n",
        "\n",
        "# ==============================\n",
        "#        LOAD SPACY MODEL\n",
        "# ==============================\n",
        "# Attempting to load the English spaCy model with small word vectors\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # Downloading the model if it's not available locally\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Adding entity merging to the pipeline for better text analysis\n",
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "# ==============================\n",
        "#        FINE-GRAINED METRICS\n",
        "# ==============================\n",
        "# Defining a class for calculating fine-grained evaluation metrics\n",
        "class FineGrainedMetrics:\n",
        "    @staticmethod\n",
        "    def related_to_noun(doc, attribute: str, noun: str) -> bool:\n",
        "        # Checking if an attribute is syntactically related to a specific noun\n",
        "        for token in doc:\n",
        "            if token.text == noun:\n",
        "                # Looking for the attribute within the noun's syntactic subtree\n",
        "                if attribute in [t.text for t in token.subtree]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @classmethod\n",
        "    def color(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        # Defining a comprehensive set of color words for detection\n",
        "        COLORS = {'red','blue','green','yellow','black','white','gray','grey',\n",
        "                  'orange','pink','purple','brown','violet','indigo','turquoise',\n",
        "                  'cyan','magenta'}\n",
        "        score, original_colors = 0.0, set()\n",
        "        # Analyzing each generated noun for color associations\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting colors that are modifying nouns in metadata\n",
        "            meta_colors = {t.text for t in meta\n",
        "                           if cls.related_to_noun(meta, t.text, noun)\n",
        "                           and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            # Extracting colors that are modifying nouns in original text\n",
        "            orig_colors = {t.text for t in orig\n",
        "                           if cls.related_to_noun(orig, t.text, noun)\n",
        "                           and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            # Collecting all original colors for scoring\n",
        "            original_colors.update(orig_colors)\n",
        "            # Calculating score based on color matches\n",
        "            if orig_colors:\n",
        "                score += len(orig_colors & meta_colors)\n",
        "        # Returning -1 if no colors found, otherwise calculating precision\n",
        "        return -1 if not original_colors else score / len(original_colors)\n",
        "\n",
        "    @classmethod\n",
        "    def number(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        # Creating mapping from quantity words to numerical values\n",
        "        QUANTITY_MAP = {'a':'1','an':'1','the':'1','one':'1','two':'2','three':'3',\n",
        "                        'couple':'2','few':'3','several':'4','many':'5','dozen':'12'}\n",
        "        score, original_numbers = 0.0, set()\n",
        "        # Analyzing each noun for numerical quantifiers\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting numerical modifiers from metadata\n",
        "            meta_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in meta\n",
        "                         if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            # Extracting numerical modifiers from original text\n",
        "            orig_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in orig\n",
        "                         if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            # Collecting all original numbers for scoring\n",
        "            original_numbers.update(orig_nums)\n",
        "            # Calculating score based on number matches\n",
        "            if orig_nums:\n",
        "                score += len(orig_nums & meta_nums)\n",
        "        # Handling cases where no numbers are found\n",
        "        if not original_numbers:\n",
        "            return -1\n",
        "        # Calculating precision and ensuring it doesn't exceed 1.0\n",
        "        raw_val = score / len(original_numbers)\n",
        "        return min(raw_val, 1.0)\n",
        "\n",
        "    @classmethod\n",
        "    def text(cls, meta, orig, _: Set[str]) -> float:\n",
        "        # Defining words that indicate text content is being described\n",
        "        TEXT_INDICATORS = {'written','saying','says','reading','text'}\n",
        "        # Pattern for detecting quoted text in captions\n",
        "        QUOTE_PATTERN = r'[\\\"\\'«»\"]([^\"\\'«»\"]*)[\\\"\\'«»\"]'\n",
        "        # Checking if original caption contains text indicators\n",
        "        if any(t.text in TEXT_INDICATORS for t in orig):\n",
        "            # Extracting quoted text from both documents\n",
        "            orig_matches = re.findall(QUOTE_PATTERN, orig.text)\n",
        "            meta_matches = re.findall(QUOTE_PATTERN, meta.text)\n",
        "            if not orig_matches:\n",
        "                return -1\n",
        "            # Normalizing text by removing spaces and converting to lowercase\n",
        "            orig_norm = [''.join(s.lower().split()) for s in orig_matches]\n",
        "            meta_norm = [''.join(s.lower().split()) for s in meta_matches]\n",
        "            # Counting how many original quotes appear in metadata\n",
        "            matches = sum(any(o in m for m in meta_norm) for o in orig_norm)\n",
        "            return matches / len(orig_matches)\n",
        "        return -1\n",
        "\n",
        "    # ==============================\n",
        "    # UPDATED SPATIAL RELATIONS FOR POSITION\n",
        "    # ==============================\n",
        "    @staticmethod\n",
        "    def extract_spatial_relations(doc):\n",
        "        \"\"\"\n",
        "        Extracting spatial relationships as (subject, preposition, object) tuples.\n",
        "        Uses token ancestors to find subjects (nsubj, dobj, pobj) for prepositions.\n",
        "        \"\"\"\n",
        "        rels = set()\n",
        "        # Looking for preposition tokens in the document\n",
        "        for token in doc:\n",
        "            if token.dep_ == 'prep':\n",
        "                # Finding prepositional objects\n",
        "                for pobj in [c for c in token.children if c.dep_ == 'pobj']:\n",
        "                    subj = None\n",
        "                    # Traversing ancestors to find the subject\n",
        "                    for anc in token.ancestors:\n",
        "                        if anc.dep_ in {'nsubj', 'nsubjpass', 'dobj', 'pobj'}:\n",
        "                            subj = anc.lemma_\n",
        "                            break\n",
        "                    # Creating spatial relation triple if subject is found\n",
        "                    if subj:\n",
        "                        rels.add((subj, token.lemma_, pobj.lemma_))\n",
        "        return rels\n",
        "\n",
        "    @classmethod\n",
        "    def position(cls, meta, orig, _: Set[str]) -> float:\n",
        "        # Extracting spatial relations from both documents\n",
        "        orig_rel = cls.extract_spatial_relations(orig)\n",
        "        meta_rel = cls.extract_spatial_relations(meta)\n",
        "        if not orig_rel:\n",
        "            return -1\n",
        "        # Calculating precision of spatial relation matches\n",
        "        return len(orig_rel & meta_rel) / len(orig_rel)\n",
        "\n",
        "# ==============================\n",
        "#        ANALYZE CAPTION PAIR\n",
        "# ==============================\n",
        "def analyze_caption_pair(meta_caption: str, orig_caption: str) -> Dict[str, float]:\n",
        "    # Converting captions to strings and handling missing values\n",
        "    meta_caption = str(meta_caption) if pd.notna(meta_caption) else \"\"\n",
        "    orig_caption = str(orig_caption) if pd.notna(orig_caption) else \"\"\n",
        "    # Processing text with spaCy to create document objects\n",
        "    meta_doc, orig_doc = nlp(meta_caption), nlp(orig_caption)\n",
        "\n",
        "    # Object FiG using simpler noun-recall logic\n",
        "    # Extracting nouns and proper nouns from both documents\n",
        "    meta_nouns = {t.text for t in meta_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    orig_nouns = {t.text for t in orig_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    # Finding common nouns between original and metadata\n",
        "    common_nouns = orig_nouns & meta_nouns\n",
        "    # Calculating noun recall metric\n",
        "    noun_recall = len(common_nouns) / len(orig_nouns) if orig_nouns else 0\n",
        "\n",
        "    # Returning dictionary with all fine-grained metrics\n",
        "    return {\n",
        "        \"Object FiG\": noun_recall,\n",
        "        \"Colour FiG\": FineGrainedMetrics.color(meta_doc, orig_doc, common_nouns),\n",
        "        \"Number FiG\": FineGrainedMetrics.number(meta_doc, orig_doc, common_nouns),\n",
        "        \"Positional FiG\": FineGrainedMetrics.position(meta_doc, orig_doc, common_nouns),\n",
        "        \"Text FiG\": FineGrainedMetrics.text(meta_doc, orig_doc, common_nouns)\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "#       PROCESS MODEL CSV\n",
        "# ==============================\n",
        "def process_model_csv(file_path: str):\n",
        "    # Reading CSV file and removing rows with missing prompts or captions\n",
        "    df = pd.read_csv(file_path).dropna(subset=[\"Prompts\", \"Meta Caption\"]).reset_index(drop=True)\n",
        "    # Analyzing each caption pair to compute metrics\n",
        "    results = [analyze_caption_pair(row[\"Meta Caption\"], row[\"Prompts\"]) for _, row in df.iterrows()]\n",
        "    # Creating DataFrame from results and combining with original data\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    return pd.concat([df, metrics_df], axis=1)\n",
        "\n",
        "# ==============================\n",
        "#      PARSE HUMAN RESPONSES\n",
        "# ==============================\n",
        "def parse_human_responses(file_path: str) -> pd.DataFrame:\n",
        "    # Reading Excel file with human annotation data\n",
        "    df = pd.read_excel(file_path, header=None)\n",
        "    rows = []\n",
        "    # Processing each column in the Excel file\n",
        "    for col in df.columns:\n",
        "        caption_cell = str(df.iloc[0, col])\n",
        "        # Using regex to extract model, image, and caption information\n",
        "        match = re.match(r\"\\[(.*?)\\] Image: (\\d+) \\| Prompt: (.*)\", caption_cell)\n",
        "        if not match:\n",
        "            continue\n",
        "        model, image, caption = match.groups()\n",
        "        human_labels = []\n",
        "        # Processing responses from two human annotators\n",
        "        for annot_idx in range(1, 3):\n",
        "            resp_cell = str(df.iloc[annot_idx, col]).strip()\n",
        "            # Determining if response is positive (Yes) or negative (No)\n",
        "            human_label = \"Yes\" if resp_cell.startswith(\"Yes\") else \"No\"\n",
        "            # Extracting alignment issues from negative responses\n",
        "            not_aligning = \", \".join(re.findall(r\"(Object|Colour|Number|Position|Text|Others)\", resp_cell)) if human_label==\"No\" else \"\"\n",
        "            # Formatting human label with alignment issues\n",
        "            human_labels.append(f\"{human_label} ({not_aligning})\" if not_aligning else human_label)\n",
        "        # Storing parsed data for each image-caption pair\n",
        "        rows.append({\n",
        "            \"Model\": model,\n",
        "            \"Image\": image,\n",
        "            \"Caption\": caption,\n",
        "            \"Human_Responses\": \"; \".join(human_labels)\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ==============================\n",
        "#          HUMAN MAJORITY\n",
        "# ==============================\n",
        "def compute_human_majority(human_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    def majority_vote(responses):\n",
        "        # Calculating majority vote (Yes if at least 2 out of 2 say Yes)\n",
        "        return 'Yes' if sum([r.startswith('Yes') for r in responses.split(\"; \")]) >= 2 else 'No'\n",
        "    # Applying majority vote to create consolidated human judgment\n",
        "    human_df['Human_Majority'] = human_df['Human_Responses'].apply(majority_vote)\n",
        "    return human_df\n",
        "\n",
        "# ==============================\n",
        "#       MERGE WITH HUMAN\n",
        "# ==============================\n",
        "\n",
        "def merge_with_human(model_df: pd.DataFrame, human_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
        "    # Ensuring consistent data types for merging\n",
        "    model_df['image_name'] = model_df['image_name'].astype(str)\n",
        "    human_df['Image'] = human_df['Image'].astype(str)\n",
        "\n",
        "    # Merging model outputs with human evaluations\n",
        "    merged = pd.merge(\n",
        "        model_df,\n",
        "        human_df[human_df['Model']==model_name][['Image','Caption','Human_Responses','Human_Majority']],\n",
        "        left_on=['image_name','Prompts'],\n",
        "        right_on=['Image','Caption'],\n",
        "        how='left'\n",
        "    ).drop(columns=['Image','Caption'])\n",
        "\n",
        "    # Object FiG alignment\n",
        "    def check_object_alignment(row):\n",
        "        # Checking if Object FiG metric aligns with human judgment\n",
        "        val = row.get(\"Object FiG\", None)\n",
        "        if val == -1 or pd.isna(val):\n",
        "            return None\n",
        "        # For positive human judgment, expecting perfect object recall\n",
        "        if row['Human_Majority'] == 'Yes':\n",
        "            return 1 if val == 1 else 0\n",
        "        # For negative human judgment, expecting imperfect object recall\n",
        "        else:\n",
        "            return 1 if val < 1 else 0\n",
        "    merged[\"Object FiG_alignment\"] = merged.apply(check_object_alignment, axis=1)\n",
        "\n",
        "    # Other FiG metrics alignment, with special logic for Positional and Text FiG\n",
        "    fig_columns = ['Colour FiG','Number FiG','Positional FiG','Text FiG']\n",
        "    def check_other_alignment(row):\n",
        "        alignment = {}\n",
        "        human = row.get(\"Human_Majority\", \"Yes\")\n",
        "        # Checking alignment for each fine-grained metric\n",
        "        for col in fig_columns:\n",
        "            val = row.get(col, None)\n",
        "            if val == -1 or pd.isna(val):\n",
        "                alignment[col+'_alignment'] = None\n",
        "                continue\n",
        "            val = min(val,1.0)\n",
        "            # SPECIAL LOGIC for Positional and Text FiG\n",
        "            if col in ['Positional FiG','Text FiG']:\n",
        "                # Considering zero score with negative human judgment as aligned\n",
        "                if val == 0 and human == 'No':\n",
        "                    alignment[col+'_alignment'] = 1\n",
        "                    continue\n",
        "                alignment[col+'_alignment'] = 1 if val == 1 else 0\n",
        "            else:\n",
        "                alignment[col+'_alignment'] = 1 if val == 1 else 0\n",
        "        return pd.Series(alignment)\n",
        "\n",
        "    # Applying alignment checks to all fine-grained metrics\n",
        "    merged[[c+'_alignment' for c in fig_columns]] = merged.apply(check_other_alignment, axis=1)\n",
        "    merged['Model'] = model_name\n",
        "    return merged\n",
        "\n",
        "\n",
        "# ==============================\n",
        "#   HELPER: PARSE HUMAN RESPONSE\n",
        "# ==============================\n",
        "def parse_human_response(resp):\n",
        "    # Parsing human responses to extract alignment issues\n",
        "    if pd.isna(resp) or resp.strip() == \"\":\n",
        "        return []\n",
        "    # Extracting the alignment issue categories from parentheses\n",
        "    parts = [p.strip().split('(')[-1].replace(')','').strip().lower() for p in resp.split(';')]\n",
        "    return parts\n",
        "\n",
        "# ==============================\n",
        "#       COMPUTE METRICS PER CATEGORY\n",
        "# ==============================\n",
        "def compute_fig_metrics(df, category):\n",
        "    # Computing evaluation metrics for each fine-grained category\n",
        "    alignment_col = f\"{category} FiG_alignment\"\n",
        "    # Creating true labels from human responses\n",
        "    y_true = df[\"Human_Responses\"].apply(lambda x: 0 if category.lower() in parse_human_response(x) else 1)\n",
        "    # Creating predicted labels from alignment scores\n",
        "    y_pred = df[alignment_col].apply(lambda x: 1 if pd.notna(x) and x == 1 else 0)\n",
        "    # Filtering out rows with missing values\n",
        "    mask = y_true.notna() & y_pred.notna()\n",
        "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
        "    num_samples = len(y_true)\n",
        "    # Calculating various evaluation metrics\n",
        "    accuracy = round(df[alignment_col].mean(), 3)\n",
        "    precision = round(precision_score(y_true, y_pred, zero_division=0), 3)\n",
        "    recall = round(recall_score(y_true, y_pred, zero_division=0), 3)\n",
        "    f1 = round(f1_score(y_true, y_pred, zero_division=0), 3)\n",
        "    return {\"Category\": category, \"Accuracy\":accuracy, \"Precision\":precision, \"Recall\":recall, \"F1\":f1, \"Num_Samples\":num_samples}\n",
        "\n",
        "# ==============================\n",
        "#            MAIN PIPELINE\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    # Processing model CSVs for different AI models\n",
        "    sd2_df = process_model_csv(\"drawbench_sd2.csv\")\n",
        "    sdxl_df = process_model_csv(\"drawbench_sdxl.csv\")\n",
        "    flux_df = process_model_csv(\"drawbench_fluxdev.csv\")\n",
        "\n",
        "    # Parsing and processing human evaluation responses\n",
        "    human_df = parse_human_responses(\"human_response.xlsx\")\n",
        "    human_df = compute_human_majority(human_df)\n",
        "\n",
        "    # Merging model outputs with human evaluations\n",
        "    sd2_merged = merge_with_human(sd2_df, human_df, \"SD2\")\n",
        "    sdxl_merged = merge_with_human(sdxl_df, human_df, \"SDXL\")\n",
        "    flux_merged = merge_with_human(flux_df, human_df, \"Flux-Dev\")\n",
        "\n",
        "    # Combining all model results into final dataset\n",
        "    final_df = pd.concat([sd2_merged, sdxl_merged, flux_merged], ignore_index=True)\n",
        "    final_df.to_csv(\"drawbench_all_models_fig_detailed.csv\", index=False)\n",
        "    print(\"Saved -> drawbench_all_models_fig_detailed.csv\")\n",
        "\n",
        "    # Generating per-category filtered CSVs for analysis\n",
        "    categories = [\"Object\",\"Colour\",\"Number\",\"Positional\",\"Text\"]\n",
        "    results = []\n",
        "    for cat in categories:\n",
        "        fig_col = f\"{cat} FiG\"\n",
        "        alignment_col = f\"{cat} FiG_alignment\"\n",
        "        # Creating filtered datasets for each category\n",
        "        if cat == \"Object\":\n",
        "            filtered_df = final_df[[\"image_name\",\"Prompts\",\"Meta Caption\",fig_col,\"Human_Responses\",\"Human_Majority\",alignment_col]]\n",
        "        else:\n",
        "            # Filtering out rows where metric is -1 (not applicable)\n",
        "            filtered_df = final_df[final_df[fig_col]!=-1][[\"image_name\",\"Prompts\",\"Meta Caption\",fig_col,\"Human_Responses\",\"Human_Majority\",alignment_col]]\n",
        "        out_name = f\"{cat.lower()}_fig_non-1.csv\"\n",
        "        filtered_df.to_csv(out_name,index=False)\n",
        "        print(f\"Saved {out_name} with {len(filtered_df)} valid rows.\")\n",
        "        # Computing metrics for each category\n",
        "        metrics = compute_fig_metrics(filtered_df, cat)\n",
        "        results.append(metrics)\n",
        "\n",
        "    # Combine and compute overall\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    total_samples = metrics_df[\"Num_Samples\"].sum()\n",
        "    if total_samples>0:\n",
        "        # Calculating weighted averages across all categories\n",
        "        overall = {\n",
        "            \"Category\":\"Overall Avg\",\n",
        "            \"Accuracy\": round(np.nansum(metrics_df[\"Accuracy\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"Precision\": round(np.nansum(metrics_df[\"Precision\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"Recall\": round(np.nansum(metrics_df[\"Recall\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"F1\": round(np.nansum(metrics_df[\"F1\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"Num_Samples\": total_samples\n",
        "        }\n",
        "        metrics_df = pd.concat([metrics_df,pd.DataFrame([overall])],ignore_index=True)\n",
        "\n",
        "    # Printing summary results in formatted table\n",
        "    print(\"\\n=== DrawBench FiG Metrics Summary ===\")\n",
        "    print(tabulate(metrics_df, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "    metrics_df.to_csv(\"fig_category_metrics_summary.csv\",index=False)\n",
        "    print(\"Saved metrics summary to 'fig_category_metrics_summary.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuTCJwOaiNt",
        "outputId": "8a468f8c-8a52-4aaa-e5ef-559e04adb72d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved -> drawbench_all_models_fig_detailed.csv\n",
            "Saved object_fig_non-1.csv with 60 valid rows.\n",
            "Saved colour_fig_non-1.csv with 7 valid rows.\n",
            "Saved number_fig_non-1.csv with 44 valid rows.\n",
            "Saved positional_fig_non-1.csv with 27 valid rows.\n",
            "Saved text_fig_non-1.csv with 11 valid rows.\n",
            "\n",
            "=== DrawBench FiG Metrics Summary ===\n",
            "+-------------+----------+-----------+--------+-------+-------------+\n",
            "|  Category   | Accuracy | Precision | Recall |  F1   | Num_Samples |\n",
            "+-------------+----------+-----------+--------+-------+-------------+\n",
            "|   Object    |  0.867   |   0.731   | 0.844  | 0.784 |     60      |\n",
            "|   Colour    |  0.714   |    1.0    | 0.714  | 0.833 |      7      |\n",
            "|   Number    |   0.75   |   0.97    |  0.78  | 0.865 |     44      |\n",
            "| Positional  |  0.852   |    1.0    | 0.852  | 0.92  |     27      |\n",
            "|    Text     |   1.0    |   0.273   |  1.0   | 0.429 |     11      |\n",
            "| Overall Avg |  0.832   |   0.829   | 0.832  | 0.809 |     149     |\n",
            "+-------------+----------+-----------+--------+-------+-------------+\n",
            "Saved metrics summary to 'fig_category_metrics_summary.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Set the name of the zip file\n",
        "zip_name = \"all_files.zip\"\n",
        "\n",
        "# Zip the entire runtime folder (current working directory)\n",
        "shutil.make_archive(\"all_files\", 'zip', os.getcwd())\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JHJKQSJLyrBb",
        "outputId": "f8e7f67b-7265-4975-e4e7-0950a98e37f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2cab6863-8def-4af2-9fcb-60d396b6671e\", \"all_files.zip\", 7124795)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}