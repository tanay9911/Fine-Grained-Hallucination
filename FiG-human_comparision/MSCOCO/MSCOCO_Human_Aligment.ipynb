{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "#          IMPORTS\n",
        "# ==============================\n",
        "# Importing necessary libraries for data processing and NLP tasks\n",
        "import pandas as pd  # Using pandas for data manipulation and analysis\n",
        "import numpy as np  # Using numpy for numerical operations\n",
        "import spacy  # Using spaCy for natural language processing\n",
        "import re  # Using regular expressions for text pattern matching\n",
        "from typing import Set, Dict  # Using type hints for better code documentation\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score  # Importing metrics for model evaluation\n",
        "from tabulate import tabulate  # Using tabulate for creating formatted tables\n",
        "\n",
        "# ==============================\n",
        "#        LOAD SPACY MODEL\n",
        "# ==============================\n",
        "# Attempting to load the English spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    # Downloading the model if it's not found locally\n",
        "    import os\n",
        "    os.system(\"python -m spacy download en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Adding a pipeline component to merge entities for better text analysis\n",
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "# ==============================\n",
        "#        FINE-GRAINED METRICS\n",
        "# ==============================\n",
        "# Defining a class for calculating fine-grained evaluation metrics\n",
        "class FineGrainedMetrics:\n",
        "    @staticmethod\n",
        "    def related_to_noun(doc, attribute: str, noun: str) -> bool:\n",
        "        # Checking if an attribute is syntactically related to a specific noun in the document\n",
        "        for token in doc:\n",
        "            if token.text == noun:\n",
        "                if attribute in [t.text for t in token.subtree]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    @classmethod\n",
        "    def color(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        # Defining a set of common color words for detection\n",
        "        COLORS = {'red','blue','green','yellow','black','white','gray','grey',\n",
        "                  'orange','pink','purple','brown','violet','indigo','turquoise',\n",
        "                  'cyan','magenta'}\n",
        "        score, original_colors = 0.0, set()\n",
        "        # Analyzing each noun to find associated color attributes\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting colors from metadata that are modifying the current noun\n",
        "            meta_colors = {t.text for t in meta\n",
        "                           if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            # Extracting colors from original text that are modifying the current noun\n",
        "            orig_colors = {t.text for t in orig\n",
        "                           if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            original_colors.update(orig_colors)\n",
        "            # Calculating score based on matching colors\n",
        "            if orig_colors:\n",
        "                score += len(orig_colors & meta_colors)\n",
        "        return -1 if not original_colors else score / len(original_colors)\n",
        "\n",
        "    @classmethod\n",
        "    def number(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        # Creating a mapping from quantity words to numerical values\n",
        "        QUANTITY_MAP = {'a':'1','an':'1','the':'1','one':'1','two':'2','three':'3',\n",
        "                        'couple':'2','few':'3','several':'4','many':'5','dozen':'12'}\n",
        "        score, original_numbers = 0.0, set()\n",
        "        # Analyzing each noun to find associated numerical quantifiers\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting numbers from metadata that are quantifying the current noun\n",
        "            meta_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in meta\n",
        "                         if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            # Extracting numbers from original text that are quantifying the current noun\n",
        "            orig_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in orig\n",
        "                         if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            original_numbers.update(orig_nums)\n",
        "            # Calculating score based on matching numbers\n",
        "            if orig_nums:\n",
        "                score += len(orig_nums & meta_nums)\n",
        "        if not original_numbers:\n",
        "            return -1\n",
        "        raw_val = score / len(original_numbers)\n",
        "        return min(raw_val, 1.0)\n",
        "\n",
        "    @classmethod\n",
        "    def text(cls, meta, orig, _: Set[str]) -> float:\n",
        "        # Defining indicators that suggest text content is being described\n",
        "        TEXT_INDICATORS = {'written','saying','says','reading','text'}\n",
        "        # Creating a pattern to detect quoted text in captions\n",
        "        QUOTE_PATTERN = r'[\\\"\\'«»\"]([^\"\\'«»\"]*)[\\\"\\'«»\"]'\n",
        "        # Checking if the original caption contains text indicators\n",
        "        if any(t.text in TEXT_INDICATORS for t in orig):\n",
        "            # Extracting quoted text from both original and metadata captions\n",
        "            orig_matches = re.findall(QUOTE_PATTERN, orig.text)\n",
        "            meta_matches = re.findall(QUOTE_PATTERN, meta.text)\n",
        "            if not orig_matches:\n",
        "                return -1\n",
        "            # Normalizing text by removing spaces and converting to lowercase\n",
        "            orig_norm = [''.join(s.lower().split()) for s in orig_matches]\n",
        "            meta_norm = [''.join(s.lower().split()) for s in meta_matches]\n",
        "            # Counting matches between original and metadata quoted text\n",
        "            matches = sum(any(o in m for m in meta_norm) for o in orig_norm)\n",
        "            return matches / len(orig_matches)\n",
        "        return -1\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_spatial_relations(doc):\n",
        "        # Extracting spatial relationships from the document using dependency parsing\n",
        "        rels = set()\n",
        "        for token in doc:\n",
        "            if token.dep_ == 'prep':\n",
        "                # Finding prepositional objects for spatial relations\n",
        "                for pobj in [c for c in token.children if c.dep_ == 'pobj']:\n",
        "                    subj = None\n",
        "                    # Finding the subject related to this preposition\n",
        "                    for anc in token.ancestors:\n",
        "                        if anc.dep_ in {'nsubj', 'nsubjpass', 'dobj', 'pobj'}:\n",
        "                            subj = anc.lemma_\n",
        "                            break\n",
        "                    if subj:\n",
        "                        # Storing the spatial relation as (subject, preposition, object)\n",
        "                        rels.add((subj, token.lemma_, pobj.lemma_))\n",
        "        return rels\n",
        "\n",
        "    @classmethod\n",
        "    def position(cls, meta, orig, _: Set[str]) -> float:\n",
        "        # Extracting spatial relations from both original and metadata captions\n",
        "        orig_rel = cls.extract_spatial_relations(orig)\n",
        "        meta_rel = cls.extract_spatial_relations(meta)\n",
        "        if not orig_rel:\n",
        "            return -1\n",
        "        # Calculating the proportion of matching spatial relations\n",
        "        return len(orig_rel & meta_rel) / len(orig_rel)\n",
        "\n",
        "# ==============================\n",
        "#        ANALYZE CAPTION PAIR\n",
        "# ==============================\n",
        "def analyze_caption_pair(meta_caption: str, orig_caption: str) -> Dict[str, float]:\n",
        "    # Converting captions to strings and handling missing values\n",
        "    meta_caption = str(meta_caption) if pd.notna(meta_caption) else \"\"\n",
        "    orig_caption = str(orig_caption) if pd.notna(orig_caption) else \"\"\n",
        "    # Processing captions with spaCy to create document objects\n",
        "    meta_doc, orig_doc = nlp(meta_caption), nlp(orig_caption)\n",
        "\n",
        "    # Extracting nouns and proper nouns from both documents\n",
        "    meta_nouns = {t.text for t in meta_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    orig_nouns = {t.text for t in orig_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    # Finding common nouns between original and metadata\n",
        "    common_nouns = orig_nouns & meta_nouns\n",
        "    # Calculating noun recall (proportion of original nouns present in metadata)\n",
        "    noun_recall = len(common_nouns) / len(orig_nouns) if orig_nouns else 0\n",
        "\n",
        "    # Returning a dictionary with all fine-grained metrics\n",
        "    return {\n",
        "        \"Object FiG\": noun_recall,\n",
        "        \"Colour FiG\": FineGrainedMetrics.color(meta_doc, orig_doc, common_nouns),\n",
        "        \"Number FiG\": FineGrainedMetrics.number(meta_doc, orig_doc, common_nouns),\n",
        "        \"Positional FiG\": FineGrainedMetrics.position(meta_doc, orig_doc, common_nouns),\n",
        "        \"Text FiG\": FineGrainedMetrics.text(meta_doc, orig_doc, common_nouns)\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "#       PROCESS MODEL CSV\n",
        "# ==============================\n",
        "def process_model_csv(file_path: str):\n",
        "    # Reading CSV file and removing rows with missing captions\n",
        "    df = pd.read_csv(file_path).dropna(subset=[\"mscoco_caption\", \"Meta Caption\"]).reset_index(drop=True)\n",
        "    # Analyzing each caption pair to compute fine-grained metrics\n",
        "    results = [analyze_caption_pair(row[\"Meta Caption\"], row[\"mscoco_caption\"]) for _, row in df.iterrows()]\n",
        "    # Creating a DataFrame from the results and combining with original data\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    return pd.concat([df, metrics_df], axis=1)\n",
        "\n",
        "# ==============================\n",
        "#      PARSE HUMAN RESPONSES\n",
        "# ==============================\n",
        "def parse_human_responses(file_path: str) -> pd.DataFrame:\n",
        "    # Reading Excel file with human annotation data\n",
        "    df = pd.read_excel(file_path, header=None)\n",
        "    rows = []\n",
        "    # Processing each column in the Excel file\n",
        "    for col in df.columns:\n",
        "        caption_cell = str(df.iloc[0, col])\n",
        "        # Using regex to extract model, image, and caption information\n",
        "        match = re.match(r\"\\[(.*?)\\] Image: (.*?) \\| Caption: (.*)\", caption_cell)\n",
        "        if not match:\n",
        "            continue\n",
        "        model, image, caption = match.groups()\n",
        "        human_labels = []\n",
        "        # Processing responses from two human annotators\n",
        "        for annot_idx in range(1, 3):\n",
        "            resp_cell = str(df.iloc[annot_idx, col]).strip()\n",
        "            # Determining if the response is positive (Yes) or negative (No)\n",
        "            human_label = \"Yes\" if resp_cell.startswith(\"Yes\") else \"No\"\n",
        "            # Extracting which aspects are not aligning in negative responses\n",
        "            not_aligning = \", \".join(re.findall(r\"(Object|Colour|Number|Position|Text|Others)\", resp_cell)) if human_label==\"No\" else \"\"\n",
        "            # Formatting the human label with alignment issues if present\n",
        "            human_labels.append(f\"{human_label} ({not_aligning})\" if not_aligning else human_label)\n",
        "        rows.append({\n",
        "            \"Model\": model,\n",
        "            \"Image\": image,\n",
        "            \"Caption\": caption,\n",
        "            \"Human_Responses\": \"; \".join(human_labels)\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ==============================\n",
        "#          HUMAN MAJORITY\n",
        "# ==============================\n",
        "def compute_human_majority(human_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    def majority_vote(responses):\n",
        "        # Calculating majority vote from human responses (Yes if at least 2 out of 2 say Yes)\n",
        "        return 'Yes' if sum([r.startswith('Yes') for r in responses.split(\"; \")]) >= 2 else 'No'\n",
        "    # Applying majority vote to create a consolidated human judgment column\n",
        "    human_df['Human_Majority'] = human_df['Human_Responses'].apply(majority_vote)\n",
        "    return human_df\n",
        "\n",
        "# ==============================\n",
        "#       MERGE WITH HUMAN\n",
        "# ==============================\n",
        "def merge_with_human(model_df: pd.DataFrame, human_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
        "    # Ensuring image name columns have consistent data types for merging\n",
        "    model_df['image_name'] = model_df['image_name'].astype(str)\n",
        "    human_df['Image'] = human_df['Image'].astype(str)\n",
        "\n",
        "    # Merging model data with human evaluation data\n",
        "    merged = pd.merge(\n",
        "        model_df,\n",
        "        human_df[human_df['Model']==model_name][['Image','Caption','Human_Responses','Human_Majority']],\n",
        "        left_on=['image_name','mscoco_caption'],\n",
        "        right_on=['Image','Caption'],\n",
        "        how='left'\n",
        "    ).drop(columns=['Image','Caption'])\n",
        "\n",
        "    # Object FiG alignment\n",
        "    def check_object_alignment(row):\n",
        "        # Checking if Object FiG metric is aligning with human judgment\n",
        "        val = row.get(\"Object FiG\", None)\n",
        "        if val == -1 or pd.isna(val):\n",
        "            return None\n",
        "        if row['Human_Majority'] == 'Yes':\n",
        "            return 1 if val == 1 else 0\n",
        "        else:\n",
        "            return 1 if val < 1 else 0\n",
        "    merged[\"Object FiG_alignment\"] = merged.apply(check_object_alignment, axis=1)\n",
        "\n",
        "    # Other FiG metrics alignment\n",
        "    fig_columns = ['Colour FiG','Number FiG','Positional FiG','Text FiG']\n",
        "    def check_other_alignment(row):\n",
        "        # Checking alignment for other fine-grained metrics\n",
        "        alignment = {}\n",
        "        human = row.get(\"Human_Majority\", \"Yes\")\n",
        "        for col in fig_columns:\n",
        "            val = row.get(col, None)\n",
        "            if val == -1 or pd.isna(val):\n",
        "                alignment[col+'_alignment'] = None\n",
        "                continue\n",
        "            val = min(val,1.0)\n",
        "            # Handling special cases for Positional and Text metrics\n",
        "            if col in ['Positional FiG','Text FiG']:\n",
        "                if val == 0 and human == 'No':\n",
        "                    alignment[col+'_alignment'] = 1\n",
        "                    continue\n",
        "                alignment[col+'_alignment'] = 1 if val == 1 else 0\n",
        "            else:\n",
        "                alignment[col+'_alignment'] = 1 if val == 1 else 0\n",
        "        return pd.Series(alignment)\n",
        "    merged[[c+'_alignment' for c in fig_columns]] = merged.apply(check_other_alignment, axis=1)\n",
        "    merged['Model'] = model_name\n",
        "    return merged\n",
        "\n",
        "# ==============================\n",
        "#   HELPER: PARSE HUMAN RESPONSE\n",
        "# ==============================\n",
        "def parse_human_response(resp):\n",
        "    # Parsing human responses to extract alignment issues\n",
        "    if pd.isna(resp) or resp.strip() == \"\":\n",
        "        return []\n",
        "    parts = [p.strip().split('(')[-1].replace(')','').strip().lower() for p in resp.split(';')]\n",
        "    return parts\n",
        "\n",
        "# ==============================\n",
        "#       COMPUTE METRICS PER CATEGORY\n",
        "# ==============================\n",
        "def compute_fig_metrics(df, category):\n",
        "    # Computing evaluation metrics for each fine-grained category\n",
        "    alignment_col = f\"{category} FiG_alignment\"\n",
        "    # Creating true labels from human responses\n",
        "    y_true = df[\"Human_Responses\"].apply(lambda x: 0 if category.lower() in parse_human_response(x) else 1)\n",
        "    # Creating predicted labels from alignment scores\n",
        "    y_pred = df[alignment_col].apply(lambda x: 1 if pd.notna(x) and x == 1 else 0)\n",
        "    # Filtering out rows with missing values\n",
        "    mask = y_true.notna() & y_pred.notna()\n",
        "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
        "    num_samples = len(y_true)\n",
        "    # Calculating various evaluation metrics\n",
        "    accuracy = round(df[alignment_col].mean(), 3)\n",
        "    precision = round(precision_score(y_true, y_pred, zero_division=0), 3)\n",
        "    recall = round(recall_score(y_true, y_pred, zero_division=0), 3)\n",
        "    f1 = round(f1_score(y_true, y_pred, zero_division=0), 3)\n",
        "    return {\"Category\": category, \"Accuracy\":accuracy, \"Precision\":precision, \"Recall\":recall, \"F1\":f1, \"Num_Samples\":num_samples}\n",
        "\n",
        "# ==============================\n",
        "#            MAIN PIPELINE\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    # Processing model CSVs for different AI models\n",
        "    sd2_df = process_model_csv(\"mscoco_sd2_caps.csv\")\n",
        "    sdxl_df = process_model_csv(\"mscoco_sdxl_caps.csv\")\n",
        "    flux_df = process_model_csv(\"mscoco_fluxdev_caps.csv\")\n",
        "\n",
        "    # Parsing and processing human evaluation responses\n",
        "    human_df = parse_human_responses(\"human_responses.xlsx\")\n",
        "    human_df = compute_human_majority(human_df)\n",
        "\n",
        "    # Merging model outputs with human evaluations\n",
        "    sd2_merged = merge_with_human(sd2_df, human_df, \"SD2\")\n",
        "    sdxl_merged = merge_with_human(sdxl_df, human_df, \"SDXL\")\n",
        "    flux_merged = merge_with_human(flux_df, human_df, \"Flux-Dev\")\n",
        "\n",
        "    # Combining all model results into a single DataFrame\n",
        "    final_df = pd.concat([sd2_merged, sdxl_merged, flux_merged], ignore_index=True)\n",
        "    final_df.to_csv(\"mscoco_all_models_fig_detailed.csv\", index=False)\n",
        "    print(\"Saved -> mscoco_all_models_fig_detailed.csv\")\n",
        "\n",
        "    # Generating per-category filtered CSVs for analysis\n",
        "    categories = [\"Object\",\"Colour\",\"Number\",\"Positional\",\"Text\"]\n",
        "    results = []\n",
        "    for cat in categories:\n",
        "        fig_col = f\"{cat} FiG\"\n",
        "        alignment_col = f\"{cat} FiG_alignment\"\n",
        "        # Creating filtered datasets for each category\n",
        "        if cat == \"Object\":\n",
        "            filtered_df = final_df[[\"image_name\",\"mscoco_caption\",\"Meta Caption\",fig_col,\"Human_Responses\",\"Human_Majority\",alignment_col]]\n",
        "        else:\n",
        "            filtered_df = final_df[final_df[fig_col]!=-1][[\"image_name\",\"mscoco_caption\",\"Meta Caption\",fig_col,\"Human_Responses\",\"Human_Majority\",alignment_col]]\n",
        "        out_name = f\"{cat.lower()}_fig_non-1.csv\"\n",
        "        filtered_df.to_csv(out_name,index=False)\n",
        "        print(f\"Saved {out_name} with {len(filtered_df)} valid rows.\")\n",
        "        # Computing metrics for each category\n",
        "        metrics = compute_fig_metrics(filtered_df, cat)\n",
        "        results.append(metrics)\n",
        "\n",
        "    # Combining metrics and computing overall averages\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    total_samples = metrics_df[\"Num_Samples\"].sum()\n",
        "    if total_samples>0:\n",
        "        # Calculating weighted averages across all categories\n",
        "        overall = {\n",
        "            \"Category\":\"Overall Avg\",\n",
        "            \"Accuracy\": round(np.nansum(metrics_df[\"Accuracy\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"Precision\": round(np.nansum(metrics_df[\"Precision\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"Recall\": round(np.nansum(metrics_df[\"Recall\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"F1\": round(np.nansum(metrics_df[\"F1\"]*metrics_df[\"Num_Samples\"])/total_samples,3),\n",
        "            \"Num_Samples\": total_samples\n",
        "        }\n",
        "        metrics_df = pd.concat([metrics_df,pd.DataFrame([overall])],ignore_index=True)\n",
        "\n",
        "    # Printing summary results in a formatted table\n",
        "    print(\"\\n=== MSCOCO FiG Metrics Summary ===\")\n",
        "    print(tabulate(metrics_df, headers=\"keys\", tablefmt=\"pretty\", showindex=False))\n",
        "    metrics_df.to_csv(\"mscoco_fig_category_metrics_summary.csv\",index=False)\n",
        "    print(\"Saved metrics summary to 'mscoco_fig_category_metrics_summary.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_mzllNgltMF",
        "outputId": "58ef46e0-e096-40cd-a736-2bd33b87f95c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved -> mscoco_all_models_fig_detailed.csv\n",
            "Saved object_fig_non-1.csv with 29 valid rows.\n",
            "Saved colour_fig_non-1.csv with 5 valid rows.\n",
            "Saved number_fig_non-1.csv with 19 valid rows.\n",
            "Saved positional_fig_non-1.csv with 11 valid rows.\n",
            "Saved text_fig_non-1.csv with 0 valid rows.\n",
            "\n",
            "=== MSCOCO FiG Metrics Summary ===\n",
            "+-------------+----------+-----------+--------+-------+-------------+\n",
            "|  Category   | Accuracy | Precision | Recall |  F1   | Num_Samples |\n",
            "+-------------+----------+-----------+--------+-------+-------------+\n",
            "|   Object    |  0.552   |   0.75    |  0.5   |  0.6  |     29      |\n",
            "|   Colour    |   0.8    |    1.0    |  0.8   | 0.889 |      5      |\n",
            "|   Number    |  0.895   |   0.882   | 0.882  | 0.882 |     19      |\n",
            "| Positional  |  0.455   |    1.0    | 0.455  | 0.625 |     11      |\n",
            "|    Text     |   nan    |    0.0    |  0.0   |  0.0  |      0      |\n",
            "| Overall Avg |  0.657   |   0.852   | 0.629  | 0.711 |     64      |\n",
            "+-------------+----------+-----------+--------+-------+-------------+\n",
            "Saved metrics summary to 'mscoco_fig_category_metrics_summary.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tabulate import tabulate\n",
        "\n",
        "# ==============================\n",
        "# CHAIR score function\n",
        "# ==============================\n",
        "def compute_chair(meta_caption: str, orig_caption: str) -> float:\n",
        "    meta_caption = str(meta_caption).lower()\n",
        "    orig_caption = str(orig_caption).lower()\n",
        "\n",
        "    meta_nouns = set(re.findall(r'\\b[a-z]+\\b', meta_caption))\n",
        "    orig_nouns = set(re.findall(r'\\b[a-z]+\\b', orig_caption))\n",
        "\n",
        "    if not meta_nouns:\n",
        "        return -1  # cannot compute\n",
        "    hallucinated = meta_nouns - orig_nouns\n",
        "    return len(hallucinated) / len(meta_nouns)\n",
        "\n",
        "# ==============================\n",
        "# Load final merged CSV\n",
        "# ==============================\n",
        "final_df = pd.read_csv(\"mscoco_all_models_fig_detailed.csv\")\n",
        "\n",
        "# Models and FiG categories\n",
        "models = ['Flux-Dev', 'SD2', 'SDXL']\n",
        "fig_categories = ['Object FiG', 'Colour FiG', 'Number FiG', 'Positional FiG', 'Text FiG']\n",
        "\n",
        "# Compute CHAIR scores for all rows\n",
        "final_df['CHAIR'] = final_df.apply(lambda row: compute_chair(row['Meta Caption'], row['mscoco_caption']), axis=1)\n",
        "\n",
        "# Build table\n",
        "table_data = []\n",
        "for model in models:\n",
        "    row = {'Model': model}\n",
        "    model_df = final_df[final_df['Model'] == model]\n",
        "\n",
        "    # FiG scores\n",
        "    for cat in fig_categories:\n",
        "        filtered = model_df[model_df[cat] != -1]  # skip -1 rows\n",
        "        if len(filtered) == 0:\n",
        "            row[cat] = \"N/A\"\n",
        "        else:\n",
        "            row[cat] = round(filtered[cat].mean(), 2)\n",
        "\n",
        "    # CHAIR score (skip -1 rows)\n",
        "    chair_filtered = model_df[model_df['CHAIR'] != -1]\n",
        "    if len(chair_filtered) == 0:\n",
        "        row['CHAIR_i'] = \"N/A\"\n",
        "    else:\n",
        "        row['CHAIR_i'] = round(chair_filtered['CHAIR'].mean(), 2)\n",
        "\n",
        "    table_data.append(row)\n",
        "\n",
        "# Convert to DataFrame and print table\n",
        "table_df = pd.DataFrame(table_data)\n",
        "print(tabulate(table_df, headers='keys', tablefmt='pretty', showindex=False))\n",
        "\n",
        "# Optional: save LaTeX table\n",
        "latex_code = table_df.to_latex(index=False, na_rep=\"N/A\", float_format=\"%.2f\")\n",
        "with open(\"mscoco_fig_table.tex\", \"w\") as f:\n",
        "    f.write(latex_code)\n",
        "print(\"\\nSaved LaTeX table to 'mscoco_fig_table.tex'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ktin33qsSE8",
        "outputId": "c1ffcafa-b028-46ad-da8d-de85bb778aaa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+------------+------------+----------------+----------+---------+\n",
            "|  Model   | Object FiG | Colour FiG | Number FiG | Positional FiG | Text FiG | CHAIR_i |\n",
            "+----------+------------+------------+------------+----------------+----------+---------+\n",
            "| Flux-Dev |    0.46    |    1.0     |    0.93    |      0.0       |   N/A    |  0.87   |\n",
            "|   SD2    |    0.51    |    N/A     |    0.86    |      0.0       |   N/A    |  0.91   |\n",
            "|   SDXL   |    0.29    |    0.67    |    1.0     |      0.0       |   N/A    |  0.91   |\n",
            "+----------+------------+------------+------------+----------------+----------+---------+\n",
            "\n",
            "Saved LaTeX table to 'mscoco_fig_table.tex'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Set the name of the zip file\n",
        "zip_name = \"all_files.zip\"\n",
        "\n",
        "# Zip the entire runtime folder (current working directory)\n",
        "shutil.make_archive(\"all_files\", 'zip', os.getcwd())\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jBE3gH4JwUVU",
        "outputId": "30e31169-6f0f-4dde-8b39-34febd74efbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eacb6125-604f-4f08-b00c-e3ab523708ea\", \"all_files.zip\", 7108313)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}