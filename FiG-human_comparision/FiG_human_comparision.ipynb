{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "#          IMPORTS\n",
        "# ==============================\n",
        "# Importing necessary libraries for handling CSV/Excel files, text processing, and type annotations\n",
        "import pandas as pd       # Handling tabular data (CSV/Excel) efficiently\n",
        "import numpy as np        # Handling numerical operations if needed\n",
        "import spacy              # Performing natural language processing (tokenization, parsing)\n",
        "from typing import Set, Dict  # Providing type hints for functions and variables\n",
        "import re                 # Using regular expressions to extract patterns from text\n",
        "\n",
        "# ==============================\n",
        "#        LOAD SPACY MODEL\n",
        "# ==============================\n",
        "# Trying to load the small English SpaCy model for natural language processing\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")  # Loading pre-trained English model\n",
        "except OSError:\n",
        "    # Downloading model if not already installed\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")  # Loading it after download\n",
        "\n",
        "# Adding a pipeline component to merge multi-token entities into single tokens\n",
        "# This is helping to treat entities like \"New York\" or \"Red Apple\" as single units\n",
        "nlp.add_pipe(\"merge_entities\")\n",
        "\n",
        "# ==============================\n",
        "#        FINE-GRAINED METRICS\n",
        "# ==============================\n",
        "# Defining a class for computing object-level metrics (color, number, text, position)\n",
        "class FineGrainedMetrics:\n",
        "\n",
        "    # Checking if a given attribute word is related to a noun in the parsed document\n",
        "    @staticmethod\n",
        "    def related_to_noun(doc, attribute: str, noun: str) -> bool:\n",
        "        # Iterating through each token in the document\n",
        "        for token in doc:\n",
        "            if token.text == noun:\n",
        "                # Looking inside the noun's subtree to see if the attribute exists\n",
        "                if attribute in [t.text for t in token.subtree]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    # Computing color consistency between meta and original captions\n",
        "    @classmethod\n",
        "    def color(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        # Defining a set of colors we are considering\n",
        "        COLORS = {'red','blue','green','yellow','black','white','gray','grey',\n",
        "                  'orange','pink','purple','brown','violet','indigo','turquoise',\n",
        "                  'cyan','magenta'}\n",
        "        score, original_colors = 0.0, set()\n",
        "        # Iterating over each noun in the generated caption\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting colors linked to the noun in meta and original captions\n",
        "            meta_colors = {t.text for t in meta\n",
        "                           if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            orig_colors = {t.text for t in orig\n",
        "                           if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'acomp','amod'} and t.text.lower() in COLORS}\n",
        "            original_colors.update(orig_colors)\n",
        "            if orig_colors:\n",
        "                # Adding overlap between meta and original colors to score\n",
        "                score += len(orig_colors & meta_colors)\n",
        "        # Returning -1 if no color is found in original (cannot compute), else computing ratio\n",
        "        return -1 if not original_colors else score / len(original_colors)\n",
        "\n",
        "    # Computing number consistency (quantity of objects) between meta and original captions\n",
        "    @classmethod\n",
        "    def number(cls, meta, orig, generated_nouns: Set[str]) -> float:\n",
        "        # Mapping common words to numbers for comparison\n",
        "        QUANTITY_MAP = {'a':'1','an':'1','the':'1','one':'1','two':'2','three':'3',\n",
        "                        'couple':'2','few':'3','several':'4','many':'5','dozen':'12'}\n",
        "        score, original_numbers = 0.0, set()\n",
        "        for noun in generated_nouns:\n",
        "            # Extracting numbers associated with nouns in meta and original captions\n",
        "            meta_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in meta\n",
        "                         if cls.related_to_noun(meta, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            orig_nums = {QUANTITY_MAP.get(t.text.lower(), t.text) for t in orig\n",
        "                         if cls.related_to_noun(orig, t.text, noun) and t.dep_ in {'nummod','det'}}\n",
        "            original_numbers.update(orig_nums)\n",
        "            if orig_nums:\n",
        "                score += len(orig_nums & meta_nums)\n",
        "        if not original_numbers:\n",
        "            return -1\n",
        "        raw_val = score / len(original_numbers)\n",
        "        return min(raw_val, 1.0)  # <-- FIX: Cap at 1.0 so Number FiG never exceeds 1\n",
        "\n",
        "    # Computing consistency for textual content (quotes or written words)\n",
        "    @classmethod\n",
        "    def text(cls, meta, orig, _: Set[str]) -> float:\n",
        "        TEXT_INDICATORS = {'written','saying','says','reading','text'}\n",
        "        QUOTE_PATTERN = r'[\\\"\\'«»“”]([^\\\"\\'«»“”]*)[\\\"\\'«»“”]'\n",
        "        if any(t.text in TEXT_INDICATORS for t in orig):\n",
        "            # Extracting quoted text from captions\n",
        "            orig_matches = re.findall(QUOTE_PATTERN, orig.text)\n",
        "            meta_matches = re.findall(QUOTE_PATTERN, meta.text)\n",
        "            if not orig_matches:\n",
        "                return -1  # Cannot compute if original has no quotes\n",
        "            # Normalizing text to lowercase and removing spaces for comparison\n",
        "            orig_norm = [''.join(s.lower().split()) for s in orig_matches]\n",
        "            meta_norm = [''.join(s.lower().split()) for s in meta_matches]\n",
        "            # Counting how many original quotes appear in meta\n",
        "            matches = sum(any(o in m for m in meta_norm) for o in orig_norm)\n",
        "            return matches / len(orig_matches)\n",
        "        return -1  # Returning -1 if no textual indicators\n",
        "\n",
        "    # Extracting spatial relationships (prepositions) to evaluate position consistency\n",
        "    @staticmethod\n",
        "    def extract_spatial_relations(doc):\n",
        "        rels = set()\n",
        "        for token in doc:\n",
        "            if token.dep_ == 'prep':\n",
        "                # Collecting objects related to prepositions\n",
        "                pobjects = [child for child in token.children if child.dep_ == 'pobj']\n",
        "                if pobjects:\n",
        "                    pobj = pobjects[0].text\n",
        "                    subj = None\n",
        "                    # Searching for subject/object linked to preposition\n",
        "                    for anc in token.ancestors:\n",
        "                        if anc.dep_ in {'nsubj','nsubjpass','dobj','pobj'}:\n",
        "                            subj = anc.text\n",
        "                            break\n",
        "                    if subj:\n",
        "                        rels.add((subj, token.text, pobj))\n",
        "        return rels\n",
        "\n",
        "    # Computing positional consistency\n",
        "    @classmethod\n",
        "    def position(cls, meta, orig, _: Set[str]) -> float:\n",
        "        orig_rel = cls.extract_spatial_relations(orig)\n",
        "        meta_rel = cls.extract_spatial_relations(meta)\n",
        "        # Returning -1 if no original relationships exist\n",
        "        return -1 if not orig_rel else len(orig_rel & meta_rel) / len(orig_rel)\n",
        "\n",
        "# ==============================\n",
        "#        ANALYZE CAPTION PAIR\n",
        "# ==============================\n",
        "# Comparing meta-generated caption with original to compute all fine-grained metrics\n",
        "def analyze_caption_pair(meta_caption: str, orig_caption: str) -> Dict[str, float]:\n",
        "    meta_caption = str(meta_caption) if pd.notna(meta_caption) else \"\"  # Handling missing values\n",
        "    orig_caption = str(orig_caption) if pd.notna(orig_caption) else \"\"\n",
        "    # Parsing captions using SpaCy NLP model\n",
        "    meta_doc, orig_doc = nlp(meta_caption), nlp(orig_caption)\n",
        "    # Extracting nouns and proper nouns\n",
        "    meta_nouns = {t.text for t in meta_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    orig_nouns = {t.text for t in orig_doc if t.pos_ in {\"NOUN\",\"PROPN\"}}\n",
        "    common_nouns = orig_nouns & meta_nouns\n",
        "    noun_recall = len(common_nouns) / len(orig_nouns) if orig_nouns else 0\n",
        "    # Returning dictionary of fine-grained metrics\n",
        "    return {\n",
        "        \"Object FiG\": noun_recall,\n",
        "        \"Colour FiG\": FineGrainedMetrics.color(meta_doc, orig_doc, common_nouns),\n",
        "        \"Number FiG\": FineGrainedMetrics.number(meta_doc, orig_doc, common_nouns),\n",
        "        \"Positional FiG\": FineGrainedMetrics.position(meta_doc, orig_doc, common_nouns),\n",
        "        \"Text FiG\": FineGrainedMetrics.text(meta_doc, orig_doc, common_nouns)\n",
        "    }\n",
        "\n",
        "# ==============================\n",
        "#       PROCESS MODEL CSV\n",
        "# ==============================\n",
        "# Reading model-generated CSV and computing metrics for each row\n",
        "def process_model_csv(file_path: str):\n",
        "    df = pd.read_csv(file_path).dropna(subset=[\"mscoco_caption\", \"Meta Caption\"]).reset_index(drop=True)\n",
        "    results = [analyze_caption_pair(row[\"Meta Caption\"], row[\"mscoco_caption\"]) for _, row in df.iterrows()]\n",
        "    metrics_df = pd.DataFrame(results)\n",
        "    # Combining original CSV data with computed metrics\n",
        "    return pd.concat([df, metrics_df], axis=1)\n",
        "\n",
        "# ==============================\n",
        "#      PARSE HUMAN RESPONSES\n",
        "# ==============================\n",
        "# Reading human annotations and organizing them per image and model\n",
        "def parse_human_responses(file_path: str) -> pd.DataFrame:\n",
        "    df = pd.read_excel(file_path, header=None)\n",
        "    rows = []\n",
        "    for col in df.columns:\n",
        "        caption_cell = str(df.iloc[0, col])\n",
        "        match = re.match(r\"\\[(Flux-Dev|SD2|SDXL)\\] Image: (.*?) \\| Caption: (.*)\", caption_cell)\n",
        "        if not match:\n",
        "            continue\n",
        "        model, image, caption = match.groups()\n",
        "        human_labels = []\n",
        "        # Looping over 3 annotators\n",
        "        for annot_idx in range(1, 4):\n",
        "            resp_cell = str(df.iloc[annot_idx, col]).strip()\n",
        "            human_label = \"Yes\" if resp_cell.startswith(\"Yes\") else \"No\"\n",
        "            not_aligning = \", \".join(re.findall(r\"(Color|Position|Text|Number|Object|Others)\", resp_cell)) if human_label==\"No\" else \"\"\n",
        "            human_labels.append(f\"{human_label} ({not_aligning})\" if not_aligning else human_label)\n",
        "        rows.append({\n",
        "            \"Model\": model,\n",
        "            \"Image\": image,\n",
        "            \"Caption\": caption,\n",
        "            \"Human_Responses\": \"; \".join(human_labels)\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# ==============================\n",
        "#          HUMAN MAJORITY\n",
        "# ==============================\n",
        "# Computing majority vote among human annotators\n",
        "def compute_human_majority(human_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    def majority_vote(responses):\n",
        "        # Counting how many annotators said Yes\n",
        "        return 'Yes' if sum([r.startswith('Yes') for r in responses.split(\"; \")]) >= 2 else 'No'\n",
        "    human_df['Human_Majority'] = human_df['Human_Responses'].apply(majority_vote)\n",
        "    return human_df\n",
        "\n",
        "# ==============================\n",
        "#          MERGE & CONSISTENCY\n",
        "# ==============================\n",
        "# Merging model metrics with human judgments and computing consistency\n",
        "def merge_with_human(model_df: pd.DataFrame, human_df: pd.DataFrame, model_name: str) -> pd.DataFrame:\n",
        "    # Joining datasets on image name and caption text\n",
        "    merged = pd.merge(\n",
        "        model_df,\n",
        "        human_df[human_df['Model']==model_name][['Image','Caption','Human_Responses','Human_Majority']],\n",
        "        left_on=['image_name','mscoco_caption'],\n",
        "        right_on=['Image','Caption'],\n",
        "        how='left'\n",
        "    ).drop(columns=['Image','Caption'])\n",
        "\n",
        "    fig_columns = ['Object FiG','Colour FiG','Number FiG','Positional FiG','Text FiG']\n",
        "\n",
        "    # Computing consistency per FiG metric\n",
        "    def check_consistency(row):\n",
        "        consistency = {}\n",
        "        for col in fig_columns:\n",
        "            val = row.get(col, None)\n",
        "            if val == -1 or pd.isna(val):\n",
        "                # -1 or NaN indicates we cannot evaluate this metric\n",
        "                consistency[col+'_Consistency'] = None\n",
        "            else:\n",
        "                # Special handling for Number FiG: cap at 1\n",
        "                val_to_check = min(val, 1) if col == 'Number FiG' else val\n",
        "                if row['Human_Majority'] == 'Yes':\n",
        "                    consistency[col+'_Consistency'] = 1 if val_to_check == 1 else 0\n",
        "                else:\n",
        "                    consistency[col+'_Consistency'] = 1 if val_to_check < 1 else 0\n",
        "        return pd.Series(consistency)\n",
        "\n",
        "    # Apply consistency computation to all rows\n",
        "    merged[[c+'_Consistency' for c in fig_columns]] = merged.apply(check_consistency, axis=1)\n",
        "    merged['Model'] = model_name\n",
        "    return merged\n",
        "\n",
        "# ==============================\n",
        "#            MAIN PIPELINE\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # --- Step 1: Process model CSVs ---\n",
        "    sd2_df = process_model_csv(\"mscoco_sd2_caps.csv\")\n",
        "    sdxl_df = process_model_csv(\"mscoco_sdxl_caps.csv\")\n",
        "    flux_df = process_model_csv(\"mscoco_fluxdev_caps.csv\")\n",
        "\n",
        "    # --- Step 2: Parse human responses and compute majority votes ---\n",
        "    human_df = parse_human_responses(\"human_responses.xlsx\")\n",
        "    human_df = compute_human_majority(human_df)\n",
        "\n",
        "    # --- Step 3: Merge metrics with human judgments ---\n",
        "    sd2_merged = merge_with_human(sd2_df, human_df, \"SD2\")\n",
        "    sdxl_merged = merge_with_human(sdxl_df, human_df, \"SDXL\")\n",
        "    flux_merged = merge_with_human(flux_df, human_df, \"Flux-Dev\")\n",
        "\n",
        "    # --- Step 4: Combine all models into one CSV ---\n",
        "    final_df = pd.concat([sd2_merged, sdxl_merged, flux_merged], ignore_index=True)\n",
        "    final_df.to_csv(\"all_models_human_fig_detailed.csv\", index=False)\n",
        "    print(\"Final merged CSV with detailed human responses saved as all_models_human_fig_detailed.csv\")\n",
        "\n",
        "    # --- Step 5: Compute average consistency per FiG metric for each model ---\n",
        "    fig_consistency_cols = ['Object FiG_Consistency','Colour FiG_Consistency','Number FiG_Consistency',\n",
        "                            'Positional FiG_Consistency','Text FiG_Consistency']\n",
        "    avg_consistency = final_df.groupby('Model')[fig_consistency_cols].mean().round(3).reset_index()\n",
        "    avg_consistency.to_csv(\"avg_fig_consistency_per_model.csv\", index=False)\n",
        "    print(\"Average consistency per model saved as avg_fig_consistency_per_model.csv\")\n",
        "    print(avg_consistency)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3tHJRqgFSJ4",
        "outputId": "2d1b87ac-2d4c-4a86-d53c-6ea4fbca3c81"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final merged CSV with detailed human responses saved as all_models_human_fig_detailed.csv\n",
            "Average consistency per model saved as avg_fig_consistency_per_model.csv\n",
            "      Model  Object FiG_Consistency  Colour FiG_Consistency  \\\n",
            "0  Flux-Dev                   0.100                   0.500   \n",
            "1       SD2                   0.556                     NaN   \n",
            "2      SDXL                   0.800                   0.667   \n",
            "\n",
            "   Number FiG_Consistency  Positional FiG_Consistency  Text FiG_Consistency  \n",
            "0                   0.714                       0.000                   NaN  \n",
            "1                   0.286                       0.667                   NaN  \n",
            "2                   0.200                       0.500                   NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the detailed CSV\n",
        "df = pd.read_csv(\"all_models_human_fig_detailed.csv\")\n",
        "\n",
        "# List of FiG metric columns\n",
        "fig_cols = ['Object FiG','Colour FiG','Number FiG','Positional FiG','Text FiG']\n",
        "\n",
        "# Replace -1 with NaN (cannot compute)\n",
        "df[fig_cols] = df[fig_cols].replace(-1, pd.NA)\n",
        "\n",
        "# Compute average normal FiG scores per model\n",
        "avg_fig_scores = df.groupby('Model')[fig_cols].mean().round(3).reset_index()\n",
        "\n",
        "# Save to CSV\n",
        "avg_fig_scores.to_csv(\"avg_fig_scores_per_model.csv\", index=False)\n",
        "\n",
        "print(\"Average FiG scores per model saved as avg_fig_scores_per_model.csv\")\n",
        "print(avg_fig_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-6X6jgVSz8J",
        "outputId": "786be520-4fe3-42a6-80d5-b21f26b8736c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average FiG scores per model saved as avg_fig_scores_per_model.csv\n",
            "      Model  Object FiG Colour FiG Number FiG Positional FiG Text FiG\n",
            "0  Flux-Dev       0.465        1.0   0.928571            0.0      NaN\n",
            "1       SD2       0.509        NaN   0.857143            0.0      NaN\n",
            "2      SDXL       0.292   0.666667        1.0            0.0      NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# ==============================\n",
        "#      CHAIR SCORE FUNCTION\n",
        "# ==============================\n",
        "# CHAIR score measures object hallucination: fraction of objects in generated caption that are *not* in ground-truth\n",
        "def compute_chair(meta_caption: str, orig_caption: str) -> float:\n",
        "    meta_caption = str(meta_caption).lower()\n",
        "    orig_caption = str(orig_caption).lower()\n",
        "\n",
        "    # Extract nouns (very simple version using words with letters only)\n",
        "    meta_nouns = set(re.findall(r'\\b[a-z]+\\b', meta_caption))\n",
        "    orig_nouns = set(re.findall(r'\\b[a-z]+\\b', orig_caption))\n",
        "\n",
        "    if not meta_nouns:\n",
        "        return -1  # cannot compute\n",
        "    # Objects in meta that are NOT in original (hallucinated)\n",
        "    hallucinated = meta_nouns - orig_nouns\n",
        "    return len(hallucinated) / len(meta_nouns)\n",
        "\n",
        "# ==============================\n",
        "#       PROCESS MODEL CSV\n",
        "# ==============================\n",
        "def add_chair_score(file_path: str, model_name: str):\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['CHAIR'] = df.apply(lambda row: compute_chair(row['Meta Caption'], row['mscoco_caption']), axis=1)\n",
        "    df['Model'] = model_name\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "#         MAIN\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "    sd2_df = add_chair_score(\"mscoco_sd2_caps.csv\", \"SD2\")\n",
        "    sdxl_df = add_chair_score(\"mscoco_sdxl_caps.csv\", \"SDXL\")\n",
        "    flux_df = add_chair_score(\"mscoco_fluxdev_caps.csv\", \"Flux-Dev\")\n",
        "\n",
        "    # Combine all models\n",
        "    all_models_chair = pd.concat([sd2_df, sdxl_df, flux_df], ignore_index=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    all_models_chair.to_csv(\"all_models_chair_scores.csv\", index=False)\n",
        "    print(\"CHAIR scores computed and saved to all_models_chair_scores.csv\")\n",
        "\n",
        "    # Optional: average CHAIR per model\n",
        "    avg_chair = all_models_chair.groupby(\"Model\")['CHAIR'].mean().round(3).reset_index()\n",
        "    print(avg_chair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-ZXImVXIyfj",
        "outputId": "55b859e2-19d9-4d03-bd87-48dd6b380bd5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAIR scores computed and saved to all_models_chair_scores.csv\n",
            "      Model  CHAIR\n",
            "0  Flux-Dev  0.869\n",
            "1       SD2  0.816\n",
            "2      SDXL  0.906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "# Name of the output zip\n",
        "zip_name = \"FiG-human_comparision.zip\"\n",
        "\n",
        "# Find all CSV files in the current directory\n",
        "csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
        "\n",
        "# Create the zip and add all CSVs\n",
        "with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
        "    for f in csv_files:\n",
        "        zipf.write(f)\n",
        "\n",
        "# Trigger download in Colab\n",
        "files.download(zip_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "l54cH-UaJ-Sl",
        "outputId": "2582fbdd-c754-458f-89c4-54bfafd4cbef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6e8fc9f1-5322-41d3-b703-43a616c2d2a8\", \"FiG-human_comparision.zip\", 45509)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}