{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClvIe8rHos1d"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Load Qwen Model\n",
        "# ========================\n",
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\" Qwen loaded successfully in 8-bit mode\")\n"
      ],
      "metadata": {
        "id": "2Le3elY4ozfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# Paths\n",
        "# ========================\n",
        "input_json = \"/content/drive/MyDrive/Flux-Dev_mscoco_dense_caps.json\"      # Replace these paths correspondingly for SDXL, SD2\n",
        "output_csv = \"/content/drive/MyDrive/FluxDev_QWEN_META_CAPS.csv\"\n",
        "\n",
        "# ========================\n",
        "# Load JSON\n",
        "# ========================\n",
        "with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\" Loaded {len(data)} images from {input_json}\")\n",
        "\n",
        "\n",
        "# ========================\n",
        "# Meta-caption function\n",
        "# ========================\n",
        "def get_meta_caption(dense_captions, meta_model, meta_tokenizer, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Combine multiple dense captions into one unified meta-caption.\n",
        "    \"\"\"\n",
        "    pattern = r\"<caption>(.*?)</caption>\"\n",
        "\n",
        "    # Prompt\n",
        "    prompt = \"\"\"I am providing you with captions for sub-regions of an image.\n",
        "You need to stitch all the captions into one unified caption for the entire image.\n",
        "Do not add new information. Keep it short but detailed.\n",
        "Ignore mentions of backgrounds. Do not hallucinate details.\n",
        "Return the final caption wrapped inside <caption></caption> tags.\n",
        "\"\"\"\n",
        "\n",
        "    # Deduplicate + filter blanks\n",
        "    seen = set()\n",
        "    filtered_caps = []\n",
        "    for cap in dense_captions:\n",
        "        cap = cap.strip()\n",
        "        if cap != \"\" and cap not in seen:\n",
        "            seen.add(cap)\n",
        "            filtered_caps.append(cap)\n",
        "\n",
        "    for i, cap in enumerate(filtered_caps):\n",
        "        prompt += f\"{i+1}. {cap}\\n\"\n",
        "\n",
        "    if not filtered_caps:\n",
        "        return \"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a meta image captioning model. You combine multiple sub-captions into one coherent grounded caption.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "\n",
        "    # Format for Qwen chat\n",
        "    text = meta_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = meta_tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate\n",
        "    generated_ids = meta_model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=400,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = meta_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Extract caption\n",
        "    try:\n",
        "        response = re.findall(pattern, response, re.DOTALL)[0]\n",
        "        return response.strip()\n",
        "    except:\n",
        "        return response.strip()\n",
        "\n",
        "# ========================\n",
        "# Run metacaptioning\n",
        "# ========================\n",
        "rows = []\n",
        "for image_name, segments in tqdm(data.items(), desc=\"Processing Images\"):\n",
        "    dense_captions = list(segments.values())\n",
        "    meta_caption = get_meta_caption(dense_captions, model, tokenizer, device)\n",
        "    rows.append({\n",
        "        \"image_name\": image_name,\n",
        "        \"Meta_caption_Qwen\": meta_caption\n",
        "    })\n",
        "\n",
        "# ========================\n",
        "# Save CSV\n",
        "# ========================\n",
        "df = pd.DataFrame(rows)\n",
        "df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"  Meta-captions saved to {output_csv}\")\n"
      ],
      "metadata": {
        "id": "MKrtkf58pifB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}