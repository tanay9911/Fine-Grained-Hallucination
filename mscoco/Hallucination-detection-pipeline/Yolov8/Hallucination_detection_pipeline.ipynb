{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas sentence-transformers scikit-learn\n"
      ],
      "metadata": {
        "id": "-YH3fL5xyPdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------\n",
        "# Step 1: Load Required Libraries\n",
        "# -------------------------------\n",
        "# Importing necessary libraries for file operations, natural language processing, and data processing\n",
        "import os\n",
        "import nltk\n",
        "\n",
        "# Downloading required NLTK datasets for tokenization and lemmatization\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Importing pandas for data manipulation and JSON handling\n",
        "import pandas as pd\n",
        "import json\n",
        "import ast\n",
        "\n",
        "# Importing sentence transformers for semantic similarity and cosine similarity calculations\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Initialize Tools\n",
        "# -------------------------------\n",
        "# Initializing the WordNet lemmatizer for text normalization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Loading the pre-trained sentence transformer model for semantic embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Load MSCOCO Entities\n",
        "# -------------------------------\n",
        "# Reading the MSCOCO captions entities CSV file into a DataFrame\n",
        "mscoco_df = pd.read_csv('mscoco_captions_entities.csv')\n",
        "\n",
        "# Creating a dictionary to store image names and their associated entities\n",
        "mscoco_entities = {}\n",
        "for _, row in mscoco_df.iterrows():\n",
        "    image = row['image_name']\n",
        "    # Converting string representation of entities list to actual list\n",
        "    entities = ast.literal_eval(row['entities'])\n",
        "    # Storing entities as a set of lowercase strings for case-insensitive matching\n",
        "    mscoco_entities[image] = set(e.strip().lower() for e in entities)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Load YOLOv8 Detections\n",
        "# -------------------------------\n",
        "# Loading YOLOv8 detection results from JSON file\n",
        "with open('Flux-Dev_yolov8_detections.json', 'r') as f:  # Replace the file name with SDXL_1.0 , SD2 for their corresponding computation\n",
        "    detection_data = json.load(f)\n",
        "\n",
        "# Defining the COCO dataset class labels for mapping label IDs to names\n",
        "coco_labels = [\n",
        "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
        "    \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
        "    \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\",\n",
        "    \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\",\n",
        "    \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\",\n",
        "    \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\",\n",
        "    \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\",\n",
        "    \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\",\n",
        "    \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\",\n",
        "    \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Text Normalization Utils\n",
        "# -------------------------------\n",
        "# Defining function to normalize text by tokenizing, lowercasing, and lemmatizing\n",
        "def normalize(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    return \" \".join(lemmatizer.lemmatize(t) for t in tokens if t.isalpha())\n",
        "\n",
        "# Defining function to check semantic relationship between two words\n",
        "def is_semantically_related(word1, word2):\n",
        "    # Normalizing both input words\n",
        "    norm1 = normalize(word1)\n",
        "    norm2 = normalize(word2)\n",
        "\n",
        "    # Converting normalized texts to sets of tokens\n",
        "    tokens1, tokens2 = set(norm1.split()), set(norm2.split())\n",
        "    if not tokens1 or not tokens2:\n",
        "        return False\n",
        "\n",
        "    # Calculating Jaccard similarity between token sets\n",
        "    jaccard = len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
        "    if jaccard >= 0.5:\n",
        "        return True\n",
        "\n",
        "    # Checking WordNet synsets for semantic relationships\n",
        "    for t1 in tokens1:\n",
        "        for t2 in tokens2:\n",
        "            s1 = wn.synsets(t1)\n",
        "            s2 = wn.synsets(t2)\n",
        "            if not s1 or not s2:\n",
        "                continue\n",
        "            for syn1 in s1:\n",
        "                for syn2 in s2:\n",
        "                    if syn1 == syn2:\n",
        "                        return True\n",
        "                    # Checking if one synset is a hypernym of the other\n",
        "                    if syn1 in syn2.closure(lambda s: s.hypernyms()):\n",
        "                        return True\n",
        "                    if syn2 in syn1.closure(lambda s: s.hypernyms()):\n",
        "                        return True\n",
        "    return False\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Compare YOLO Labels to MSCOCO\n",
        "# -------------------------------\n",
        "# Initializing list to store comparison results and dictionary for CHAIR score calculation\n",
        "results = []\n",
        "image_to_labels = {}  # For CHAIRi\n",
        "\n",
        "# Processing each detection entry from YOLOv8 results\n",
        "for entry in detection_data:\n",
        "    # Extracting and normalizing image name to match MSCOCO format\n",
        "    image_name_full = entry['image']\n",
        "    image_name = image_name_full.replace(\"Flux-Dev-Pregen-\", \"\").replace(\".jpeg\", \"\") # Here we are removing the prefixes of images names inside the json so that they match MSCOCO's image names\n",
        "                                        #Replace the file name with SDXL_1.0 , SD2 for their corresponding computation\n",
        "    # Extracting detected labels from YOLO bounding boxes\n",
        "    detected_labels = set()\n",
        "    for box in entry['boxes']:\n",
        "        label_id = box['label']\n",
        "        if label_id < len(coco_labels):\n",
        "            detected_labels.add(coco_labels[label_id].strip().lower())\n",
        "\n",
        "    # Retrieving ground truth entities for the current image\n",
        "    ground_truth_entities = mscoco_entities.get(image_name, set())\n",
        "    if not ground_truth_entities:\n",
        "        continue\n",
        "\n",
        "    # Generating embeddings for all ground truth entities\n",
        "    entity_list = list(ground_truth_entities)\n",
        "    entity_embeddings = model.encode(entity_list, convert_to_tensor=True)\n",
        "\n",
        "    # Initializing set to track hallucinated labels for this image\n",
        "    hallucinated_labels = set()\n",
        "\n",
        "    # Comparing each detected label with ground truth entities\n",
        "    for label in detected_labels:\n",
        "        # Generating embedding for the current detected label\n",
        "        label_embedding = model.encode(label, convert_to_tensor=True)\n",
        "        # Calculating cosine similarities between label and all entities\n",
        "        sims = util.cos_sim(label_embedding, entity_embeddings)[0]\n",
        "        max_sim = sims.max().item()\n",
        "        matched_entity = entity_list[sims.argmax().item()]\n",
        "\n",
        "        # Checking semantic relationship between label and matched entity\n",
        "        if is_semantically_related(label, matched_entity):\n",
        "            hallucinated = False\n",
        "        else:\n",
        "            # Marking as hallucinated if similarity is below threshold\n",
        "            hallucinated = max_sim < 0.45\n",
        "\n",
        "        # Recording hallucinated labels for CHAIR score calculation\n",
        "        if hallucinated:\n",
        "            hallucinated_labels.add(label)\n",
        "\n",
        "        # Storing comparison results for this label\n",
        "        results.append({\n",
        "            'image_name': image_name,\n",
        "            'yolo_label': label,\n",
        "            'matched_entity': matched_entity,\n",
        "            'similarity_score': round(max_sim, 4),\n",
        "            'hallucinated': hallucinated\n",
        "        })\n",
        "\n",
        "    # Storing hallucination data for CHAIR Score computation\n",
        "    if detected_labels:\n",
        "        image_to_labels[image_name] = {\n",
        "            'hallucinated': hallucinated_labels,\n",
        "            'all_detected': detected_labels\n",
        "        }\n",
        "\n",
        "# -------------------------------\n",
        "# Step 7: Save Results\n",
        "# -------------------------------\n",
        "# Converting results to DataFrame and saving to CSV files\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('Flux-Dev_yolov8_vs_mscoco_hallucination_check.csv', index=False)             #Replace the file name with SDXL_1.0 , SD2 for their corresponding file generations\n",
        "results_df[results_df['hallucinated']].to_csv('Flux-Dev_yolov8_hallucinations_only.csv', index=False)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 8: Final Metrics\n",
        "# -------------------------------\n",
        "# Calculating various evaluation metrics\n",
        "total_preds = len(results_df)\n",
        "hallucinations = results_df['hallucinated'].sum()\n",
        "correct_detections = total_preds - hallucinations\n",
        "precision = correct_detections / total_preds if total_preds > 0 else 0.0\n",
        "hallucination_rate = hallucinations / total_preds if total_preds > 0 else 0.0\n",
        "\n",
        "# Calculating CHAIR score (average over images)\n",
        "chair_scores = []\n",
        "for v in image_to_labels.values():\n",
        "    total = len(v['all_detected'])\n",
        "    hallucinated = len(v['hallucinated'])\n",
        "    if total > 0:\n",
        "        chair_scores.append(hallucinated / total)\n",
        "CHAIR = sum(chair_scores) / len(chair_scores) if chair_scores else 0.0\n",
        "\n",
        "# Calculating recall metric\n",
        "total_gt_entities = sum(len(ents) for ents in mscoco_entities.values())\n",
        "recall = correct_detections / total_gt_entities if total_gt_entities > 0 else 0.0\n",
        "\n",
        "# Calculating F1 Score from precision and recall\n",
        "if precision + recall > 0:\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "else:\n",
        "    f1_score = 0.0\n",
        "\n",
        "# Printing all calculated metrics\n",
        "print(f\" Total YOLO Predictions Checked: {total_preds}\")\n",
        "print(f\" Correct Detections (Matched MSCOCO): {correct_detections}\")\n",
        "print(f\" Hallucinated Labels (No Match): {hallucinations}\")\n",
        "print(f\" Precision: {precision:.4f}\")\n",
        "print(f\" Recall: {recall:.4f}\")\n",
        "print(f\" F1 Score: {f1_score:.4f}\")\n",
        "print(f\" Hallucination Rate: {hallucination_rate:.4f}\")\n",
        "print(f\" CHAIR Score (avg over images): {CHAIR:.4f}\")"
      ],
      "metadata": {
        "id": "B_HODNZ4mcM0",
        "outputId": "3866b2dd-2742-4b9b-cd76-e043e03f9bfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Total YOLO Predictions Checked: 507\n",
            " Correct Detections (Matched MSCOCO): 350\n",
            " Hallucinated Labels (No Match): 157\n",
            " Precision: 0.6903\n",
            " Recall: 0.5636\n",
            " F1 Score: 0.6206\n",
            " Hallucination Rate: 0.3097\n",
            " CHAIR Score (avg over images): 0.2378\n"
          ]
        }
      ]
    }
  ]
}