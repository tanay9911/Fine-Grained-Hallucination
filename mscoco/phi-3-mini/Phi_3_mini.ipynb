{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "hf_token = \"YOUR_TOKEN_HERE\"\n",
        "# ===============================\n",
        "# 1. Load Phi-3-mini-4k\n",
        "# ===============================\n",
        "model_name = \"microsoft/phi-3-mini-4k-instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",   # GPU if available\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# 2. Function: extract noun phrases\n",
        "# ===============================\n",
        "def get_entities(caption: str):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": (\n",
        "            \"Extract all the noun phrases in the given sentence. \"\n",
        "            \"Return them separated by commas, without rephrasing or extra text. \"\n",
        "            \"Only keep phrases that contain a noun. \"\n",
        "            f\"\\nSentence: {caption}\\nEntities:\"\n",
        "        )}\n",
        "    ]\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": 50,\n",
        "        \"return_full_text\": False,\n",
        "        \"temperature\": 0.0,\n",
        "        \"do_sample\": False\n",
        "    }\n",
        "    output = pipe(messages, **generation_args)\n",
        "    text = output[0]['generated_text'].strip()\n",
        "    # Convert comma-separated string -> list\n",
        "    entities = [ent.strip() for ent in text.split(\",\") if ent.strip()]\n",
        "    return entities\n",
        "\n",
        "# ===============================\n",
        "# 3. Load captions CSV\n",
        "# ===============================\n",
        "df = pd.read_csv(\"mscoco_captions.csv\")\n",
        "\n",
        "df[\"entities\"] = df[\"mscoco_caption\"].apply(get_entities)\n",
        "\n",
        "# ===============================\n",
        "# 4. Save results\n",
        "# ===============================\n",
        "df.to_csv(\"mscoco_captions_entities.csv\", index=False)\n",
        "print(\"Saved mscoco_captions_entities.csv with entities extracted for\", len(df), \"captions.\")\n"
      ],
      "metadata": {
        "id": "T8Z3kX3kJ9-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}