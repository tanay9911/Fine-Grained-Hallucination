{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 12:58:28.054362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-19 12:58:28.054443: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-19 12:58:28.193028: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-19 12:58:28.430924: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-19 12:58:30.169118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-09-19 12:58:32.461207: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 12:58:32.461540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-19 12:58:32.461621: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pos_tag_pipe = spacy.load(\"en_core_web_sm\")\n",
    "pos_tag_pipe.add_pipe(\"merge_entities\")\n",
    "# pos_tag_pipe.add_pipe(\"merge_noun_chunks\")\n",
    "sdxl_captions = pd.read_csv('./meta_captions_sdxl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_categories = ['Colors', 'Positional', 'Counting', 'Descriptions']\n",
    "df = pd.read_csv('/notebooks/Fine-Grained-Hallucination/DrawBenchPrompts.csv')\n",
    "df = df.loc[df['Category'].isin(target_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('target_prompt_dataset.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Reddit                 38\n",
       "Colors                 25\n",
       "Text                   21\n",
       "DALL-E                 20\n",
       "Descriptions           20\n",
       "Positional             20\n",
       "Counting               19\n",
       "Conflicting            10\n",
       "Gary Marcus et al.     10\n",
       "Misspellings           10\n",
       "Rare Words              7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdxl_captions['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FineGrainedMetrics:\n",
    "    @classmethod\n",
    "    def related_to_noun(self, doc, attr, noun):\n",
    "        for token in doc:\n",
    "            if token.text == noun:\n",
    "                subtree = [t.text for t in token.subtree]\n",
    "                if attr in subtree:\n",
    "                    return True\n",
    "                else:\n",
    "                    pass            #We might not get correct attr on the first occurence of that noun \n",
    "        return False                #If we dont get any matches return false\n",
    "\n",
    "    @classmethod\n",
    "    def colour(cls, meta, orig, generated_nouns):\n",
    "        colours = set([\n",
    "        'red', 'blue', 'green', 'yellow', 'black', 'white', 'gray', 'grey', 'orange',\n",
    "        'pink', 'purple', 'brown', 'violet', 'indigo', 'turquoise', 'cyan', 'magenta'\n",
    "        ])\n",
    "        colour_avg = 0.\n",
    "        orig_colour_set = set()\n",
    "        for this_noun in generated_nouns:\n",
    "            meta_adjectives = set([token.text for token in meta if cls.related_to_noun(meta, token.text, this_noun) and (token.dep_ in ['acomp', \"amod\"])])\n",
    "            orig_adjectives = set([token.text for token in orig if cls.related_to_noun(orig, token.text, this_noun) and (token.dep_ in ['acomp', \"amod\"])])\n",
    "            meta_colours = colours.intersection(meta_adjectives)\n",
    "            orig_colours = colours.intersection(orig_adjectives)\n",
    "            orig_colour_set.update(orig_colours)\n",
    "            if len(orig_colours)==0:\n",
    "                pass\n",
    "            else:\n",
    "                colour_avg += len(orig_colours.intersection(meta_colours))\n",
    "        if len(orig_colour_set)==0:\n",
    "            return -1\n",
    "        else:\n",
    "            return colour_avg / len(orig_colour_set)\n",
    "    \n",
    "    @classmethod\n",
    "    def number(cls, meta, orig, generated_nouns):\n",
    "        num_avg = 0.\n",
    "        orig_num_set = set()\n",
    "        quantities_map = {\n",
    "            'a': '1',\n",
    "            'an': '1',\n",
    "            'the': '1',\n",
    "            'couple': '2',\n",
    "            'dozen': '12', \n",
    "                         }\n",
    "        \n",
    "        for this_noun in generated_nouns:\n",
    "            meta_nums = set([token.text for token in meta if cls.related_to_noun(meta, token.text, this_noun) and (token.dep_ in [\"nummod\", 'det'])])   ##determiners --> 1\n",
    "            orig_nums = set([token.text for token in orig if cls.related_to_noun(orig, token.text, this_noun) and (token.dep_ in [\"nummod\", 'det'])])   ## make mapping of words like a, and the to one and couple to two dozen to 12 etc and replace in original caption to compute\n",
    "            for token in range(len(list(meta_nums))):\n",
    "                if meta_nums[token].lower() in quantities_map.keys():\n",
    "                    meta_nums[token] = quantities_map[meta_nums[token].lower()]\n",
    "\n",
    "            for token in range(len(list(orig_nums))):\n",
    "                if orig_nums[token].lower() in quantities_map.keys():\n",
    "                    orig_nums[token] = quantities_map[orig_nums[token].lower()]\n",
    "                    \n",
    "            orig_num_set.update(orig_nums)\n",
    "            if len(orig_nums)==0:\n",
    "                pass\n",
    "            else:\n",
    "                num_avg += len(orig_nums.intersection(meta_nums))\n",
    "        \n",
    "        if len(orig_num_set) == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            return num_avg/len(orig_num_set)\n",
    "\n",
    "    @classmethod\n",
    "    def text(cls, meta, orig, _):\n",
    "        indicators = ['written', 'saying', 'says']\n",
    "        pattern = r'[\"\\']([^\"\\']*)[\"\\']'\n",
    "        orig_matches = None\n",
    "        meta_matches = None\n",
    "        for token in orig:\n",
    "            if token.text in indicators:\n",
    "                orig_matches = re.findall(pattern, orig.text)\n",
    "                meta_matches = re.findall(pattern, meta.text)\n",
    "                break\n",
    "        text_avg = 0.\n",
    "        if orig_matches is not None:\n",
    "            orig_matches = list(map(lambda s: ''.join(s.lower().split()), orig_matches))\n",
    "            meta_matches = list(map(lambda s: ''.join(s.lower().split()), meta_matches))\n",
    "            for this_text in orig_matches:\n",
    "                for this_meta_text in meta_matches:\n",
    "                    if this_text in this_meta_text:\n",
    "                        text_avg += 1\n",
    "            return text_avg/len(orig_matches)\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    @classmethod\n",
    "    def extract_triplets(cls, doc):\n",
    "        triplets = []\n",
    "        for token in doc:\n",
    "            # Identify prepositions ('prep') and their objects ('pobj')\n",
    "            if token.dep_ == 'prep':\n",
    "                # Find the prepositional object (pobj)\n",
    "                pobj = [child for child in token.children if child.dep_ == 'pobj']\n",
    "                if len(pobj) > 0:\n",
    "                    # Now find the subject of the preposition (related to the governing noun or verb)\n",
    "                    subject = None\n",
    "                    for ancestor in token.ancestors:\n",
    "                        if ancestor.dep_ in ['nsubj', 'nsubjpass']:  # Subject of the sentence\n",
    "                            subject = ancestor.text\n",
    "                            break\n",
    "\n",
    "                    # Create a triplet only if we have both a subject and a prepositional object\n",
    "                    if subject is not None:\n",
    "                        triplets.append((subject, token.text, pobj[0].text))\n",
    "\n",
    "        return triplets\n",
    "    \n",
    "    @classmethod\n",
    "    def position(cls, meta, orig, generated_nouns):\n",
    "        # Extract triplets from both captions\n",
    "        orig_triplets = cls.extract_triplets(orig)\n",
    "        meta_triplets = cls.extract_triplets(meta)\n",
    "        \n",
    "        # Convert triplets to sets for easier comparison\n",
    "        orig_triplets_set = set(orig_triplets)\n",
    "        meta_triplets_set = set(meta_triplets)\n",
    "        \n",
    "        # If no triplets are found in the original caption, return -1 (no recall calculation possible)\n",
    "        if len(orig_triplets_set) == 0:\n",
    "            return -1\n",
    "        \n",
    "        # Calculate the intersection of triplets between orig and meta captions\n",
    "        matched_triplets = orig_triplets_set.intersection(meta_triplets_set)\n",
    "        \n",
    "        # Compute recall as the ratio of correctly recalled triplets\n",
    "        recall = len(matched_triplets) / len(orig_triplets_set)\n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    @classmethod\n",
    "    def shape(cls, meta, orig, generated_nouns):\n",
    "        # Define a set of shape-related adjectives\n",
    "        shape_adjectives = set([\n",
    "            'circular', 'square', 'triangular', 'rectangular', 'oval', \n",
    "            'hexagonal', 'pentagonal', 'octagonal', 'spherical', 'cubical',\n",
    "            'cylindrical', 'conical', 'pyramidal'\n",
    "        ])\n",
    "        \n",
    "        shape_recall = 0.  # Variable to store the recall score\n",
    "        orig_shape_set = set()  # To track shapes in the original caption\n",
    "        \n",
    "        # Loop through each noun in generated nouns\n",
    "        for this_noun in generated_nouns:\n",
    "            # Get adjectives related to the current noun in 'meta' and 'orig'\n",
    "            meta_shapes = set([token.text for token in meta \n",
    "                               if cls.related_to_noun(meta, token.text, this_noun) and token.dep_ in ['acomp', 'amod']])\n",
    "            orig_shapes = set([token.text for token in orig \n",
    "                               if cls.related_to_noun(orig, token.text, this_noun) and token.dep_ in ['acomp', 'amod']])\n",
    "            \n",
    "            # Filter for shape-related adjectives\n",
    "            meta_shapes_filtered = shape_adjectives.intersection(meta_shapes)\n",
    "            orig_shapes_filtered = shape_adjectives.intersection(orig_shapes)\n",
    "            \n",
    "            # Update the original shape set to calculate recall\n",
    "            orig_shape_set.update(orig_shapes_filtered)\n",
    "            \n",
    "            # If no shapes in the original caption, continue\n",
    "            if len(orig_shapes_filtered) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                # Calculate the intersection of shapes between orig and meta captions\n",
    "                shape_recall += len(orig_shapes_filtered.intersection(meta_shapes_filtered))\n",
    "        \n",
    "        # If no shapes were found in the original caption, return -1 (no recall calculation possible)\n",
    "        if len(orig_shape_set) == 0:\n",
    "            return -1\n",
    "        else:\n",
    "            # Return the recall score as the ratio of correctly recalled shapes\n",
    "            return shape_recall / len(orig_shape_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stage_one_metric(meta, orig):\n",
    "    meta_nouns = set([token.text for token in meta if token.pos_==\"NOUN\" or token.pos_==\"PROPN\"])\n",
    "    orig_nouns = set([token.text for token in orig if token.pos_==\"NOUN\"or token.pos_==\"PROPN\"])\n",
    "    non_generated = orig_nouns.difference(meta_nouns)\n",
    "    noun_recall = len(orig_nouns.intersection(meta_nouns))/len(orig_nouns)\n",
    "    generated_nouns = orig_nouns.intersection(meta_nouns)\n",
    "    return noun_recall, non_generated, generated_nouns\n",
    "\n",
    "def stage_two_metric(meta, orig, generated_nouns, aspects=['colour', 'number', 'shape', 'position', 'text']):\n",
    "    metrics = {}\n",
    "    for aspect in aspects:\n",
    "        metric = getattr(FineGrainedMetrics, aspect)\n",
    "        metrics[aspect] = metric(meta, orig, generated_nouns)\n",
    "    return metrics\n",
    "\n",
    "def stage_three_metric():\n",
    "    pass\n",
    "\n",
    "for i, row in sdxl_captions.iterrows():\n",
    "    meta_caption = row['Meta Caption']\n",
    "    orig_caption = row['Prompts']\n",
    "    meta_ = pos_tag_pipe(meta_caption)\n",
    "    orig_ = pos_tag_pipe(orig_caption)\n",
    "    n_recall, non_generated, generated_nouns = stage_one_metric(meta_, orig_)\n",
    "    fine_grained_metrics = stage_two_metric(meta_, orig_, generated_nouns, aspects=['colour', 'number', 'text', 'position', 'shape'])\n",
    "    if i==180:\n",
    "        break\n",
    "    # print(n_recall, list(non_generated))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T10:25:02.054577Z",
     "iopub.status.busy": "2024-09-09T10:25:02.053794Z",
     "iopub.status.idle": "2024-09-09T10:25:02.065867Z",
     "shell.execute_reply": "2024-09-09T10:25:02.065388Z",
     "shell.execute_reply.started": "2024-09-09T10:25:02.054546Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_caption' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmeta_caption\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta_caption' is not defined"
     ]
    }
   ],
   "source": [
    "meta_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colour': -1, 'number': -1, 'text': 0.0}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_grained_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('advcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
