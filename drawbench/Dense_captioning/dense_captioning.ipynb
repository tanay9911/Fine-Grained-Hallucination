{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdxl_dataset_path = '/mnt/data/workspace/misc/sdxl_outputs'\n",
    "sd_2_dataset_path = '/mnt/data/workspace/misc/sd_2_outputs'\n",
    "sdxl_mask_path = os.path.join(sdxl_dataset_path, 'masks')\n",
    "sd_2_mask_path = os.path.join(sd_2_dataset_path, 'masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 11:03:31.050598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-20 11:03:31.050651: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-20 11:03:31.051765: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-20 11:03:31.057915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-20 11:03:31.847207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "blip_processor = InstructBlipProcessor.from_pretrained(\"Salesforce/instructblip-vicuna-7b\")\n",
    "blip_model = InstructBlipForConditionalGeneration.from_pretrained(\"Salesforce/instructblip-vicuna-7b\", torch_dtype=torch.half).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "meta_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-7B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "meta_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroid(mask):\n",
    "    mask_array = np.array(mask)\n",
    "    mask_coordinates = np.column_stack(np.where(mask_array > 0))\n",
    "    if len(mask_coordinates) == 0:\n",
    "        centroid = (None, None)\n",
    "    else:\n",
    "        # Calculate the median of the coordinates\n",
    "        centroid = np.mean(mask_coordinates, axis=0)\n",
    "        centroid = centroid[1], centroid[0]\n",
    "    return tuple(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_caption(mask, img):\n",
    "    # prompt = \"Describe the image with a focus on the intricate details of each object, including their colors, shapes, and numbers. Include any physical aspects that appear unusual or incorrect according to general knowledge.\"\n",
    "    mask = np.array(mask)\n",
    "    mask = mask[:, :, np.newaxis]\n",
    "    mask = np.concatenate((mask, mask, mask), axis=2)\n",
    "    img = np.array(img)\n",
    "    img = mask * img\n",
    "    img = Image.fromarray(img)\n",
    "    prompt = 'Describe the image with a focus on the intricate details of the object, including their color, shape, and number. Include any physical aspects that appear unusual or incorrect according to general knowledge. You can ignore the pitch black background.'\n",
    "    inputs = blip_processor(img, \n",
    "                            prompt, \n",
    "                            return_tensors=\"pt\"\n",
    "                            ).to(\"cuda\", torch.float16)\n",
    "    out = blip_model.generate(**inputs, max_length=200, do_sample=False)\n",
    "    return blip_processor.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_caption(dense_captions):\n",
    "    pattern = r'<caption>(.*?)</caption>'\n",
    "    prompt = \"\"\"I am providing you with captions for sub-regions of an image. These captions will be provided by the corresponding centroids\n",
    "for the objects in the sub-regions. I want you to stitch all the dense captions into one unified caption for the entire image.\n",
    "You have to use the centroid information to deduce the relative positions of each of the objects. Do not add any new information\n",
    "to the captions. Make the caption as short as possible without losing too many details. Any mention of a black background should be ignored. Do not hallucinate any details. Generate the final caption within the <caption></caption> tags.\n",
    "\n",
    "\"\"\"\n",
    "    for i, this_caption in enumerate(dense_captions):\n",
    "        string = f\"{i+1}. {this_caption['centroid']} {this_caption['caption']}\\n\"\n",
    "        prompt = prompt + string\n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a meta image captioning model. You look at various sub-captions and create a meaningful grounded caption using those. You can  use additional provided information to facilitate spatial reasoning.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "    text = meta_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "    model_inputs = meta_tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    generated_ids = meta_model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=1024\n",
    ")\n",
    "    generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "    response = meta_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    try:\n",
    "        response = re.findall(pattern, response, re.DOTALL)[0]\n",
    "        return response\n",
    "    except:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "201it [00:00, 221798.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.jpg\n",
      "106.jpg\n",
      "70.jpg\n",
      "151.jpg\n",
      "54.jpg\n",
      "190.jpg\n",
      "9.jpg\n",
      "18.jpg\n",
      "39.jpg\n",
      "6.jpg\n",
      "22.jpg\n",
      "129.jpg\n",
      "40.jpg\n",
      "198.jpg\n",
      "138.jpg\n",
      "51.jpg\n",
      "90.jpg\n",
      "91.jpg\n",
      "125.jpg\n",
      "110.jpg\n",
      "104.jpg\n",
      "157.jpg\n",
      "0.jpg\n",
      "181.jpg\n",
      "186.jpg\n",
      "123.jpg\n",
      "169.jpg\n",
      "65.jpg\n",
      "62.jpg\n",
      "46.jpg\n",
      "99.jpg\n",
      "48.jpg\n",
      "115.jpg\n",
      "141.jpg\n",
      "160.jpg\n",
      "85.jpg\n",
      "148.jpg\n",
      "126.jpg\n",
      "178.jpg\n",
      "77.jpg\n",
      "101.jpg\n",
      "162.jpg\n",
      "124.jpg\n",
      "100.jpg\n",
      "92.jpg\n",
      "23.jpg\n",
      "97.jpg\n",
      "103.jpg\n",
      "75.jpg\n",
      "93.jpg\n",
      "43.jpg\n",
      "130.jpg\n",
      "191.jpg\n",
      "149.jpg\n",
      "155.jpg\n",
      "179.jpg\n",
      "113.jpg\n",
      "44.jpg\n",
      "187.jpg\n",
      "21.jpg\n",
      "2.jpg\n",
      "27.jpg\n",
      "172.jpg\n",
      "56.jpg\n",
      "147.jpg\n",
      "84.jpg\n",
      "16.jpg\n",
      "25.jpg\n",
      "29.jpg\n",
      "83.jpg\n",
      "180.jpg\n",
      "133.jpg\n",
      "122.jpg\n",
      "143.jpg\n",
      "59.jpg\n",
      "114.jpg\n",
      "34.jpg\n",
      "71.jpg\n",
      "139.jpg\n",
      "61.jpg\n",
      "12.jpg\n",
      "60.jpg\n",
      "7.jpg\n",
      "195.jpg\n",
      "63.jpg\n",
      "127.jpg\n",
      "52.jpg\n",
      "88.jpg\n",
      "94.jpg\n",
      "95.jpg\n",
      "102.jpg\n",
      "33.jpg\n",
      "masks\n",
      "170.jpg\n",
      "194.jpg\n",
      "121.jpg\n",
      "5.jpg\n",
      "13.jpg\n",
      "72.jpg\n",
      "45.jpg\n",
      "42.jpg\n",
      "193.jpg\n",
      "82.jpg\n",
      "111.jpg\n",
      "108.jpg\n",
      "168.jpg\n",
      "4.jpg\n",
      "176.jpg\n",
      "78.jpg\n",
      "35.jpg\n",
      "87.jpg\n",
      "118.jpg\n",
      "158.jpg\n",
      "161.jpg\n",
      "24.jpg\n",
      "182.jpg\n",
      "47.jpg\n",
      "152.jpg\n",
      "79.jpg\n",
      "188.jpg\n",
      "163.jpg\n",
      "117.jpg\n",
      "20.jpg\n",
      "86.jpg\n",
      "189.jpg\n",
      "185.jpg\n",
      "89.jpg\n",
      "68.jpg\n",
      "164.jpg\n",
      "128.jpg\n",
      "159.jpg\n",
      "55.jpg\n",
      "74.jpg\n",
      "14.jpg\n",
      "96.jpg\n",
      "166.jpg\n",
      "119.jpg\n",
      "11.jpg\n",
      "8.jpg\n",
      "132.jpg\n",
      "105.jpg\n",
      "37.jpg\n",
      "136.jpg\n",
      "53.jpg\n",
      "32.jpg\n",
      "31.jpg\n",
      "17.jpg\n",
      "144.jpg\n",
      "30.jpg\n",
      "38.jpg\n",
      "66.jpg\n",
      "120.jpg\n",
      "81.jpg\n",
      "15.jpg\n",
      "192.jpg\n",
      "58.jpg\n",
      "171.jpg\n",
      "177.jpg\n",
      "131.jpg\n",
      "67.jpg\n",
      "173.jpg\n",
      "1.jpg\n",
      "109.jpg\n",
      "145.jpg\n",
      "142.jpg\n",
      "167.jpg\n",
      "154.jpg\n",
      "26.jpg\n",
      "116.jpg\n",
      "41.jpg\n",
      "197.jpg\n",
      "137.jpg\n",
      "112.jpg\n",
      "140.jpg\n",
      "135.jpg\n",
      "196.jpg\n",
      "10.jpg\n",
      "134.jpg\n",
      "183.jpg\n",
      "199.jpg\n",
      "3.jpg\n",
      "146.jpg\n",
      "36.jpg\n",
      "165.jpg\n",
      "153.jpg\n",
      "28.jpg\n",
      "174.jpg\n",
      "57.jpg\n",
      "107.jpg\n",
      "50.jpg\n",
      "76.jpg\n",
      "69.jpg\n",
      "184.jpg\n",
      "19.jpg\n",
      "64.jpg\n",
      "156.jpg\n",
      "73.jpg\n",
      "150.jpg\n",
      "80.jpg\n",
      "175.jpg\n",
      "49.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "df = pd.read_csv('/mnt/data/workspace/misc/DrawBenchPrompts.csv')\n",
    "for i, image_name in tqdm(enumerate(os.listdir(sdxl_dataset_path))):\n",
    "    print(image_name)\n",
    "    if not image_name.endswith('jpg'):\n",
    "        continue\n",
    "    idx = int(image_name.split('.')[0])\n",
    "    image_path = os.path.join(sdxl_dataset_path, image_name)\n",
    "    img = Image.open(image_path)\n",
    "    # img.show()\n",
    "    mask_dir = [i for i in os.listdir(sdxl_mask_path) if idx==int(i.split('-')[0])][0]\n",
    "    mask_dir = os.path.join(sdxl_mask_path, mask_dir)\n",
    "    dense_captions = []\n",
    "    for root, nouns, masks in os.walk(mask_dir):\n",
    "        for mask in masks:\n",
    "            mask_path = os.path.join(root, mask)\n",
    "            mask = Image.open(mask_path)\n",
    "            centroid = get_centroid(mask)\n",
    "            caption = get_dense_caption(mask, img)\n",
    "            description = {\"centroid\": centroid, \"caption\": caption}\n",
    "            dense_captions.append(description)\n",
    "    meta_caption = get_meta_caption(dense_captions)\n",
    "    print(meta_caption)\n",
    "    print(df.loc[idx, 'Prompts'])\n",
    "    df.loc[idx, 'Meta Caption'] = meta_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('meta_captions_sdxl.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prompts         An elephant is behind a tree. You can see the ...\n",
       "Category                                      Gary Marcus et al. \n",
       "Meta Caption    \\nIn the scene, a sizable elephant, distinguis...\n",
       "Name: 98, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[98, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_array = np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0, ..., 890, 890, 890]),\n",
       " array([  0,   1,   2, ..., 833, 834, 835]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(mask_array>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw.ellipse(((0,0), (1000, 500)), fill='yellow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
